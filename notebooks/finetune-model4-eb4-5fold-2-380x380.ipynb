{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time, gc\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pretrainedmodels\n",
    "from argparse import Namespace\n",
    "from sklearn.utils import shuffle\n",
    "from apex import amp\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from cvcore.data.auto_augment import RandAugment\n",
    "from PIL import Image\n",
    "from utils import bn_update, moving_average, copy_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_map.csv\t\t       train.csv\r\n",
      "sample_submission.csv\t       train.csv.zip\r\n",
      "test.csv\t\t       train_image_data_0.parquet\r\n",
      "test_image_data_0.parquet      train_image_data_0.parquet.zip\r\n",
      "test_image_data_0.parquet.zip  train_image_data_1.parquet\r\n",
      "test_image_data_1.parquet      train_image_data_1.parquet.zip\r\n",
      "test_image_data_1.parquet.zip  train_image_data_2.parquet\r\n",
      "test_image_data_2.parquet      train_image_data_2.parquet.zip\r\n",
      "test_image_data_2.parquet.zip  train_image_data_3.parquet\r\n",
      "test_image_data_3.parquet      train_image_data_3.parquet.zip\r\n",
      "test_image_data_3.parquet.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/chec/data/bengali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /home/chec/data/bengali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/chec/data/bengali'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "class_map_df = pd.read_csv(f'{DATA_DIR}/class_map.csv')\n",
    "sample_sub_df = pd.read_csv(f'{DATA_DIR}/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>ক্ট্রো</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_1</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>হ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>খ্রী</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_3</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>র্টি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_4</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>থ্রো</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  grapheme_root  vowel_diacritic  consonant_diacritic grapheme\n",
       "0  Train_0             15                9                    5   ক্ট্রো\n",
       "1  Train_1            159                0                    0        হ\n",
       "2  Train_2             22                3                    5     খ্রী\n",
       "3  Train_3             53                2                    2     র্টি\n",
       "4  Train_4             71                9                    5     থ্রো"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "def class_balanced_sampler(labels):\n",
    "    class_counts = np.bincount(labels)\n",
    "    total_samples = len(labels)\n",
    "    sample_weights = np.zeros_like(labels).astype(np.float32)\n",
    "    for idx, label in enumerate(labels):\n",
    "        sample_weights[idx] = total_samples / class_counts[label]\n",
    "    # return sample_weights\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights,\n",
    "        num_samples=total_samples)\n",
    "    return sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 137\n",
    "WIDTH = 236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import albumentations as albu\n",
    "def get_train_augs():\n",
    "    return RandAugment(n=2, m=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(x)\n",
    "finetune_class = 'grapheme_root'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class BengaliDataset(Dataset):\n",
    "    def __init__(self, df, img_df, train_mode=True, test_mode=False):\n",
    "        self.df = df\n",
    "        self.img_df = img_df\n",
    "        self.train_mode = train_mode\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = self.get_img(row.image_id)\n",
    "        orig_img = img.copy()\n",
    "        #print(img.shape)\n",
    "        if self.train_mode:\n",
    "            augs = get_train_augs()\n",
    "            #img = augs(image=img)['image']\n",
    "            img = np.asarray(augs(Image.fromarray(img)))\n",
    "        \n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "        orig_img = np.expand_dims(orig_img, axis=-1)\n",
    "        \n",
    "        #print('###', img.shape)\n",
    "        #img = np.concatenate([img, img, img], 2)\n",
    "        #print('>>>', img.shape)\n",
    "        \n",
    "        # taken from https://www.kaggle.com/iafoss/image-preprocessing-128x128\n",
    "        #MEAN = [ 0.06922848809290576,  0.06922848809290576,  0.06922848809290576]\n",
    "        #STD = [ 0.20515700083327537,  0.20515700083327537,  0.20515700083327537]\n",
    "        \n",
    "        img = transforms.functional.to_tensor(img)\n",
    "        orig_img = transforms.functional.to_tensor(orig_img)\n",
    "        \n",
    "        #img = transforms.functional.normalize(img, mean=MEAN, std=STD)\n",
    "        \n",
    "        if self.test_mode:\n",
    "            return img\n",
    "        elif self.train_mode:\n",
    "            return img, orig_img, torch.tensor([row.grapheme_root, row.vowel_diacritic, row.consonant_diacritic, row.word_label])\n",
    "        else:\n",
    "            return img, torch.tensor([row.grapheme_root, row.vowel_diacritic, row.consonant_diacritic, row.word_label])\n",
    "                    \n",
    "    def get_img(self, img_id):\n",
    "        return 255 - self.img_df.loc[img_id].values.reshape(HEIGHT, WIDTH).astype(np.uint8)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "def get_train_val_loaders(batch_size=4, val_batch_size=4, ifold=0, dev_mode=False):\n",
    "    print('fold:', ifold)\n",
    "    train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "\n",
    "    train_df = shuffle(train_df, random_state=1234)\n",
    "\n",
    "    grapheme_words = np.unique(train_df.grapheme.values)\n",
    "    grapheme_words_dict = {grapheme: i for i, grapheme in enumerate(grapheme_words)}\n",
    "    train_df['word_label'] = train_df['grapheme'].map(lambda x: grapheme_words_dict[x])\n",
    "\n",
    "    print(train_df.shape)\n",
    "\n",
    "    if dev_mode:\n",
    "        img_df = pd.read_parquet(f'{DATA_DIR}/train_image_data_0.parquet').set_index('image_id')\n",
    "        train_df = train_df.iloc[:1000]\n",
    "    else:\n",
    "        img_dfs = [pd.read_parquet(f'{DATA_DIR}/train_image_data_{i}.parquet') for i in range(4)]\n",
    "        img_df = pd.concat(img_dfs, axis=0).set_index('image_id')\n",
    "    print(img_df.shape)\n",
    "    #split_index = int(len(train_df) * 0.9)\n",
    "    \n",
    "    #train = train_df.iloc[:split_index]\n",
    "    #val = train_df.iloc[split_index:]\n",
    "    \n",
    "    kf = StratifiedKFold(5, random_state=1234, shuffle=True)\n",
    "    for i, (train_idx, val_idx) in enumerate(kf.split(train_df, train_df['grapheme_root'].values)):\n",
    "        if i == ifold:\n",
    "            #print(val_idx)\n",
    "            train = train_df.iloc[train_idx]\n",
    "            val = train_df.iloc[val_idx]\n",
    "            break\n",
    "    assert i == ifold\n",
    "    print(train.shape, val.shape)\n",
    "    \n",
    "    train_ds = BengaliDataset(train, img_df, True, False)\n",
    "    val_ds = BengaliDataset(val, img_df, False, False)\n",
    "    \n",
    "    sampler = class_balanced_sampler(train[finetune_class])\n",
    "    print('sampler length: {}'.format(sampler.weights.shape))\n",
    "\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=batch_size, shuffle=False, num_workers=8, drop_last=False, pin_memory=True, sampler=sampler)\n",
    "    train_loader.num = len(train_ds)\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=val_batch_size, shuffle=False, num_workers=8, drop_last=False, pin_memory=True)\n",
    "    val_loader.num = len(val_ds)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader, val_loader = get_train_val_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x in train_loader:\n",
    "#    print(x)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'resnet50' # could be fbresnet152 or inceptionresnetv2\n",
    "#model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet').cuda()\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import timm\n",
    "from timm.models.activations import Swish, Mish\n",
    "from timm.models.adaptive_avgmax_pool import SelectAdaptivePool2d\n",
    "\n",
    "MEAN = [ 0.06922848809290576 ]\n",
    "STD = [ 0.20515700083327537 ]\n",
    "\n",
    "class BengaliNetOld(nn.Module):\n",
    "    \"\"\"\n",
    "    EfficientNet B0-B8.\n",
    "    Args:\n",
    "        cfg (CfgNode): configs\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super(BengaliNetOld, self).__init__()\n",
    "        model_name = cfg.MODEL_NAME\n",
    "        pretrained = cfg.PRETRAINED\n",
    "        input_channels = cfg.IN_CHANNELS\n",
    "        pool_type = cfg.POOL_TYPE\n",
    "        drop_connect_rate = cfg.DROP_CONNECT\n",
    "        self.drop_rate = cfg.DROPOUT\n",
    "        cls_head = cfg.CLS_HEAD\n",
    "        num_total_classes = cfg.NUM_GRAPHEME_CLASSES + cfg.NUM_VOWEL_CLASSES + cfg.NUM_CONSONANT_CLASSES \\\n",
    "            + cfg.NUM_WORD_CLASSES\n",
    "\n",
    "        backbone = timm.create_model(\n",
    "            model_name=model_name,\n",
    "            pretrained=pretrained,\n",
    "            in_chans=input_channels,\n",
    "            drop_connect_rate=drop_connect_rate,\n",
    "        )\n",
    "        self.conv_stem = backbone.conv_stem\n",
    "        self.bn1 = backbone.bn1\n",
    "        self.act1 = backbone.act1\n",
    "        ### Original blocks ###\n",
    "        for i in range(len((backbone.blocks))):\n",
    "            setattr(self, \"block{}\".format(str(i)), backbone.blocks[i])\n",
    "        self.conv_head = backbone.conv_head\n",
    "        self.bn2 = backbone.bn2\n",
    "        self.act2 = backbone.act2\n",
    "        self.aux_block5 = backbone.blocks[5]\n",
    "        self.aux_num_features = self.block5[-1].bn3.num_features\n",
    "        self.aux_head4 = nn.Conv2d(self.aux_num_features, self.aux_num_features * 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(self.aux_num_features * 4)\n",
    "        self.act4 = Swish()\n",
    "        self.aux_head5 = nn.Conv2d(self.aux_num_features, self.aux_num_features * 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(self.aux_num_features * 4)\n",
    "        self.act5 = Swish()\n",
    "        self.global_pool = SelectAdaptivePool2d(pool_type=pool_type)\n",
    "        self.num_features = backbone.num_features * self.global_pool.feat_mult()\n",
    "        assert cls_head == 'linear'\n",
    "        if cls_head == \"linear\":\n",
    "            ### Baseline head ###\n",
    "            self.fc = nn.Linear(self.num_features, num_total_classes)            \n",
    "            self.aux_fc1 = nn.Linear(self.aux_num_features*4, num_total_classes)\n",
    "            self.aux_fc2 = nn.Linear(self.aux_num_features*4, num_total_classes)\n",
    "            \n",
    "            for fc in [self.fc, self.aux_fc1, self.aux_fc2]:\n",
    "                nn.init.zeros_(fc.bias.data)\n",
    "        elif cls_head == \"norm_softmax\":\n",
    "            ### NormSoftmax ###\n",
    "            self.grapheme_fc = NormSoftmax(self.num_features, num_grapheme_classes)\n",
    "            self.consonant_fc = NormSoftmax(self.num_features, num_consonant_classes)\n",
    "            self.vowel_fc = NormSoftmax(self.num_features, num_vowel_classes)\n",
    "        # Replace with Mish activation\n",
    "        if cfg.MODEL_ACTIVATION == \"mish\":\n",
    "            convert_swish_to_mish(self)\n",
    "        del backbone\n",
    "\n",
    "    def _features(self, x):\n",
    "        x = self.conv_stem(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.block0(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x); b4 = x\n",
    "        x = self.block5(x); b4 = self.aux_block5(b4); b5 = x\n",
    "        x = self.block6(x)\n",
    "        x = self.conv_head(x); b4 = self.aux_head4(b4); b5 = self.aux_head5(b5)\n",
    "        x = self.bn2(x); b4 = self.bn4(b4); b5 = self.bn5(b5)\n",
    "        x = self.act2(x); b4 = self.act4(b4); b5 = self.act5(b5)\n",
    "        return b4, b5, x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, size=(380, 380), mode='bilinear', align_corners=False)\n",
    "        for i in range(len(x)):\n",
    "            transforms.functional.normalize(x[i], mean=MEAN, std=STD, inplace=True)\n",
    "\n",
    "        # _, _, x = self._features(x)\n",
    "        b4, b5, x = self._features(x)\n",
    "        x = self.global_pool(x); b4 = self.global_pool(b4); b5 = self.global_pool(b5)\n",
    "        x = torch.flatten(x, 1); b4 = torch.flatten(b4, 1); b5 = torch.flatten(b5, 1)\n",
    "        if self.drop_rate > 0.:\n",
    "            x = F.dropout(x, p=self.drop_rate, training=self.training)\n",
    "        logits = self.fc(x)\n",
    "        \n",
    "        aux_logits1 = self.aux_fc1(b4)\n",
    "        aux_logits2 = self.aux_fc2(b5)\n",
    "        \n",
    "        return logits, aux_logits1, aux_logits2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import timm\n",
    "from timm.models.activations import Swish, Mish\n",
    "from timm.models.adaptive_avgmax_pool import SelectAdaptivePool2d\n",
    "\n",
    "MEAN = [ 0.06922848809290576 ]\n",
    "STD = [ 0.20515700083327537 ]\n",
    "\n",
    "class BengaliNet(nn.Module):\n",
    "    \"\"\"\n",
    "    EfficientNet B0-B8.\n",
    "    Args:\n",
    "        cfg (CfgNode): configs\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super(BengaliNet, self).__init__()\n",
    "        model_name = cfg.MODEL_NAME\n",
    "        pretrained = cfg.PRETRAINED\n",
    "        input_channels = cfg.IN_CHANNELS\n",
    "        pool_type = cfg.POOL_TYPE\n",
    "        drop_connect_rate = cfg.DROP_CONNECT\n",
    "        self.drop_rate = cfg.DROPOUT\n",
    "        cls_head = cfg.CLS_HEAD\n",
    "        num_total_classes = cfg.NUM_GRAPHEME_CLASSES + cfg.NUM_VOWEL_CLASSES + cfg.NUM_CONSONANT_CLASSES \\\n",
    "            + cfg.NUM_WORD_CLASSES\n",
    "\n",
    "        backbone = timm.create_model(\n",
    "            model_name=model_name,\n",
    "            pretrained=pretrained,\n",
    "            in_chans=input_channels,\n",
    "            drop_connect_rate=drop_connect_rate,\n",
    "        )\n",
    "        self.conv_stem = backbone.conv_stem\n",
    "        self.bn1 = backbone.bn1\n",
    "        self.act1 = backbone.act1\n",
    "        ### Original blocks ###\n",
    "        for i in range(len((backbone.blocks))):\n",
    "            setattr(self, \"block{}\".format(str(i)), backbone.blocks[i])\n",
    "        self.conv_head = backbone.conv_head\n",
    "        self.bn2 = backbone.bn2\n",
    "        self.act2 = backbone.act2\n",
    "        self.aux_block5 = backbone.blocks[5]\n",
    "        self.aux_num_features = self.block5[-1].bn3.num_features\n",
    "        self.aux_head4 = nn.Conv2d(self.aux_num_features, self.aux_num_features * 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(self.aux_num_features * 4)\n",
    "        self.act4 = Swish()\n",
    "        self.aux_head5 = nn.Conv2d(self.aux_num_features, self.aux_num_features * 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(self.aux_num_features * 4)\n",
    "        self.act5 = Swish()\n",
    "        self.global_pool = SelectAdaptivePool2d(pool_type=pool_type)\n",
    "        self.num_features = backbone.num_features * self.global_pool.feat_mult()\n",
    "        assert cls_head == 'linear'\n",
    "        if cls_head == \"linear\":\n",
    "            ### Baseline head ###\n",
    "            #self.fc = nn.Linear(self.num_features, num_total_classes)\n",
    "            self.fc_g = nn.Linear(self.num_features, 168)\n",
    "            self.fc_v = nn.Linear(self.num_features, 11)\n",
    "            self.fc_c = nn.Linear(self.num_features, 7)\n",
    "            self.fc_w = nn.Linear(self.num_features, 1295)\n",
    "            \n",
    "            self.aux_fc1 = nn.Linear(self.aux_num_features*4, num_total_classes)\n",
    "            self.aux_fc2 = nn.Linear(self.aux_num_features*4, num_total_classes)\n",
    "            \n",
    "            for fc in [self.fc_g, self.fc_v, self.fc_c, self.fc_w, self.aux_fc1, self.aux_fc2]:\n",
    "                nn.init.zeros_(fc.bias.data)\n",
    "        del backbone\n",
    "\n",
    "    def _features(self, x):\n",
    "        x = self.conv_stem(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.block0(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x); b4 = x\n",
    "        x = self.block5(x); b4 = self.aux_block5(b4); b5 = x\n",
    "        x = self.block6(x)\n",
    "        x = self.conv_head(x); b4 = self.aux_head4(b4); b5 = self.aux_head5(b5)\n",
    "        x = self.bn2(x); b4 = self.bn4(b4); b5 = self.bn5(b5)\n",
    "        x = self.act2(x); b4 = self.act4(b4); b5 = self.act5(b5)\n",
    "        return b4, b5, x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, size=(380, 380), mode='bilinear', align_corners=False)\n",
    "        for i in range(len(x)):\n",
    "            transforms.functional.normalize(x[i], mean=MEAN, std=STD, inplace=True)\n",
    "\n",
    "        # _, _, x = self._features(x)\n",
    "        b4, b5, x = self._features(x)\n",
    "        x = self.global_pool(x); b4 = self.global_pool(b4); b5 = self.global_pool(b5)\n",
    "        x = torch.flatten(x, 1); b4 = torch.flatten(b4, 1); b5 = torch.flatten(b5, 1)\n",
    "        if self.drop_rate > 0.:\n",
    "            x = F.dropout(x, p=self.drop_rate, training=self.training)\n",
    "        #logits = self.fc(x)\n",
    "        g, v, c, w = self.fc_g(x), self.fc_v(x), self.fc_c(x), self.fc_w(x)\n",
    "        logits = torch.cat([g, v, c, w], 1)\n",
    "        \n",
    "        aux_logits1 = self.aux_fc1(b4)\n",
    "        aux_logits2 = self.aux_fc2(b5)\n",
    "        \n",
    "        return logits, aux_logits1, aux_logits2\n",
    "    \n",
    "    def set_mode(self, mode, is_freeze_bn=True):\n",
    "        self.mode = mode\n",
    "        if mode in ['eval', 'valid', 'test']:\n",
    "            self.eval()\n",
    "\n",
    "        elif mode in ['train']:\n",
    "            self.train()\n",
    "        if is_freeze_bn == True:  ##freeze\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.BatchNorm2d):\n",
    "                    m.eval()\n",
    "                    m.weight.requires_grad = False\n",
    "                    m.bias.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Namespace()\n",
    "cfg.MODEL_NAME = 'tf_efficientnet_b4'\n",
    "cfg.PRETRAINED = True\n",
    "cfg.IN_CHANNELS = 1\n",
    "cfg.POOL_TYPE = 'avg'\n",
    "cfg.CLS_HEAD = 'linear'\n",
    "cfg.MODEL_ACTIVATION = 'swish'\n",
    "cfg.DROP_CONNECT = 0.2\n",
    "cfg.DROPOUT= 0.\n",
    "cfg.NUM_WORD_CLASSES = 1295\n",
    "cfg.NUM_GRAPHEME_CLASSES = 168\n",
    "cfg.NUM_VOWEL_CLASSES = 11\n",
    "cfg.NUM_CONSONANT_CLASSES = 7\n",
    "#cfg.CKP_NAME = 'model4_eb4_fold2_380.pth'\n",
    "cfg.CKP_NAME = 'model4_finetune_eb4_fold2_380.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model():\n",
    "    model_old = BengaliNetOld(cfg)\n",
    "    model_old.load_state_dict(torch.load('./model4-ckps/tf_efficientnet_b4/model4_eb4_fold2_380_swa_cv998693.pth'))    \n",
    "    model = BengaliNet(cfg)\n",
    "    model.load_state_dict(torch.load('./model4-ckps/tf_efficientnet_b4/model4_eb4_fold2_380_swa_cv998693.pth'), strict=False)\n",
    "    model.fc_g.weight.data = model_old.fc.weight.data[:168] \n",
    "    model.fc_v.weight.data = model_old.fc.weight.data[168:168+11] \n",
    "    model.fc_c.weight.data = model_old.fc.weight.data[168+11:168+11+7] \n",
    "    model.fc_w.weight.data = model_old.fc.weight.data[168+11+7:] \n",
    "    model = model.cuda()\n",
    "    model = amp.initialize(model, opt_level='O1')\n",
    "    model = nn.DataParallel(model)\n",
    "    validate(model, val_loader)\n",
    "    torch.save(model.module.state_dict(), './model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './model4-finetune-ckps'\n",
    "def create_model(cfg):\n",
    "    model = BengaliNet(cfg)\n",
    "    model_file = os.path.join(MODEL_DIR, cfg.MODEL_NAME, cfg.CKP_NAME)\n",
    "\n",
    "    parent_dir = os.path.dirname(model_file)\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "\n",
    "    print('model file: {}, exist: {}'.format(model_file, os.path.exists(model_file)))\n",
    "\n",
    "    if os.path.exists(model_file):\n",
    "        print('loading {}...'.format(model_file))\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "    \n",
    "    return model, model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bnet = BengaliNet('se_resnext50_32x4d').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bnet(torch.randn((2, 1, 137, 236)).cuda()).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.111111"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(1/9, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "\n",
    "\n",
    "def calc_metrics(preds0, preds1, preds2, preds3, y):\n",
    "    assert len(y) == len(preds0) == len(preds1) == len(preds2) == len(preds3)\n",
    "\n",
    "    recall_grapheme = sklearn.metrics.recall_score(y[:, 0], preds0, average='macro')\n",
    "    recall_vowel = sklearn.metrics.recall_score(y[:, 1], preds1, average='macro')\n",
    "    recall_consonant = sklearn.metrics.recall_score(y[:, 2], preds2, average='macro')\n",
    "    recall_word = sklearn.metrics.recall_score(y[:, 3], preds3, average='macro')\n",
    "    \n",
    "    scores = [recall_grapheme, recall_vowel, recall_consonant]\n",
    "    final_recall_score = np.average(scores, weights=[2, 1, 1])\n",
    "    \n",
    "    old_recall_grapheme = sklearn.metrics.recall_score(preds0, y[:, 0], average='macro')\n",
    "    old_recall_vowel = sklearn.metrics.recall_score(preds1, y[:, 1], average='macro')\n",
    "    old_recall_consonant = sklearn.metrics.recall_score(preds2, y[:, 2], average='macro')\n",
    "    old_recall_word = sklearn.metrics.recall_score(preds3, y[:, 3], average='macro')\n",
    "    \n",
    "    old_scores = [old_recall_grapheme, old_recall_vowel, old_recall_consonant]\n",
    "    old_final_recall_score = np.average(old_scores, weights=[2, 1, 1])\n",
    "\n",
    "    \n",
    "    metrics = {}\n",
    "    metrics['recall'] = round(final_recall_score, 6)\n",
    "    metrics['recall_grapheme'] = round(recall_grapheme, 6)\n",
    "    metrics['recall_vowel'] = round(recall_vowel, 6)\n",
    "    metrics['recall_consonant'] = round(recall_consonant, 6)\n",
    "    metrics['recall_word'] = round(recall_word, 6)\n",
    "    \n",
    "    metrics['acc_grapheme'] = round((preds0 == y[:, 0]).sum() / len(y), 6)\n",
    "    metrics['acc_vowel'] = round((preds1 == y[:, 1]).sum() / len(y), 6)\n",
    "    metrics['acc_consonant'] = round((preds2 == y[:, 2]).sum() / len(y), 6)\n",
    "    metrics['acc_word'] = round((preds3 == y[:, 3]).sum() / len(y), 6)  \n",
    "    \n",
    "    metrics['old_recall'] = round(old_final_recall_score, 6)\n",
    "    metrics['old_recall_grapheme'] = round(old_recall_grapheme, 6)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(outputs, y_true):\n",
    "    # outputs: (N, 182)\n",
    "    # y_true: (N, 3)\n",
    "    \n",
    "    outputs = torch.split(outputs, [168, 11, 7, 1295], dim=1)\n",
    "    loss0 = F.cross_entropy(outputs[0], y_true[:, 0], reduction='mean')\n",
    "    loss1 = F.cross_entropy(outputs[1], y_true[:, 1], reduction='mean')\n",
    "    loss2 = F.cross_entropy(outputs[2], y_true[:, 2], reduction='mean')\n",
    "    loss3 = F.cross_entropy(outputs[3], y_true[:, 3], reduction='mean')\n",
    "    \n",
    "    return loss0 + loss1 + loss2 + loss3 #, loss0.item(), loss1.item(), loss2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader):\n",
    "    #model.eval()\n",
    "    model.module.set_mode('valid', is_freeze_bn=True)\n",
    "\n",
    "    loss0, loss1, loss2, loss3 = 0., 0., 0., 0.\n",
    "    preds0, preds1, preds2, preds3 = [], [], [], []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            y_true.append(y)\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            outputs, _, _ = model(x)\n",
    "            outputs = torch.split(outputs, [168, 11, 7, 1295], dim=1)\n",
    "            \n",
    "            preds0.append(torch.max(outputs[0], dim=1)[1])\n",
    "            preds1.append(torch.max(outputs[1], dim=1)[1])\n",
    "            preds2.append(torch.max(outputs[2], dim=1)[1])\n",
    "            preds3.append(torch.max(outputs[3], dim=1)[1])\n",
    "            loss0 += F.cross_entropy(outputs[0], y[:, 0], reduction='sum').item()\n",
    "            loss1 += F.cross_entropy(outputs[1], y[:, 1], reduction='sum').item()\n",
    "            loss2 += F.cross_entropy(outputs[2], y[:, 2], reduction='sum').item()\n",
    "            loss3 += F.cross_entropy(outputs[3], y[:, 3], reduction='sum').item()\n",
    "            \n",
    "            # for debug\n",
    "            #metrics = {}\n",
    "            #metrics['loss_grapheme'] =  F.cross_entropy(outputs[0], y[:, 0], reduction='mean').item()\n",
    "            #metrics['loss_vowel'] =  F.cross_entropy(outputs[1], y[:, 1], reduction='mean').item()\n",
    "            #metrics['loss_consonant'] =  F.cross_entropy(outputs[2], y[:, 2], reduction='mean').item()\n",
    "            #return metrics\n",
    "    \n",
    "    preds0 = torch.cat(preds0, 0).cpu().numpy()\n",
    "    preds1 = torch.cat(preds1, 0).cpu().numpy()\n",
    "    preds2 = torch.cat(preds2, 0).cpu().numpy()\n",
    "    preds3 = torch.cat(preds3, 0).cpu().numpy()\n",
    "    \n",
    "    y_true = torch.cat(y_true, 0).numpy()\n",
    "    \n",
    "    #print('y_true:', y_true.shape)\n",
    "    #print('preds0:', preds0.shape)\n",
    "    \n",
    "    metrics = calc_metrics(preds0, preds1, preds2, preds3, y_true)\n",
    "    metrics['loss_grapheme'] = round(loss0 / val_loader.num, 6)\n",
    "    metrics['loss_vowel'] = round(loss1 / val_loader.num, 6)\n",
    "    metrics['loss_consonant'] = round(loss2 / val_loader.num, 6)\n",
    "    metrics['loss_word'] = round(loss3 / val_loader.num, 6)\n",
    "    \n",
    "    return metrics\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lrs(optimizer):\n",
    "    lrs = []\n",
    "    for pgs in optimizer.state_dict()['param_groups']:\n",
    "        lrs.append(pgs['lr'])\n",
    "    lrs = ['{:.6f}'.format(x) for x in lrs]\n",
    "    return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_file):\n",
    "    parent_dir = os.path.dirname(model_file)\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        torch.save(model.module.state_dict(), model_file)\n",
    "    else:\n",
    "        torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(data, targets, alpha=1):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets = targets[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    data = data * lam + shuffled_data * (1 - lam)\n",
    "    targets = (targets, shuffled_targets, lam)\n",
    "\n",
    "    return data, targets\n",
    "\n",
    "\n",
    "def mixup_criterion(outputs, targets):\n",
    "    targets1, targets2, lam = targets\n",
    "    #criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    return lam * criterion(outputs, targets1) + (1 - lam) * criterion(outputs, targets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox_new(size, lam):\n",
    "    H = size[2]\n",
    "    W = size[3]\n",
    "\n",
    "    x_margin_rate = 0.2\n",
    "\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * (1-x_margin_rate*2) * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "    \n",
    "    min_x_center = np.int(W * x_margin_rate + cut_w / 2)\n",
    "    max_x_center = np.int(W * (1-x_margin_rate) - cut_w / 2)\n",
    "    #print(min_x_center, max_x_center, lam, cut_w)\n",
    "    min_y_center = cut_h // 2\n",
    "    max_y_center = H - cut_h // 2\n",
    "    if max_y_center == min_y_center:\n",
    "        max_y_center += 1\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(min_x_center, max_x_center)\n",
    "    cy = np.random.randint(min_y_center, max_y_center)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    #print(bbx1, bbx2, bby1, bby2)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7521994223058489"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from over9000.over9000 import Over9000\n",
    "from over9000.radam import RAdam\n",
    "from gridmask import GridMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvcore.solver import WarmupCyclicalLR\n",
    "def make_optimizer(model, base_lr=4e-4, weight_decay=0., weight_decay_bias=0., epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    Create optimizer with per-layer learning rate and weight decay.\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    for key, value in model.named_parameters():\n",
    "        if not value.requires_grad:\n",
    "            continue\n",
    "        lr = base_lr\n",
    "        params += [{\"params\": [value], \"lr\": lr, \"weight_decay\": weight_decay_bias if 'bias' in key else weight_decay}]\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(params, lr, eps=epsilon)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_for_finetune(net):\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = False\n",
    "    if finetune_class == 'grapheme_root':\n",
    "        for param in net.fc_g.parameters():\n",
    "            param.requires_grad = True\n",
    "    elif finetune_class == 'vowel_diacritic':\n",
    "        for param in net.fc_v.parameters():\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        for param in net.fc_c.parameters():\n",
    "            param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_epoch(args, model, train_loader, epoch, optimizer, lr_scheduler, grid):\n",
    "    train_loss = 0\n",
    "    bg = time.time()\n",
    "\n",
    "    for batch_idx, (img, orig_img, targets) in enumerate(train_loader):\n",
    "        img, orig_img, targets  = img.cuda(), orig_img.cuda(), targets.cuda()\n",
    "        batch_size = img.size(0)\n",
    "        r = np.random.rand()\n",
    "\n",
    "        if True:\n",
    "            outputs, outputs_aux1, outputs_aux2 = model(orig_img)\n",
    "            loss = criterion(outputs, targets)\n",
    "            #loss_aux1 = criterion(outputs_aux1, targets)\n",
    "            #loss_aux2 = criterion(outputs_aux2, targets)\n",
    "            #loss = loss_primary + (loss_aux1 + loss_aux2)*0.8\n",
    "        elif r < 0.3:\n",
    "            # generate mixed sample\n",
    "            lam = np.random.beta(args.beta, args.beta)\n",
    "            rand_index = torch.randperm(img.size()[0]).cuda()\n",
    "            target_a = targets\n",
    "            target_b = targets[rand_index]\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(img.size(), lam)\n",
    "            #img[:, :, bby1:bby2, bbx1:bbx2] = img[rand_index, :, bby1:bby2, bbx1:bbx2] #for new cutmix\n",
    "            img[:, :, bbx1:bbx2, bby1:bby2] = img[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            \n",
    "            # adjust lambda to exactly match pixel ratio\n",
    "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (img.size()[-1] * img.size()[-2]))\n",
    "            # compute output\n",
    "            outputs, outputs_aux1, outputs_aux2 = model(img)\n",
    "            loss_primary = criterion(outputs, target_a) * lam + criterion(outputs, target_b) * (1. - lam)\n",
    "            loss_aux1 = criterion(outputs_aux1, target_a) * lam + criterion(outputs_aux1, target_b) * (1. - lam)\n",
    "            loss_aux2 = criterion(outputs_aux2, target_a) * lam + criterion(outputs_aux2, target_b) * (1. - lam)\n",
    "            loss = loss_primary + (loss_aux1 + loss_aux2)*0.8\n",
    "        elif r > 0.7:\n",
    "            img = grid(img)\n",
    "            outputs, outputs_aux1, outputs_aux2 = model(img)\n",
    "            loss_primary = criterion(outputs, targets)\n",
    "            loss_aux1 = criterion(outputs_aux1, targets)\n",
    "            loss_aux2 = criterion(outputs_aux2, targets)\n",
    "            loss = loss_primary + (loss_aux1 + loss_aux2)*0.8\n",
    "        else:\n",
    "            orig_img, targets = mixup(orig_img, targets)\n",
    "            outputs, outputs_aux1, outputs_aux2 = model(orig_img)\n",
    "            loss_primary = mixup_criterion(outputs, targets)\n",
    "            loss_aux1 = mixup_criterion(outputs_aux1, targets)\n",
    "            loss_aux2 = mixup_criterion(outputs_aux2, targets)\n",
    "            loss = loss_primary + (loss_aux1 + loss_aux2)*0.8\n",
    "            #loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        #loss.backward()\n",
    "        lr_scheduler(optimizer, batch_idx, epoch)\n",
    "        optimizer.step()            \n",
    "        \n",
    "        current_lr = get_lrs(optimizer)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        print('\\r {:4d} | {:.6f} | {:06d}/{} | {:.4f} | {:.4f} | {:.2f} |'.format(\n",
    "            epoch, float(current_lr[0]), batch_size*(batch_idx+1), train_loader.num, \n",
    "            loss.item(), train_loss/(batch_idx+1), (time.time() - bg) / 60), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "best_metrics = 0.\n",
    "best_metrics_swa = 0.\n",
    "\n",
    "\n",
    "def validate_and_save(model, model_file, val_loader, save=False):\n",
    "    global best_metrics\n",
    "    best_key = 'recall'\n",
    "    val_metrics = validate(model, val_loader)\n",
    "    print('\\nval:', val_metrics)\n",
    "    \n",
    "    if val_metrics[best_key] > best_metrics:\n",
    "        best_metrics = val_metrics[best_key]\n",
    "        if save:\n",
    "            save_model(model, model_file)\n",
    "            print('###>>>>> saved', model_file)\n",
    "    model.module.set_mode('train', is_freeze_bn=True)\n",
    "\n",
    "def validate_and_save_swa(model, model_file, val_loader, save=False):\n",
    "    global best_metrics_swa\n",
    "    best_key = 'recall'\n",
    "    val_metrics = validate(model, val_loader)\n",
    "    print('\\nval:', val_metrics)\n",
    "    \n",
    "    if val_metrics[best_key] > best_metrics_swa:\n",
    "        best_metrics_swa = val_metrics[best_key]\n",
    "        if save:\n",
    "            save_model(model, model_file)\n",
    "            print('###>>>>> saved', model_file)\n",
    "    model.module.set_mode('train', is_freeze_bn=True)\n",
    "\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    model, model_file = create_model(cfg)\n",
    "    model = model.cuda()\n",
    "    \n",
    "    freeze_for_finetune(model)\n",
    "\n",
    "    swa_cfg = copy.deepcopy(cfg)\n",
    "    swa_cfg.CKP_NAME = cfg.CKP_NAME + '_swa'\n",
    "    swa_model, swa_model_file = create_model(swa_cfg)\n",
    "    swa_model = swa_model.cuda()\n",
    "\n",
    "    optimizer = make_optimizer(model)\n",
    "    lr_scheduler = WarmupCyclicalLR(\n",
    "        \"cos\", args.base_lr, args.num_epochs, iters_per_epoch=len(train_loader), warmup_epochs=args.warmup_epochs)\n",
    "    \n",
    "    [model, swa_model], optimizer = amp.initialize([model, swa_model], optimizer, opt_level=\"O1\",verbosity=0)\n",
    "    #[model, swa_model], optimizer = amp.initialize(\n",
    "    #    [model, swa_model], optimizer, opt_level=\"O2\",verbosity=0, keep_batchnorm_fp32=True)\n",
    "    \n",
    "    #opt_level=\"O2\", keep_batchnorm_fp32=True\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "        swa_model = nn.DataParallel(swa_model)\n",
    "    \n",
    "    validate_and_save(model, model_file, val_loader, save=False)\n",
    "\n",
    "    swa_model_loaded = False\n",
    "    if os.path.exists(swa_model_file):\n",
    "        swa_model_loaded = True\n",
    "        validate_and_save_swa(swa_model, swa_model_file, val_loader, save=False)\n",
    "        \n",
    "    model.module.set_mode('train', is_freeze_bn=True)\n",
    "    \n",
    "    for cycle in range(1, args.num_cycles+1):\n",
    "        print('CYCLE:', cycle)\n",
    "        grid = GridMask(64, 128, rotate=15, ratio=0.6, mode=1, prob=1.)\n",
    "\n",
    "        for epoch in range(args.start_epoch, args.num_epochs):\n",
    "            grid.set_prob(epoch, args.st_epochs)\n",
    "            train_epoch(args, model, train_loader, epoch, optimizer, lr_scheduler, grid)\n",
    "            validate_and_save(model, model_file, val_loader, save=True)\n",
    "            \n",
    "            if (epoch+1) == args.swa_start and cycle == 1:\n",
    "                if not swa_model_loaded:\n",
    "                    copy_model(swa_model, model)\n",
    "                #swa_n = 0\n",
    "                swa_n = args.swa_n\n",
    "            if (epoch+1) >= args.swa_start and (epoch+1) % args.swa_freq == 0:\n",
    "                print('SWA>>>:')\n",
    "                moving_average(swa_model, model, 1.0 / (swa_n + 1))\n",
    "                swa_n += 1\n",
    "                bn_update(train_loader, swa_model)\n",
    "                validate_and_save_swa(swa_model, swa_model_file, val_loader, save=True)\n",
    "\n",
    "        #args.base_lr = 2e-4\n",
    "        #args.num_epochs = 40\n",
    "        #args.warmup_epochs = 1\n",
    "\n",
    "        # reset scheduler at each cycle\n",
    "        lr_scheduler = WarmupCyclicalLR(\n",
    "            \"cos\", args.base_lr, args.num_epochs, iters_per_epoch=len(train_loader), warmup_epochs=args.warmup_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model, model_file = create_model(cfg)\n",
    "#model(torch.randn(2,1,137,236))[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "\n",
    "args.base_lr = 3e-4\n",
    "args.num_epochs = 50\n",
    "args.start_epoch = 0\n",
    "args.warmup_epochs = 5\n",
    "args.num_cycles = 100\n",
    "args.batch_size = 800\n",
    "args.val_batch_size = 800\n",
    "args.st_epochs = 5\n",
    "\n",
    "args.swa_start = 3000\n",
    "args.swa_freq = 3\n",
    "args.swa_n = 5\n",
    "\n",
    "args.beta = 1.0\n",
    "args.cutmix_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 2\n",
      "(200840, 6)\n",
      "(200840, 32332)\n",
      "(160678, 6) (40162, 6)\n",
      "sampler length: torch.Size([160678])\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_train_val_loaders(batch_size=args.batch_size, val_batch_size=args.val_batch_size, ifold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model file: ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth, exist: True\n",
      "loading ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth...\n",
      "model file: ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth_swa, exist: False\n",
      "\n",
      "val: {'recall': 0.998852, 'recall_grapheme': 0.998329, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998171, 'old_recall_grapheme': 0.997711, 'loss_grapheme': 0.073741, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "CYCLE: 1\n",
      "    0 | 0.000060 | 136278/160678 | 0.1901 | 0.1916 | 2.35 |\n",
      "val: {'recall': 0.99875, 'recall_grapheme': 0.998124, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998127, 'old_recall_grapheme': 0.997623, 'loss_grapheme': 0.069903, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "    1 | 0.000119 | 136278/160678 | 0.1750 | 0.1805 | 2.33 |\n",
      "val: {'recall': 0.998734, 'recall_grapheme': 0.998092, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998099, 'old_recall_grapheme': 0.997567, 'loss_grapheme': 0.057675, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "    2 | 0.000178 | 136278/160678 | 0.1529 | 0.1640 | 2.33 |\n",
      "val: {'recall': 0.998686, 'recall_grapheme': 0.997996, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998006, 'old_recall_grapheme': 0.997381, 'loss_grapheme': 0.04385, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "    3 | 0.000236 | 136278/160678 | 0.1437 | 0.1506 | 2.34 |\n",
      "val: {'recall': 0.998708, 'recall_grapheme': 0.998041, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998026, 'old_recall_grapheme': 0.99742, 'loss_grapheme': 0.032999, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "    4 | 0.000292 | 136278/160678 | 0.1392 | 0.1415 | 2.33 |\n",
      "val: {'recall': 0.998708, 'recall_grapheme': 0.998041, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998026, 'old_recall_grapheme': 0.99742, 'loss_grapheme': 0.025614, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "    5 | 0.000289 | 136278/160678 | 0.1365 | 0.1357 | 2.33 |\n",
      "val: {'recall': 0.998699, 'recall_grapheme': 0.998022, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998026, 'old_recall_grapheme': 0.99742, 'loss_grapheme': 0.021055, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "    6 | 0.000286 | 136278/160678 | 0.1337 | 0.1320 | 2.34 |\n",
      "val: {'recall': 0.998699, 'recall_grapheme': 0.998022, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998026, 'old_recall_grapheme': 0.99742, 'loss_grapheme': 0.018292, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "    8 | 0.000277 | 136278/160678 | 0.1291 | 0.1285 | 2.32 |\n",
      "val: {'recall': 0.998689, 'recall_grapheme': 0.998003, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998016, 'old_recall_grapheme': 0.997401, 'loss_grapheme': 0.015179, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "    9 | 0.000271 | 136278/160678 | 0.1273 | 0.1274 | 2.34 |\n",
      "val: {'recall': 0.998689, 'recall_grapheme': 0.998003, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998056, 'old_recall_grapheme': 0.997481, 'loss_grapheme': 0.014236, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "   10 | 0.000266 | 136278/160678 | 0.1266 | 0.1266 | 2.33 |\n",
      "val: {'recall': 0.998671, 'recall_grapheme': 0.997966, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998047, 'old_recall_grapheme': 0.997464, 'loss_grapheme': 0.013517, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "   11 | 0.000259 | 136278/160678 | 0.1226 | 0.1259 | 2.34 |\n",
      "val: {'recall': 0.998656, 'recall_grapheme': 0.997936, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.997983, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998044, 'old_recall_grapheme': 0.997457, 'loss_grapheme': 0.012955, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "   12 | 0.000253 | 136278/160678 | 0.1255 | 0.1256 | 2.33 |\n",
      "val: {'recall': 0.998656, 'recall_grapheme': 0.997936, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.997983, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998044, 'old_recall_grapheme': 0.997457, 'loss_grapheme': 0.012509, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "   13 | 0.000246 | 136278/160678 | 0.1255 | 0.1251 | 2.34 |\n",
      "val: {'recall': 0.99866, 'recall_grapheme': 0.997943, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998063, 'old_recall_grapheme': 0.997495, 'loss_grapheme': 0.012142, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "   14 | 0.000238 | 136278/160678 | 0.1221 | 0.1250 | 2.34 |\n",
      "val: {'recall': 0.99866, 'recall_grapheme': 0.997943, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998063, 'old_recall_grapheme': 0.997495, 'loss_grapheme': 0.011848, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "   15 | 0.000230 | 136278/160678 | 0.1270 | 0.1244 | 2.35 |\n",
      "val: {'recall': 0.99866, 'recall_grapheme': 0.997943, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998063, 'old_recall_grapheme': 0.997495, 'loss_grapheme': 0.011603, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "   16 | 0.000222 | 136278/160678 | 0.1224 | 0.1243 | 2.33 |\n",
      "val: {'recall': 0.99866, 'recall_grapheme': 0.997943, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998063, 'old_recall_grapheme': 0.997495, 'loss_grapheme': 0.011391, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "   17 | 0.000214 | 136278/160678 | 0.1251 | 0.1240 | 2.34 |\n",
      "val: {'recall': 0.99866, 'recall_grapheme': 0.997943, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998063, 'old_recall_grapheme': 0.997495, 'loss_grapheme': 0.011214, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   18 | 0.000205 | 136278/160678 | 0.1264 | 0.1239 | 2.33 |\n",
      "val: {'recall': 0.99866, 'recall_grapheme': 0.997943, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998063, 'old_recall_grapheme': 0.997495, 'loss_grapheme': 0.011061, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "   19 | 0.000196 | 136278/160678 | 0.1262 | 0.1240 | 2.34 |\n",
      "val: {'recall': 0.99866, 'recall_grapheme': 0.997943, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998063, 'old_recall_grapheme': 0.997495, 'loss_grapheme': 0.010931, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "   20 | 0.000187 | 136278/160678 | 0.1246 | 0.1237 | 2.33 |\n",
      "val: {'recall': 0.99866, 'recall_grapheme': 0.997943, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998063, 'old_recall_grapheme': 0.997495, 'loss_grapheme': 0.010818, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "   21 | 0.000178 | 136278/160678 | 0.1212 | 0.1236 | 2.36 |\n",
      "val: {'recall': 0.99866, 'recall_grapheme': 0.997943, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998063, 'old_recall_grapheme': 0.997495, 'loss_grapheme': 0.010721, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "   22 | 0.000169 | 136278/160678 | 0.1277 | 0.1234 | 2.34 |\n",
      "val: {'recall': 0.99866, 'recall_grapheme': 0.997943, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998063, 'old_recall_grapheme': 0.997495, 'loss_grapheme': 0.010637, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "   23 | 0.000159 | 136278/160678 | 0.1233 | 0.1234 | 2.34 |\n",
      "val: {'recall': 0.99866, 'recall_grapheme': 0.997943, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998063, 'old_recall_grapheme': 0.997495, 'loss_grapheme': 0.010563, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "   24 | 0.000150 | 136278/160678 | 0.1242 | 0.1237 | 2.34 |\n",
      "val: {'recall': 0.99866, 'recall_grapheme': 0.997943, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998063, 'old_recall_grapheme': 0.997495, 'loss_grapheme': 0.010498, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "   25 | 0.000141 | 136278/160678 | 0.1253 | 0.1230 | 2.32 |\n",
      "val: {'recall': 0.99866, 'recall_grapheme': 0.997943, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998063, 'old_recall_grapheme': 0.997495, 'loss_grapheme': 0.010441, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "   26 | 0.000131 | 136278/160678 | 0.1222 | 0.1233 | 2.34 |\n",
      "val: {'recall': 0.99866, 'recall_grapheme': 0.997943, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998063, 'old_recall_grapheme': 0.997495, 'loss_grapheme': 0.01039, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "   27 | 0.000122 | 136278/160678 | 0.1209 | 0.1231 | 2.35 |\n",
      "val: {'recall': 0.99866, 'recall_grapheme': 0.997943, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998063, 'old_recall_grapheme': 0.997495, 'loss_grapheme': 0.010346, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "   28 | 0.000116 | 100000/160678 | 0.1241 | 0.1228 | 1.48 |"
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model file: ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth, exist: True\n",
      "loading ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth...\n",
      "model file: ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth_swa, exist: False\n",
      "\n",
      "val: {'recall': 0.998718, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998469, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "CYCLE: 1\n",
      "    0 | 0.000080 | 136278/160678 | 25.2408 | 15.6283 | 2.39 |\n",
      "val: {'recall': 0.998719, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.999015, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999402, 'acc_word': 0.998083, 'old_recall': 0.998477, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.003126, 'loss_word': 0.010276}\n",
      "###>>>>> saved ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth\n",
      "    1 | 0.000159 | 136278/160678 | 0.2835 | 15.4150 | 2.55 ||\n",
      "val: {'recall': 0.998721, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.999021, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999427, 'acc_word': 0.998083, 'old_recall': 0.998486, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.004036, 'loss_word': 0.010276}\n",
      "###>>>>> saved ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth\n",
      "    2 | 0.000237 | 136278/160678 | 18.2782 | 13.1934 | 2.71 |\n",
      "val: {'recall': 0.998722, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.999027, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998494, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.014371, 'loss_word': 0.010276}\n",
      "###>>>>> saved ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth\n",
      "    3 | 0.000315 | 136278/160678 | 31.4623 | 14.9218 | 2.73 |\n",
      "val: {'recall': 0.998711, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998981, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998476, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.04196, 'loss_word': 0.010276}\n",
      "    4 | 0.000390 | 136278/160678 | 0.3777 | 14.7415 | 2.79 ||\n",
      "val: {'recall': 0.998719, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.999015, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999402, 'acc_word': 0.998083, 'old_recall': 0.998477, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.056104, 'loss_word': 0.010276}\n",
      "    5 | 0.000386 | 136278/160678 | 7.6586 | 14.0738 | 3.00 ||\n",
      "val: {'recall': 0.998711, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998981, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998207, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.063862, 'loss_word': 0.010276}\n",
      "    6 | 0.000381 | 136278/160678 | 24.7352 | 14.6045 | 3.01 |\n",
      "val: {'recall': 0.998719, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.999015, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999402, 'acc_word': 0.998083, 'old_recall': 0.998477, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.062263, 'loss_word': 0.010276}\n",
      "    7 | 0.000375 | 136278/160678 | 28.0238 | 13.3470 | 3.03 |\n",
      "val: {'recall': 0.998721, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.999021, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999427, 'acc_word': 0.998083, 'old_recall': 0.998486, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.059115, 'loss_word': 0.010276}\n",
      "    8 | 0.000369 | 136278/160678 | 2.4117 | 15.1631 | 2.98 ||\n",
      "val: {'recall': 0.998719, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.999015, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999402, 'acc_word': 0.998083, 'old_recall': 0.998477, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.072025, 'loss_word': 0.010276}\n",
      "    9 | 0.000362 | 136278/160678 | 0.4577 | 12.9248 | 3.04 ||\n",
      "val: {'recall': 0.998719, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.999015, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999402, 'acc_word': 0.998083, 'old_recall': 0.998477, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.059212, 'loss_word': 0.010276}\n",
      "   10 | 0.000354 | 136278/160678 | 23.8696 | 15.1247 | 2.93 |\n",
      "val: {'recall': 0.998712, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998987, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999402, 'acc_word': 0.998083, 'old_recall': 0.998215, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.070819, 'loss_word': 0.010276}\n",
      "   11 | 0.000346 | 136278/160678 | 32.0288 | 13.0885 | 3.13 |\n",
      "val: {'recall': 0.998711, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998981, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998207, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.0557, 'loss_word': 0.010276}\n",
      "   12 | 0.000337 | 136278/160678 | 3.5403 | 16.0451 | 2.88 ||\n",
      "val: {'recall': 0.998711, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998981, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998207, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.069426, 'loss_word': 0.010276}\n",
      "   13 | 0.000328 | 136278/160678 | 29.4980 | 13.9166 | 3.02 |\n",
      "val: {'recall': 0.998714, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998993, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999427, 'acc_word': 0.998083, 'old_recall': 0.998224, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.062689, 'loss_word': 0.010276}\n",
      "   14 | 0.000318 | 136278/160678 | 25.8573 | 14.5222 | 2.98 |\n",
      "val: {'recall': 0.998712, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998987, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999402, 'acc_word': 0.998083, 'old_recall': 0.998215, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.05892, 'loss_word': 0.010276}\n",
      "   15 | 0.000307 | 136278/160678 | 0.4891 | 13.3755 | 2.96 ||\n",
      "val: {'recall': 0.998714, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998993, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999427, 'acc_word': 0.998083, 'old_recall': 0.998224, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.052483, 'loss_word': 0.010276}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   16 | 0.000296 | 136278/160678 | 25.4478 | 14.9934 | 2.99 |\n",
      "val: {'recall': 0.998704, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998953, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998214, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.057064, 'loss_word': 0.010276}\n",
      "   17 | 0.000285 | 136278/160678 | 29.3571 | 14.1757 | 3.03 |\n",
      "val: {'recall': 0.998712, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998987, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999402, 'acc_word': 0.998083, 'old_recall': 0.998215, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.070712, 'loss_word': 0.010276}\n",
      "   18 | 0.000274 | 136278/160678 | 20.0697 | 14.4028 | 2.90 |\n",
      "val: {'recall': 0.998697, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998925, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998221, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.064751, 'loss_word': 0.010276}\n",
      "   19 | 0.000262 | 136278/160678 | 23.3137 | 14.2046 | 3.03 |\n",
      "val: {'recall': 0.998712, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998987, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999402, 'acc_word': 0.998083, 'old_recall': 0.998215, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.055743, 'loss_word': 0.010276}\n",
      "   20 | 0.000250 | 136278/160678 | 16.1868 | 16.0566 | 2.88 |\n",
      "val: {'recall': 0.998705, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998959, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999402, 'acc_word': 0.998083, 'old_recall': 0.998222, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.069899, 'loss_word': 0.010276}\n",
      "   21 | 0.000238 | 136278/160678 | 10.6970 | 14.5684 | 2.99 |\n",
      "val: {'recall': 0.998849, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.999536, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999452, 'acc_word': 0.998083, 'old_recall': 0.998232, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.058998, 'loss_word': 0.010276}\n",
      "###>>>>> saved ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth\n",
      "   22 | 0.000225 | 136278/160678 | 0.4232 | 16.4050 | 2.83 ||\n",
      "val: {'recall': 0.998714, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998993, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999427, 'acc_word': 0.998083, 'old_recall': 0.998224, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.066971, 'loss_word': 0.010276}\n",
      "   23 | 0.000213 | 136278/160678 | 28.3092 | 14.5743 | 3.00 |\n",
      "val: {'recall': 0.998712, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998987, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999402, 'acc_word': 0.998083, 'old_recall': 0.998215, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.063398, 'loss_word': 0.010276}\n",
      "   24 | 0.000200 | 136278/160678 | 3.7772 | 14.6980 | 2.99 ||\n",
      "val: {'recall': 0.998712, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998987, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999402, 'acc_word': 0.998083, 'old_recall': 0.998215, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.06265, 'loss_word': 0.010276}\n",
      "   25 | 0.000188 | 136278/160678 | 0.4978 | 15.6001 | 2.93 ||\n",
      "val: {'recall': 0.998714, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998993, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999427, 'acc_word': 0.998083, 'old_recall': 0.998224, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.068049, 'loss_word': 0.010276}\n",
      "   26 | 0.000175 | 136278/160678 | 24.5952 | 14.7084 | 3.02 |\n",
      "val: {'recall': 0.998714, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998993, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999427, 'acc_word': 0.998083, 'old_recall': 0.998224, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.061415, 'loss_word': 0.010276}\n",
      "   27 | 0.000163 | 136278/160678 | 0.3795 | 14.3560 | 3.05 ||\n",
      "val: {'recall': 0.998714, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998993, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999427, 'acc_word': 0.998083, 'old_recall': 0.998224, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.056724, 'loss_word': 0.010276}\n",
      "   28 | 0.000150 | 136278/160678 | 0.5024 | 14.3754 | 2.98 ||\n",
      "val: {'recall': 0.998714, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998993, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999427, 'acc_word': 0.998083, 'old_recall': 0.998224, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.061399, 'loss_word': 0.010276}\n",
      "   29 | 0.000138 | 136278/160678 | 11.8159 | 13.9730 | 3.02 |\n",
      "val: {'recall': 0.998714, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998993, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999427, 'acc_word': 0.998083, 'old_recall': 0.998224, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.056387, 'loss_word': 0.010276}\n",
      "   30 | 0.000126 | 136278/160678 | 0.5015 | 14.5085 | 3.01 ||\n",
      "val: {'recall': 0.998714, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998993, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999427, 'acc_word': 0.998083, 'old_recall': 0.998224, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.057481, 'loss_word': 0.010276}\n",
      "   31 | 0.000115 | 136278/160678 | 0.3759 | 14.7652 | 3.02 ||\n",
      "val: {'recall': 0.998714, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998993, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999427, 'acc_word': 0.998083, 'old_recall': 0.998224, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.062917, 'loss_word': 0.010276}\n",
      "   32 | 0.000104 | 136278/160678 | 25.4897 | 13.9000 | 3.06 |\n",
      "val: {'recall': 0.998714, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998993, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999427, 'acc_word': 0.998083, 'old_recall': 0.998224, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.057635, 'loss_word': 0.010276}\n",
      "   33 | 0.000093 | 136278/160678 | 33.8553 | 16.5384 | 2.90 |\n",
      "val: {'recall': 0.998714, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998993, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999427, 'acc_word': 0.998083, 'old_recall': 0.998224, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.064181, 'loss_word': 0.010276}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   34 | 0.000082 | 136278/160678 | 0.5742 | 14.9787 | 2.90 ||\n",
      "val: {'recall': 0.998714, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998993, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999427, 'acc_word': 0.998083, 'old_recall': 0.998224, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.06188, 'loss_word': 0.010276}\n",
      "   35 | 0.000073 | 136278/160678 | 21.1160 | 14.5406 | 2.90 |\n",
      "val: {'recall': 0.998714, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998993, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999427, 'acc_word': 0.998083, 'old_recall': 0.998224, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.0601, 'loss_word': 0.010276}\n",
      "   36 | 0.000063 | 136278/160678 | 0.5423 | 16.0416 | 2.96 ||\n",
      "val: {'recall': 0.998714, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.998993, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999427, 'acc_word': 0.998083, 'old_recall': 0.998224, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.066511, 'loss_word': 0.010276}\n",
      "   37 | 0.000059 | 074400/160678 | 0.4144 | 15.1027 | 1.42 ||"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-aad596905035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-9803fad4f99e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mvalidate_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-16e5a4d95497>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(args, model, train_loader, epoch, optimizer, lr_scheduler, grid)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;31m#loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/apex/amp/handle.py\u001b[0m in \u001b[0;36mscale_loss\u001b[0;34m(loss, optimizers, loss_id, model, delay_unscale, delay_overflow_check)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;31m# For future fused optimizers that enable sync-free dynamic loss scaling,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# should_skip will always be False.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mshould_skip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdelay_overflow_check\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mloss_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_skip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/apex/amp/scaler.py\u001b[0m in \u001b[0;36mupdate_scale\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# If the fused kernel is available, we only need one D2H memcopy and sync.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mLossScaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_fused_kernel\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_overflow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_overflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_overflow_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_overflow\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model file: ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth, exist: True\n",
      "loading ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth...\n",
      "model file: ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth_swa, exist: False\n",
      "\n",
      "val: {'recall': 0.998718, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998469, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.054251, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "CYCLE: 1\n",
      "    0 | 0.000199 | 136278/160678 | 23.2290 | 15.2467 | 2.33 |\n",
      "val: {'recall': 0.998718, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999216, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999328, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998469, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.056836, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    1 | 0.000397 | 136278/160678 | 0.2677 | 14.3982 | 2.49 ||\n",
      "val: {'recall': 0.998715, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999203, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999303, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998466, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.061623, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    2 | 0.000594 | 136278/160678 | 29.3893 | 14.9704 | 2.63 |\n",
      "val: {'recall': 0.998712, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999192, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999278, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998445, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.073981, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    3 | 0.000786 | 136278/160678 | 0.4581 | 16.0492 | 2.67 ||\n",
      "val: {'recall': 0.998707, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999175, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999278, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998462, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.066647, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    4 | 0.000975 | 136278/160678 | 10.4352 | 12.4994 | 2.89 |\n",
      "val: {'recall': 0.998715, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999203, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999303, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.99847, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.070103, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    5 | 0.000965 | 136278/160678 | 12.9806 | 13.7323 | 2.90 |\n",
      "val: {'recall': 0.998709, 'recall_grapheme': 0.998323, 'recall_vowel': 0.99918, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999253, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998475, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.067266, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    6 | 0.000952 | 136278/160678 | 0.4703 | 15.1099 | 2.92 ||\n",
      "val: {'recall': 0.998712, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999191, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999278, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998491, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.071927, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    7 | 0.000938 | 136278/160678 | 0.6357 | 15.0471 | 2.93 ||\n",
      "val: {'recall': 0.998677, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999054, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999178, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998481, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.080452, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    8 | 0.000922 | 136278/160678 | 14.5995 | 14.3334 | 2.96 |\n",
      "val: {'recall': 0.998716, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999207, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999278, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998496, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.068537, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    9 | 0.000905 | 136278/160678 | 0.3197 | 13.6913 | 3.05 ||\n",
      "val: {'recall': 0.998708, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999179, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999253, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998493, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.059793, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   10 | 0.000885 | 136278/160678 | 0.5163 | 14.0530 | 3.00 ||\n",
      "val: {'recall': 0.998701, 'recall_grapheme': 0.998323, 'recall_vowel': 0.99915, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998461, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.053948, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   11 | 0.000865 | 136278/160678 | 30.4119 | 16.4658 | 2.85 |\n",
      "val: {'recall': 0.998698, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999138, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999203, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998459, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.069521, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   12 | 0.000842 | 136278/160678 | 0.3777 | 13.4815 | 3.02 ||\n",
      "val: {'recall': 0.998699, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999139, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999203, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.99844, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.064819, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   13 | 0.000819 | 136278/160678 | 29.0979 | 13.5572 | 3.03 |\n",
      "val: {'recall': 0.998709, 'recall_grapheme': 0.998323, 'recall_vowel': 0.99918, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999253, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998475, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.070423, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   14 | 0.000794 | 136278/160678 | 17.4016 | 13.9176 | 2.97 |\n",
      "val: {'recall': 0.998702, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999152, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998443, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.070474, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   15 | 0.000775 | 120800/160678 | 3.1316 | 14.7543 | 2.24 ||"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-aad596905035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-9803fad4f99e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mvalidate_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-16e5a4d95497>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(args, model, train_loader, epoch, optimizer, lr_scheduler, grid)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;31m#loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/apex/amp/handle.py\u001b[0m in \u001b[0;36mscale_loss\u001b[0;34m(loss, optimizers, loss_id, model, delay_unscale, delay_overflow_check)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;31m# For future fused optimizers that enable sync-free dynamic loss scaling,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# should_skip will always be False.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mshould_skip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdelay_overflow_check\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mloss_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_skip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/apex/amp/scaler.py\u001b[0m in \u001b[0;36mupdate_scale\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# If the fused kernel is available, we only need one D2H memcopy and sync.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mLossScaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_fused_kernel\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_overflow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_overflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_overflow_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_overflow\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model file: ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth, exist: True\n",
      "loading ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth...\n",
      "model file: ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth_swa, exist: False\n",
      "\n",
      "val: {'recall': 0.998569, 'recall_grapheme': 0.998084, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998157, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998636, 'old_recall_grapheme': 0.998185, 'loss_grapheme': 0.009142, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "CYCLE: 1\n",
      "    0 | 0.000080 | 136278/160678 | 4.0573 | 15.8032 | 2.39 ||\n",
      "val: {'recall': 0.998569, 'recall_grapheme': 0.998084, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998157, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998636, 'old_recall_grapheme': 0.998185, 'loss_grapheme': 0.009136, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    1 | 0.000159 | 136278/160678 | 11.9623 | 15.1033 | 2.50 |\n",
      "val: {'recall': 0.998533, 'recall_grapheme': 0.998011, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998425, 'old_recall_grapheme': 0.997763, 'loss_grapheme': 0.009151, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    2 | 0.000237 | 136278/160678 | 28.2527 | 15.6488 | 2.65 |\n",
      "val: {'recall': 0.998505, 'recall_grapheme': 0.997956, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.9984, 'old_recall_grapheme': 0.997712, 'loss_grapheme': 0.009342, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    3 | 0.000315 | 136278/160678 | 0.1761 | 12.6072 | 2.82 ||\n",
      "val: {'recall': 0.998505, 'recall_grapheme': 0.997957, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998547, 'old_recall_grapheme': 0.998007, 'loss_grapheme': 0.010234, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    4 | 0.000390 | 136278/160678 | 21.0479 | 13.0327 | 2.91 |\n",
      "val: {'recall': 0.998454, 'recall_grapheme': 0.997855, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.997983, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998443, 'old_recall_grapheme': 0.997798, 'loss_grapheme': 0.015415, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    5 | 0.000386 | 136278/160678 | 0.2933 | 15.0104 | 2.89 ||\n",
      "val: {'recall': 0.998564, 'recall_grapheme': 0.998073, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998554, 'old_recall_grapheme': 0.99802, 'loss_grapheme': 0.029964, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    6 | 0.000381 | 136278/160678 | 0.3191 | 14.4983 | 2.94 ||\n",
      "val: {'recall': 0.998555, 'recall_grapheme': 0.998056, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998494, 'old_recall_grapheme': 0.997901, 'loss_grapheme': 0.044912, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    7 | 0.000375 | 136278/160678 | 0.3948 | 13.7906 | 3.01 ||\n",
      "val: {'recall': 0.998547, 'recall_grapheme': 0.99804, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998417, 'old_recall_grapheme': 0.997746, 'loss_grapheme': 0.05259, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    8 | 0.000369 | 136278/160678 | 0.3907 | 15.3891 | 2.91 ||\n",
      "val: {'recall': 0.998519, 'recall_grapheme': 0.997984, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998394, 'old_recall_grapheme': 0.9977, 'loss_grapheme': 0.059359, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    9 | 0.000362 | 136278/160678 | 22.2663 | 14.5257 | 2.91 |\n",
      "val: {'recall': 0.998534, 'recall_grapheme': 0.998014, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998385, 'old_recall_grapheme': 0.997682, 'loss_grapheme': 0.062125, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   10 | 0.000354 | 136278/160678 | 0.3039 | 13.7581 | 3.07 ||\n",
      "val: {'recall': 0.998576, 'recall_grapheme': 0.998098, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998395, 'old_recall_grapheme': 0.997704, 'loss_grapheme': 0.063023, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "###>>>>> saved ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth\n",
      "   11 | 0.000346 | 136278/160678 | 20.6182 | 14.8924 | 2.92 |\n",
      "val: {'recall': 0.998567, 'recall_grapheme': 0.998081, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998405, 'old_recall_grapheme': 0.997722, 'loss_grapheme': 0.067707, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   12 | 0.000337 | 136278/160678 | 6.1263 | 12.9251 | 2.99 ||\n",
      "val: {'recall': 0.998586, 'recall_grapheme': 0.998118, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998413, 'old_recall_grapheme': 0.997739, 'loss_grapheme': 0.065366, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "###>>>>> saved ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth\n",
      "   13 | 0.000328 | 136278/160678 | 16.5613 | 15.2113 | 3.01 |\n",
      "val: {'recall': 0.998593, 'recall_grapheme': 0.998133, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998384, 'old_recall_grapheme': 0.997681, 'loss_grapheme': 0.069096, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "###>>>>> saved ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth\n",
      "   14 | 0.000318 | 136278/160678 | 25.9133 | 14.4141 | 2.94 |\n",
      "val: {'recall': 0.998553, 'recall_grapheme': 0.998052, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998365, 'old_recall_grapheme': 0.997643, 'loss_grapheme': 0.070566, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   15 | 0.000307 | 136278/160678 | 0.2016 | 12.6275 | 3.09 ||\n",
      "val: {'recall': 0.998564, 'recall_grapheme': 0.998074, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998314, 'old_recall_grapheme': 0.99754, 'loss_grapheme': 0.065981, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   16 | 0.000296 | 136278/160678 | 8.2989 | 13.7876 | 2.97 ||\n",
      "val: {'recall': 0.998595, 'recall_grapheme': 0.998137, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998367, 'old_recall_grapheme': 0.997646, 'loss_grapheme': 0.06877, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "###>>>>> saved ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth\n",
      "   17 | 0.000285 | 136278/160678 | 30.1506 | 13.7530 | 3.00 |\n",
      "val: {'recall': 0.998592, 'recall_grapheme': 0.99813, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998391, 'old_recall_grapheme': 0.997694, 'loss_grapheme': 0.0693, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   18 | 0.000274 | 136278/160678 | 0.3690 | 13.6842 | 3.01 ||\n",
      "val: {'recall': 0.998623, 'recall_grapheme': 0.998192, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998157, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998435, 'old_recall_grapheme': 0.997782, 'loss_grapheme': 0.069107, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "###>>>>> saved ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth\n",
      "   19 | 0.000262 | 136278/160678 | 0.1724 | 14.6815 | 2.94 ||\n",
      "val: {'recall': 0.998618, 'recall_grapheme': 0.998181, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998157, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998394, 'old_recall_grapheme': 0.997701, 'loss_grapheme': 0.072453, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   20 | 0.000250 | 136278/160678 | 0.4639 | 14.8027 | 2.93 ||\n",
      "val: {'recall': 0.998618, 'recall_grapheme': 0.998183, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998157, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998388, 'old_recall_grapheme': 0.997689, 'loss_grapheme': 0.074197, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   21 | 0.000238 | 136278/160678 | 21.6008 | 13.1694 | 3.03 |\n",
      "val: {'recall': 0.998574, 'recall_grapheme': 0.998095, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998344, 'old_recall_grapheme': 0.997601, 'loss_grapheme': 0.071386, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   22 | 0.000225 | 136278/160678 | 0.9405 | 13.2973 | 3.01 ||\n",
      "val: {'recall': 0.998593, 'recall_grapheme': 0.998131, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998343, 'old_recall_grapheme': 0.997599, 'loss_grapheme': 0.071169, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   23 | 0.000213 | 136278/160678 | 0.2360 | 12.9708 | 3.12 ||\n",
      "val: {'recall': 0.998531, 'recall_grapheme': 0.998009, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998332, 'old_recall_grapheme': 0.997576, 'loss_grapheme': 0.069311, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   24 | 0.000200 | 136278/160678 | 7.0421 | 13.5394 | 3.02 ||\n",
      "val: {'recall': 0.998614, 'recall_grapheme': 0.998175, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998133, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998375, 'old_recall_grapheme': 0.997663, 'loss_grapheme': 0.069643, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   25 | 0.000188 | 136278/160678 | 0.4364 | 13.7703 | 2.93 ||\n",
      "val: {'recall': 0.998602, 'recall_grapheme': 0.99815, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998337, 'old_recall_grapheme': 0.997586, 'loss_grapheme': 0.071997, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   26 | 0.000175 | 136278/160678 | 22.5849 | 13.3733 | 3.00 |\n",
      "val: {'recall': 0.998631, 'recall_grapheme': 0.998209, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998133, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998397, 'old_recall_grapheme': 0.997706, 'loss_grapheme': 0.070638, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "###>>>>> saved ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth\n",
      "   27 | 0.000163 | 136278/160678 | 7.8619 | 12.1400 | 3.12 ||\n",
      "val: {'recall': 0.998561, 'recall_grapheme': 0.998067, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.99837, 'old_recall_grapheme': 0.997653, 'loss_grapheme': 0.067719, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   28 | 0.000150 | 136278/160678 | 3.7624 | 15.4913 | 2.85 ||\n",
      "val: {'recall': 0.998595, 'recall_grapheme': 0.998135, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998389, 'old_recall_grapheme': 0.997691, 'loss_grapheme': 0.074017, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   29 | 0.000138 | 136278/160678 | 22.0712 | 14.8624 | 2.89 |\n",
      "val: {'recall': 0.99854, 'recall_grapheme': 0.998027, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998325, 'old_recall_grapheme': 0.997563, 'loss_grapheme': 0.076823, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   30 | 0.000126 | 136278/160678 | 13.6740 | 15.0061 | 2.96 |\n",
      "val: {'recall': 0.998577, 'recall_grapheme': 0.9981, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998367, 'old_recall_grapheme': 0.997647, 'loss_grapheme': 0.078104, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   31 | 0.000115 | 136278/160678 | 31.2150 | 13.9350 | 2.91 |\n",
      "val: {'recall': 0.998577, 'recall_grapheme': 0.9981, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998367, 'old_recall_grapheme': 0.997647, 'loss_grapheme': 0.076447, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   32 | 0.000104 | 136278/160678 | 18.4306 | 14.9234 | 2.91 |\n",
      "val: {'recall': 0.99857, 'recall_grapheme': 0.998087, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998385, 'old_recall_grapheme': 0.997684, 'loss_grapheme': 0.077352, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   33 | 0.000093 | 136278/160678 | 25.9438 | 12.9796 | 3.07 |\n",
      "val: {'recall': 0.998574, 'recall_grapheme': 0.998095, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998344, 'old_recall_grapheme': 0.997601, 'loss_grapheme': 0.074564, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   34 | 0.000082 | 136278/160678 | 0.2963 | 13.7355 | 3.04 ||\n",
      "val: {'recall': 0.998618, 'recall_grapheme': 0.998183, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998393, 'old_recall_grapheme': 0.997698, 'loss_grapheme': 0.074909, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   35 | 0.000073 | 136278/160678 | 0.2655 | 14.5378 | 2.94 ||\n",
      "val: {'recall': 0.998589, 'recall_grapheme': 0.998124, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998332, 'old_recall_grapheme': 0.997577, 'loss_grapheme': 0.075812, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   36 | 0.000063 | 136278/160678 | 0.2605 | 14.9781 | 2.94 ||\n",
      "val: {'recall': 0.998585, 'recall_grapheme': 0.998116, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998374, 'old_recall_grapheme': 0.997661, 'loss_grapheme': 0.076941, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   37 | 0.000054 | 136278/160678 | 32.8332 | 14.4541 | 2.96 |\n",
      "val: {'recall': 0.998609, 'recall_grapheme': 0.998164, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998377, 'old_recall_grapheme': 0.997667, 'loss_grapheme': 0.076348, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   38 | 0.000046 | 136278/160678 | 14.1742 | 13.5235 | 2.98 |\n",
      "val: {'recall': 0.998618, 'recall_grapheme': 0.998183, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998393, 'old_recall_grapheme': 0.997698, 'loss_grapheme': 0.075933, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   39 | 0.000038 | 136278/160678 | 0.3611 | 13.6216 | 3.05 ||\n",
      "val: {'recall': 0.998609, 'recall_grapheme': 0.998164, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998377, 'old_recall_grapheme': 0.997667, 'loss_grapheme': 0.075013, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   40 | 0.000031 | 136278/160678 | 6.9814 | 14.6703 | 2.90 ||\n",
      "val: {'recall': 0.998609, 'recall_grapheme': 0.998164, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998377, 'old_recall_grapheme': 0.997667, 'loss_grapheme': 0.075434, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   41 | 0.000025 | 136278/160678 | 31.5920 | 14.0138 | 2.95 |\n",
      "val: {'recall': 0.998609, 'recall_grapheme': 0.998164, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998377, 'old_recall_grapheme': 0.997667, 'loss_grapheme': 0.075347, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   42 | 0.000019 | 136278/160678 | 24.2822 | 13.8503 | 3.03 |\n",
      "val: {'recall': 0.998609, 'recall_grapheme': 0.998164, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998377, 'old_recall_grapheme': 0.997667, 'loss_grapheme': 0.075195, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   43 | 0.000014 | 136278/160678 | 33.1279 | 12.1167 | 3.10 |\n",
      "val: {'recall': 0.998609, 'recall_grapheme': 0.998164, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998377, 'old_recall_grapheme': 0.997667, 'loss_grapheme': 0.074593, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   44 | 0.000010 | 136278/160678 | 26.2159 | 14.1418 | 2.98 |\n",
      "val: {'recall': 0.998618, 'recall_grapheme': 0.998183, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998393, 'old_recall_grapheme': 0.997698, 'loss_grapheme': 0.074596, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   45 | 0.000006 | 136278/160678 | 19.0141 | 15.7774 | 2.83 |\n",
      "val: {'recall': 0.998618, 'recall_grapheme': 0.998183, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998393, 'old_recall_grapheme': 0.997698, 'loss_grapheme': 0.075026, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   46 | 0.000004 | 136278/160678 | 4.9078 | 14.4820 | 2.89 ||\n",
      "val: {'recall': 0.998618, 'recall_grapheme': 0.998183, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998393, 'old_recall_grapheme': 0.997698, 'loss_grapheme': 0.075118, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   47 | 0.000002 | 136278/160678 | 8.8858 | 14.6483 | 3.02 ||\n",
      "val: {'recall': 0.998618, 'recall_grapheme': 0.998183, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998393, 'old_recall_grapheme': 0.997698, 'loss_grapheme': 0.075128, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   48 | 0.000000 | 136278/160678 | 21.3201 | 14.1581 | 3.04 |\n",
      "val: {'recall': 0.998618, 'recall_grapheme': 0.998183, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998393, 'old_recall_grapheme': 0.997698, 'loss_grapheme': 0.075123, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   49 | 0.000000 | 136278/160678 | 22.4138 | 13.8733 | 3.02 |\n",
      "val: {'recall': 0.998618, 'recall_grapheme': 0.998183, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998393, 'old_recall_grapheme': 0.997698, 'loss_grapheme': 0.075123, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "CYCLE: 2\n",
      "    0 | 0.000080 | 136278/160678 | 9.9480 | 15.3798 | 2.37 ||\n",
      "val: {'recall': 0.998564, 'recall_grapheme': 0.998075, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998329, 'old_recall_grapheme': 0.99757, 'loss_grapheme': 0.07646, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    1 | 0.000159 | 136278/160678 | 8.8296 | 14.1888 | 2.50 ||\n",
      "val: {'recall': 0.998574, 'recall_grapheme': 0.998094, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998344, 'old_recall_grapheme': 0.9976, 'loss_grapheme': 0.07708, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2 | 0.000237 | 136278/160678 | 30.3277 | 15.6537 | 2.62 |\n",
      "val: {'recall': 0.998531, 'recall_grapheme': 0.998009, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998332, 'old_recall_grapheme': 0.997576, 'loss_grapheme': 0.080637, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    3 | 0.000315 | 136278/160678 | 0.2062 | 13.3680 | 2.84 ||\n",
      "val: {'recall': 0.998548, 'recall_grapheme': 0.998043, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998328, 'old_recall_grapheme': 0.997569, 'loss_grapheme': 0.079007, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    4 | 0.000390 | 136278/160678 | 12.9330 | 14.9282 | 2.81 |\n",
      "val: {'recall': 0.998668, 'recall_grapheme': 0.998282, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998402, 'old_recall_grapheme': 0.997717, 'loss_grapheme': 0.080895, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "###>>>>> saved ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth\n",
      "    5 | 0.000386 | 136278/160678 | 26.9963 | 12.6401 | 3.15 |\n",
      "val: {'recall': 0.998568, 'recall_grapheme': 0.998081, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.99838, 'old_recall_grapheme': 0.997672, 'loss_grapheme': 0.069596, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    6 | 0.000381 | 136278/160678 | 0.3602 | 13.4007 | 3.04 ||\n",
      "val: {'recall': 0.99863, 'recall_grapheme': 0.998207, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.997983, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998372, 'old_recall_grapheme': 0.997658, 'loss_grapheme': 0.07301, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    7 | 0.000375 | 136278/160678 | 21.2614 | 14.6993 | 2.93 |\n",
      "val: {'recall': 0.99855, 'recall_grapheme': 0.998046, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.99834, 'old_recall_grapheme': 0.997593, 'loss_grapheme': 0.078395, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    8 | 0.000369 | 136278/160678 | 12.8059 | 12.4079 | 3.07 |\n",
      "val: {'recall': 0.998642, 'recall_grapheme': 0.998231, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998404, 'old_recall_grapheme': 0.997721, 'loss_grapheme': 0.070844, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    9 | 0.000362 | 136278/160678 | 30.8351 | 15.4850 | 2.91 |\n",
      "val: {'recall': 0.998515, 'recall_grapheme': 0.997977, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.997983, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998337, 'old_recall_grapheme': 0.997586, 'loss_grapheme': 0.082087, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   10 | 0.000354 | 136278/160678 | 35.7248 | 13.0611 | 3.04 |\n",
      "val: {'recall': 0.998579, 'recall_grapheme': 0.998103, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998376, 'old_recall_grapheme': 0.997665, 'loss_grapheme': 0.073148, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   11 | 0.000346 | 136278/160678 | 0.2631 | 15.8020 | 2.96 ||\n",
      "val: {'recall': 0.998518, 'recall_grapheme': 0.997981, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.997983, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998321, 'old_recall_grapheme': 0.997555, 'loss_grapheme': 0.082064, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   12 | 0.000337 | 136278/160678 | 0.2358 | 15.6455 | 2.89 ||\n",
      "val: {'recall': 0.99857, 'recall_grapheme': 0.998086, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998336, 'old_recall_grapheme': 0.997584, 'loss_grapheme': 0.086083, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   13 | 0.000328 | 136278/160678 | 24.7870 | 15.2185 | 2.94 |\n",
      "val: {'recall': 0.998541, 'recall_grapheme': 0.998027, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998318, 'old_recall_grapheme': 0.997548, 'loss_grapheme': 0.080653, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   14 | 0.000318 | 136278/160678 | 0.3148 | 13.2627 | 2.97 ||\n",
      "val: {'recall': 0.998569, 'recall_grapheme': 0.998084, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998379, 'old_recall_grapheme': 0.99767, 'loss_grapheme': 0.076742, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   15 | 0.000307 | 136278/160678 | 5.6284 | 13.3543 | 2.97 ||\n",
      "val: {'recall': 0.998688, 'recall_grapheme': 0.998323, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.99846, 'old_recall_grapheme': 0.997833, 'loss_grapheme': 0.075838, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "###>>>>> saved ./model4-finetune-ckps/tf_efficientnet_b4/model4_finetune_eb4_fold2_380.pth\n",
      "   16 | 0.000296 | 136278/160678 | 0.2434 | 14.9389 | 2.91 ||\n",
      "val: {'recall': 0.998677, 'recall_grapheme': 0.9983, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.9984, 'old_recall_grapheme': 0.997713, 'loss_grapheme': 0.082292, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   17 | 0.000285 | 136278/160678 | 25.7285 | 14.3629 | 2.93 |\n",
      "val: {'recall': 0.998627, 'recall_grapheme': 0.998201, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998357, 'old_recall_grapheme': 0.997626, 'loss_grapheme': 0.083331, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   18 | 0.000274 | 136278/160678 | 0.2390 | 14.4047 | 2.94 ||\n",
      "val: {'recall': 0.998531, 'recall_grapheme': 0.998009, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.997983, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998322, 'old_recall_grapheme': 0.997556, 'loss_grapheme': 0.081705, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   19 | 0.000262 | 136278/160678 | 0.1929 | 13.6656 | 2.91 ||\n",
      "val: {'recall': 0.998575, 'recall_grapheme': 0.998097, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998366, 'old_recall_grapheme': 0.997645, 'loss_grapheme': 0.07896, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20 | 0.000250 | 136278/160678 | 20.1562 | 14.6483 | 2.87 |\n",
      "val: {'recall': 0.99851, 'recall_grapheme': 0.997966, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.997958, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998312, 'old_recall_grapheme': 0.997536, 'loss_grapheme': 0.082721, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   21 | 0.000238 | 136278/160678 | 8.9609 | 12.7421 | 2.99 ||\n",
      "val: {'recall': 0.998561, 'recall_grapheme': 0.998068, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998319, 'old_recall_grapheme': 0.99755, 'loss_grapheme': 0.075522, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   22 | 0.000225 | 136278/160678 | 20.9471 | 14.4950 | 3.03 |\n",
      "val: {'recall': 0.998503, 'recall_grapheme': 0.997953, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.997933, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998289, 'old_recall_grapheme': 0.99749, 'loss_grapheme': 0.079403, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   23 | 0.000213 | 136278/160678 | 0.3575 | 14.0575 | 2.98 ||\n",
      "val: {'recall': 0.99854, 'recall_grapheme': 0.998027, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.997983, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998346, 'old_recall_grapheme': 0.997604, 'loss_grapheme': 0.078927, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   24 | 0.000200 | 136278/160678 | 12.8564 | 14.8032 | 2.89 |\n",
      "val: {'recall': 0.998568, 'recall_grapheme': 0.998083, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998308, 'old_recall_grapheme': 0.997528, 'loss_grapheme': 0.08171, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   25 | 0.000188 | 136278/160678 | 9.5608 | 14.9748 | 2.88 ||\n",
      "val: {'recall': 0.998673, 'recall_grapheme': 0.998292, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998398, 'old_recall_grapheme': 0.99771, 'loss_grapheme': 0.082198, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   26 | 0.000175 | 136278/160678 | 0.3592 | 13.9989 | 3.04 ||\n",
      "val: {'recall': 0.998671, 'recall_grapheme': 0.998288, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998423, 'old_recall_grapheme': 0.997759, 'loss_grapheme': 0.079198, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   27 | 0.000163 | 136278/160678 | 21.2654 | 14.3983 | 3.00 |\n",
      "val: {'recall': 0.998679, 'recall_grapheme': 0.998304, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998397, 'old_recall_grapheme': 0.997706, 'loss_grapheme': 0.078952, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   28 | 0.000150 | 136278/160678 | 23.1456 | 14.1411 | 2.97 |\n",
      "val: {'recall': 0.998671, 'recall_grapheme': 0.998288, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998405, 'old_recall_grapheme': 0.997723, 'loss_grapheme': 0.078619, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   29 | 0.000138 | 136278/160678 | 33.7685 | 13.7068 | 3.01 |\n",
      "val: {'recall': 0.998647, 'recall_grapheme': 0.998241, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.99842, 'old_recall_grapheme': 0.997752, 'loss_grapheme': 0.076865, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   30 | 0.000126 | 136278/160678 | 0.2417 | 12.9400 | 3.02 ||\n",
      "val: {'recall': 0.998552, 'recall_grapheme': 0.998049, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998367, 'old_recall_grapheme': 0.997646, 'loss_grapheme': 0.075407, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   31 | 0.000115 | 136278/160678 | 30.1924 | 13.7647 | 2.88 |\n",
      "val: {'recall': 0.998568, 'recall_grapheme': 0.998083, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998387, 'old_recall_grapheme': 0.997686, 'loss_grapheme': 0.07642, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   32 | 0.000104 | 136278/160678 | 30.2843 | 13.9980 | 3.05 |\n",
      "val: {'recall': 0.998657, 'recall_grapheme': 0.998261, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998395, 'old_recall_grapheme': 0.997703, 'loss_grapheme': 0.076411, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   33 | 0.000093 | 136278/160678 | 11.8589 | 12.6537 | 3.07 |\n",
      "val: {'recall': 0.998651, 'recall_grapheme': 0.998248, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998378, 'old_recall_grapheme': 0.997668, 'loss_grapheme': 0.074554, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   34 | 0.000082 | 136278/160678 | 31.5553 | 15.4272 | 2.92 |\n",
      "val: {'recall': 0.998677, 'recall_grapheme': 0.998301, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998428, 'old_recall_grapheme': 0.997769, 'loss_grapheme': 0.077074, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   35 | 0.000073 | 136278/160678 | 0.1410 | 16.4961 | 2.81 ||\n",
      "val: {'recall': 0.99867, 'recall_grapheme': 0.998287, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998431, 'old_recall_grapheme': 0.997775, 'loss_grapheme': 0.081797, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   36 | 0.000063 | 136278/160678 | 15.1370 | 14.7674 | 2.95 |\n",
      "val: {'recall': 0.998558, 'recall_grapheme': 0.998062, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998372, 'old_recall_grapheme': 0.997656, 'loss_grapheme': 0.081373, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   37 | 0.000054 | 136278/160678 | 4.2109 | 15.5216 | 2.94 ||\n",
      "val: {'recall': 0.998661, 'recall_grapheme': 0.998268, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998416, 'old_recall_grapheme': 0.997745, 'loss_grapheme': 0.082464, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   38 | 0.000046 | 136278/160678 | 0.3236 | 15.3179 | 2.91 ||\n",
      "val: {'recall': 0.998558, 'recall_grapheme': 0.998062, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998372, 'old_recall_grapheme': 0.997656, 'loss_grapheme': 0.083137, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   39 | 0.000038 | 136278/160678 | 0.2519 | 13.5016 | 3.00 ||\n",
      "val: {'recall': 0.998558, 'recall_grapheme': 0.998062, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998372, 'old_recall_grapheme': 0.997656, 'loss_grapheme': 0.081925, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   40 | 0.000031 | 136278/160678 | 0.4168 | 13.9223 | 2.97 ||\n",
      "val: {'recall': 0.998538, 'recall_grapheme': 0.998022, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998327, 'old_recall_grapheme': 0.997566, 'loss_grapheme': 0.081384, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   41 | 0.000025 | 136278/160678 | 32.0973 | 14.3714 | 2.92 |\n",
      "val: {'recall': 0.998568, 'recall_grapheme': 0.998081, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998387, 'old_recall_grapheme': 0.997686, 'loss_grapheme': 0.081042, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   42 | 0.000019 | 136278/160678 | 17.8562 | 15.5025 | 2.87 |\n",
      "val: {'recall': 0.998561, 'recall_grapheme': 0.998069, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998382, 'old_recall_grapheme': 0.997676, 'loss_grapheme': 0.081485, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   43 | 0.000014 | 136278/160678 | 8.9478 | 14.0299 | 2.96 ||\n",
      "val: {'recall': 0.998541, 'recall_grapheme': 0.998028, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998337, 'old_recall_grapheme': 0.997586, 'loss_grapheme': 0.081362, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   44 | 0.000010 | 136278/160678 | 25.5003 | 14.8820 | 2.96 |\n",
      "val: {'recall': 0.998561, 'recall_grapheme': 0.998069, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998382, 'old_recall_grapheme': 0.997676, 'loss_grapheme': 0.081418, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   45 | 0.000006 | 136278/160678 | 24.2085 | 13.6024 | 3.03 |\n",
      "val: {'recall': 0.998541, 'recall_grapheme': 0.998028, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998337, 'old_recall_grapheme': 0.997586, 'loss_grapheme': 0.081246, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   46 | 0.000004 | 136278/160678 | 22.4905 | 13.5665 | 3.05 |\n",
      "val: {'recall': 0.998541, 'recall_grapheme': 0.998028, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998337, 'old_recall_grapheme': 0.997586, 'loss_grapheme': 0.081109, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   47 | 0.000002 | 136278/160678 | 0.2017 | 14.0200 | 2.99 ||\n",
      "val: {'recall': 0.998541, 'recall_grapheme': 0.998028, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998337, 'old_recall_grapheme': 0.997586, 'loss_grapheme': 0.081057, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   48 | 0.000000 | 136278/160678 | 0.8547 | 13.9836 | 3.06 ||\n",
      "val: {'recall': 0.998531, 'recall_grapheme': 0.998009, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.997983, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998322, 'old_recall_grapheme': 0.997556, 'loss_grapheme': 0.081048, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   49 | 0.000000 | 136278/160678 | 0.2732 | 13.6086 | 2.98 ||\n",
      "val: {'recall': 0.998531, 'recall_grapheme': 0.998009, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.997983, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998322, 'old_recall_grapheme': 0.997556, 'loss_grapheme': 0.081044, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "CYCLE: 3\n",
      "    0 | 0.000080 | 136278/160678 | 2.9251 | 14.2528 | 2.39 ||\n",
      "val: {'recall': 0.998644, 'recall_grapheme': 0.998234, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998381, 'old_recall_grapheme': 0.997675, 'loss_grapheme': 0.080727, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    1 | 0.000159 | 136278/160678 | 6.3724 | 12.9303 | 2.57 ||\n",
      "val: {'recall': 0.998575, 'recall_grapheme': 0.998096, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998366, 'old_recall_grapheme': 0.997644, 'loss_grapheme': 0.077603, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    2 | 0.000237 | 136278/160678 | 31.8355 | 15.2389 | 2.65 |\n",
      "val: {'recall': 0.998558, 'recall_grapheme': 0.998062, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998372, 'old_recall_grapheme': 0.997656, 'loss_grapheme': 0.080218, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    3 | 0.000315 | 136278/160678 | 17.9026 | 14.7991 | 2.75 |\n",
      "val: {'recall': 0.998558, 'recall_grapheme': 0.998062, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.99838, 'old_recall_grapheme': 0.997672, 'loss_grapheme': 0.080474, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    4 | 0.000390 | 136278/160678 | 16.2799 | 13.6527 | 2.84 |\n",
      "val: {'recall': 0.998512, 'recall_grapheme': 0.99797, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.997933, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998307, 'old_recall_grapheme': 0.997526, 'loss_grapheme': 0.079323, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    5 | 0.000386 | 136278/160678 | 30.4723 | 14.7205 | 3.00 |\n",
      "val: {'recall': 0.998552, 'recall_grapheme': 0.998051, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998349, 'old_recall_grapheme': 0.997611, 'loss_grapheme': 0.079667, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    6 | 0.000381 | 136278/160678 | 16.0056 | 13.5474 | 2.99 |\n",
      "val: {'recall': 0.998597, 'recall_grapheme': 0.998141, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998378, 'old_recall_grapheme': 0.997668, 'loss_grapheme': 0.078177, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    7 | 0.000375 | 136278/160678 | 17.7240 | 14.7480 | 2.86 |\n",
      "val: {'recall': 0.998682, 'recall_grapheme': 0.998311, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998108, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998424, 'old_recall_grapheme': 0.99776, 'loss_grapheme': 0.081427, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    8 | 0.000369 | 136278/160678 | 0.2201 | 15.2353 | 2.89 ||\n",
      "val: {'recall': 0.998662, 'recall_grapheme': 0.998271, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998434, 'old_recall_grapheme': 0.99778, 'loss_grapheme': 0.081052, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    9 | 0.000362 | 136278/160678 | 0.1676 | 13.4261 | 3.04 ||\n",
      "val: {'recall': 0.998669, 'recall_grapheme': 0.998283, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998445, 'old_recall_grapheme': 0.997802, 'loss_grapheme': 0.076363, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   10 | 0.000354 | 136278/160678 | 0.1739 | 14.4223 | 2.93 ||\n",
      "val: {'recall': 0.998592, 'recall_grapheme': 0.99813, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998373, 'old_recall_grapheme': 0.997658, 'loss_grapheme': 0.08134, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   11 | 0.000346 | 136278/160678 | 31.0533 | 12.7355 | 3.15 |\n",
      "val: {'recall': 0.998683, 'recall_grapheme': 0.998312, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.99841, 'old_recall_grapheme': 0.997733, 'loss_grapheme': 0.073689, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   12 | 0.000337 | 136278/160678 | 0.3281 | 15.9033 | 2.85 ||\n",
      "val: {'recall': 0.998679, 'recall_grapheme': 0.998304, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998397, 'old_recall_grapheme': 0.997706, 'loss_grapheme': 0.084326, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   13 | 0.000328 | 136278/160678 | 0.3978 | 14.8873 | 2.86 ||\n",
      "val: {'recall': 0.998558, 'recall_grapheme': 0.998062, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998349, 'old_recall_grapheme': 0.99761, 'loss_grapheme': 0.083813, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   14 | 0.000318 | 136278/160678 | 0.2571 | 13.2462 | 3.00 ||\n",
      "val: {'recall': 0.998558, 'recall_grapheme': 0.998062, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998372, 'old_recall_grapheme': 0.997656, 'loss_grapheme': 0.078159, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   15 | 0.000307 | 136278/160678 | 31.5701 | 14.7424 | 2.94 |\n",
      "val: {'recall': 0.998568, 'recall_grapheme': 0.998081, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998369, 'old_recall_grapheme': 0.99765, 'loss_grapheme': 0.082839, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   16 | 0.000296 | 136278/160678 | 12.5111 | 13.8451 | 2.98 |\n",
      "val: {'recall': 0.998533, 'recall_grapheme': 0.998012, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.997983, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.99834, 'old_recall_grapheme': 0.997593, 'loss_grapheme': 0.080181, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   17 | 0.000285 | 136278/160678 | 0.3094 | 15.1032 | 3.02 ||\n",
      "val: {'recall': 0.998557, 'recall_grapheme': 0.998061, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998344, 'old_recall_grapheme': 0.997601, 'loss_grapheme': 0.082956, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   18 | 0.000274 | 136278/160678 | 0.1304 | 13.6103 | 3.01 ||\n",
      "val: {'recall': 0.998674, 'recall_grapheme': 0.998294, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998386, 'old_recall_grapheme': 0.997685, 'loss_grapheme': 0.078096, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   19 | 0.000262 | 136278/160678 | 26.6236 | 13.8967 | 3.03 |\n",
      "val: {'recall': 0.998558, 'recall_grapheme': 0.998062, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998372, 'old_recall_grapheme': 0.997656, 'loss_grapheme': 0.077349, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   20 | 0.000257 | 065600/160678 | 0.2583 | 13.4683 | 1.27 ||\n",
      "val: {'recall': 0.99867, 'recall_grapheme': 0.998287, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998437, 'old_recall_grapheme': 0.997786, 'loss_grapheme': 0.078193, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   21 | 0.000238 | 136278/160678 | 28.0128 | 15.2830 | 2.92 |\n",
      "val: {'recall': 0.998542, 'recall_grapheme': 0.99803, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.997983, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998364, 'old_recall_grapheme': 0.99764, 'loss_grapheme': 0.085509, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   22 | 0.000225 | 136278/160678 | 0.2855 | 14.1759 | 3.01 ||\n",
      "val: {'recall': 0.998548, 'recall_grapheme': 0.998043, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998369, 'old_recall_grapheme': 0.997651, 'loss_grapheme': 0.081555, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   23 | 0.000213 | 136278/160678 | 0.2558 | 14.2337 | 2.98 ||\n",
      "val: {'recall': 0.99854, 'recall_grapheme': 0.998026, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.997983, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998367, 'old_recall_grapheme': 0.997647, 'loss_grapheme': 0.079786, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   24 | 0.000200 | 136278/160678 | 0.3532 | 13.9333 | 3.03 ||\n",
      "val: {'recall': 0.998542, 'recall_grapheme': 0.99803, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.997983, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998364, 'old_recall_grapheme': 0.99764, 'loss_grapheme': 0.077712, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   25 | 0.000188 | 136278/160678 | 0.2674 | 14.1839 | 3.02 ||\n",
      "val: {'recall': 0.998575, 'recall_grapheme': 0.998096, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998384, 'old_recall_grapheme': 0.99768, 'loss_grapheme': 0.078234, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   26 | 0.000175 | 136278/160678 | 13.7438 | 14.7381 | 2.94 |\n",
      "val: {'recall': 0.998671, 'recall_grapheme': 0.998289, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998444, 'old_recall_grapheme': 0.997801, 'loss_grapheme': 0.081209, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   27 | 0.000163 | 136278/160678 | 1.1115 | 15.4621 | 2.90 ||\n",
      "val: {'recall': 0.998543, 'recall_grapheme': 0.998032, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.997983, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998354, 'old_recall_grapheme': 0.99762, 'loss_grapheme': 0.085401, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   28 | 0.000150 | 136278/160678 | 20.1342 | 14.0228 | 3.00 |\n",
      "val: {'recall': 0.998565, 'recall_grapheme': 0.998076, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998008, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.99836, 'old_recall_grapheme': 0.997633, 'loss_grapheme': 0.081548, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   29 | 0.000138 | 136278/160678 | 3.6915 | 14.0670 | 2.91 ||\n",
      "val: {'recall': 0.998571, 'recall_grapheme': 0.998089, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998365, 'old_recall_grapheme': 0.997643, 'loss_grapheme': 0.081934, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   30 | 0.000126 | 136278/160678 | 32.7678 | 14.3580 | 3.06 |\n",
      "val: {'recall': 0.998649, 'recall_grapheme': 0.998244, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998413, 'old_recall_grapheme': 0.997739, 'loss_grapheme': 0.0799, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   31 | 0.000115 | 136278/160678 | 32.7152 | 12.9619 | 3.01 |\n",
      "val: {'recall': 0.998556, 'recall_grapheme': 0.998059, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998375, 'old_recall_grapheme': 0.997663, 'loss_grapheme': 0.077801, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   32 | 0.000104 | 136278/160678 | 0.1960 | 14.2783 | 3.01 ||\n",
      "val: {'recall': 0.998556, 'recall_grapheme': 0.998059, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998375, 'old_recall_grapheme': 0.997663, 'loss_grapheme': 0.078396, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   33 | 0.000093 | 136278/160678 | 31.0924 | 13.8144 | 3.01 |\n",
      "val: {'recall': 0.998652, 'recall_grapheme': 0.998251, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998415, 'old_recall_grapheme': 0.997742, 'loss_grapheme': 0.077984, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   34 | 0.000082 | 136278/160678 | 0.3897 | 15.1045 | 2.90 ||\n",
      "val: {'recall': 0.998659, 'recall_grapheme': 0.998264, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.99842, 'old_recall_grapheme': 0.997752, 'loss_grapheme': 0.080597, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   35 | 0.000073 | 136278/160678 | 16.5170 | 14.1989 | 2.98 |\n",
      "val: {'recall': 0.998659, 'recall_grapheme': 0.998264, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.99842, 'old_recall_grapheme': 0.997752, 'loss_grapheme': 0.080249, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   36 | 0.000063 | 136278/160678 | 0.4154 | 14.3052 | 2.95 ||\n",
      "val: {'recall': 0.998659, 'recall_grapheme': 0.998264, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.99842, 'old_recall_grapheme': 0.997752, 'loss_grapheme': 0.080709, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   37 | 0.000054 | 136278/160678 | 0.2692 | 12.5261 | 3.08 ||\n",
      "val: {'recall': 0.998677, 'recall_grapheme': 0.998301, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998428, 'old_recall_grapheme': 0.997769, 'loss_grapheme': 0.078166, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   38 | 0.000046 | 136278/160678 | 24.0164 | 14.2716 | 2.94 |\n",
      "val: {'recall': 0.998575, 'recall_grapheme': 0.998096, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998384, 'old_recall_grapheme': 0.99768, 'loss_grapheme': 0.078649, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   39 | 0.000038 | 136278/160678 | 0.2427 | 15.0308 | 2.99 ||\n",
      "val: {'recall': 0.998652, 'recall_grapheme': 0.998251, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998415, 'old_recall_grapheme': 0.997742, 'loss_grapheme': 0.079992, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   40 | 0.000031 | 136278/160678 | 29.9385 | 14.1280 | 3.00 |\n",
      "val: {'recall': 0.998593, 'recall_grapheme': 0.998132, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998382, 'old_recall_grapheme': 0.997677, 'loss_grapheme': 0.07965, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   41 | 0.000025 | 136278/160678 | 18.6251 | 15.1882 | 2.92 |\n",
      "val: {'recall': 0.998575, 'recall_grapheme': 0.998096, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998384, 'old_recall_grapheme': 0.99768, 'loss_grapheme': 0.080402, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   42 | 0.000019 | 136278/160678 | 0.3857 | 14.4606 | 3.02 ||\n",
      "val: {'recall': 0.998677, 'recall_grapheme': 0.998301, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998083, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998428, 'old_recall_grapheme': 0.997769, 'loss_grapheme': 0.08045, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   43 | 0.000014 | 136278/160678 | 16.6158 | 13.8270 | 3.00 |\n",
      "val: {'recall': 0.998575, 'recall_grapheme': 0.998096, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998384, 'old_recall_grapheme': 0.99768, 'loss_grapheme': 0.080202, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   44 | 0.000010 | 136278/160678 | 27.1567 | 14.6760 | 2.96 |\n",
      "val: {'recall': 0.998575, 'recall_grapheme': 0.998096, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998384, 'old_recall_grapheme': 0.99768, 'loss_grapheme': 0.080269, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   45 | 0.000006 | 136278/160678 | 23.4396 | 13.7624 | 3.00 |\n",
      "val: {'recall': 0.998575, 'recall_grapheme': 0.998096, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998384, 'old_recall_grapheme': 0.99768, 'loss_grapheme': 0.080223, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   46 | 0.000004 | 136278/160678 | 6.5425 | 14.8818 | 2.93 ||\n",
      "val: {'recall': 0.998575, 'recall_grapheme': 0.998096, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998384, 'old_recall_grapheme': 0.99768, 'loss_grapheme': 0.080321, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   47 | 0.000002 | 136278/160678 | 0.2679 | 14.2388 | 2.96 ||\n",
      "val: {'recall': 0.998575, 'recall_grapheme': 0.998096, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998384, 'old_recall_grapheme': 0.99768, 'loss_grapheme': 0.080328, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   48 | 0.000000 | 136278/160678 | 26.3966 | 14.2957 | 2.95 |\n",
      "val: {'recall': 0.998575, 'recall_grapheme': 0.998096, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998384, 'old_recall_grapheme': 0.99768, 'loss_grapheme': 0.080323, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "   49 | 0.000000 | 136278/160678 | 11.4871 | 13.2754 | 3.03 |\n",
      "val: {'recall': 0.998575, 'recall_grapheme': 0.998096, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998384, 'old_recall_grapheme': 0.99768, 'loss_grapheme': 0.080318, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "CYCLE: 4\n",
      "    0 | 0.000080 | 136278/160678 | 0.2038 | 14.2156 | 2.38 ||\n",
      "val: {'recall': 0.998556, 'recall_grapheme': 0.998059, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998033, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998375, 'old_recall_grapheme': 0.997663, 'loss_grapheme': 0.081061, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    1 | 0.000159 | 136278/160678 | 0.2224 | 12.9973 | 2.56 ||\n",
      "val: {'recall': 0.998575, 'recall_grapheme': 0.998096, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998384, 'old_recall_grapheme': 0.99768, 'loss_grapheme': 0.078203, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    2 | 0.000237 | 136278/160678 | 16.2251 | 14.7596 | 2.67 |\n",
      "val: {'recall': 0.998661, 'recall_grapheme': 0.998268, 'recall_vowel': 0.999098, 'recall_consonant': 0.999009, 'recall_word': 0.998056, 'acc_grapheme': 0.998058, 'acc_vowel': 0.999228, 'acc_consonant': 0.999378, 'acc_word': 0.998083, 'old_recall': 0.998416, 'old_recall_grapheme': 0.997745, 'loss_grapheme': 0.083468, 'loss_vowel': 0.004679, 'loss_consonant': 0.003201, 'loss_word': 0.010276}\n",
      "    3 | 0.000244 | 013600/160678 | 32.5749 | 12.0084 | 0.30 |"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-aad596905035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-9803fad4f99e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mvalidate_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-16e5a4d95497>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(args, model, train_loader, epoch, optimizer, lr_scheduler, grid)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mlam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbx2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbbx1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbby2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbby1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# compute output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_aux1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_aux2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mloss_primary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_a\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_b\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mloss_aux1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_aux1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_a\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_aux1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_b\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0m_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_tup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model file: ./model4-ckps/tf_efficientnet_b4/model4_eb4_fold2_380_swa_cv998693.pth, exist: True\n",
      "loading ./model4-ckps/tf_efficientnet_b4/model4_eb4_fold2_380_swa_cv998693.pth...\n",
      "model file: ./model4-ckps/tf_efficientnet_b4/model4_eb4_fold2_380_swa_cv998693.pth_swa, exist: False\n",
      "\n",
      "val: {'recall': 0.998507, 'recall_grapheme': 0.998022, 'recall_vowel': 0.99908, 'recall_consonant': 0.998903, 'recall_word': 0.998056, 'acc_grapheme': 0.998133, 'acc_vowel': 0.999228, 'acc_consonant': 0.999427, 'acc_word': 0.998083, 'old_recall': 0.998693, 'old_recall_grapheme': 0.998173, 'loss_grapheme': 0.009206, 'loss_vowel': 0.004678, 'loss_consonant': 0.002865, 'loss_word': 0.010286}\n",
      "CYCLE: 1\n",
      "    0 | 0.000001 | 002432/160678 | 13.8956 | 5.7486 | 0.26 |"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-aad596905035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-bb65010759cc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mvalidate_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-16e5a4d95497>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(args, model, train_loader, epoch, optimizer, lr_scheduler, grid)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;31m#loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
