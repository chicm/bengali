{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time, gc\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pretrainedmodels\n",
    "from argparse import Namespace\n",
    "from sklearn.utils import shuffle\n",
    "from apex import amp\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:            \n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "\n",
    "class PlainRAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "\n",
    "        super(PlainRAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(PlainRAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                beta2_t = beta2 ** state['step']\n",
    "                N_sma_max = 2 / (1 - beta2) - 1\n",
    "                N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:                    \n",
    "                    step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "                else:\n",
    "                    step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    p_data_fp32.add_(-step_size, exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, warmup = 0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, warmup = warmup)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(AdamW, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                \n",
    "                if group['warmup'] > state['step']:\n",
    "                    scheduled_lr = 1e-8 + state['step'] * group['lr'] / group['warmup']\n",
    "                else:\n",
    "                    scheduled_lr = group['lr']\n",
    "\n",
    "                step_size = scheduled_lr * math.sqrt(bias_correction2) / bias_correction1\n",
    "                \n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * scheduled_lr, p_data_fp32)\n",
    "\n",
    "                p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bengaliai-cv19.zip\t   test_image_data_3.parquet\r\n",
      "class_map.csv\t\t   train.csv\r\n",
      "sample_submission.csv\t   train_image_data_0.parquet\r\n",
      "test.csv\t\t   train_image_data_1.parquet\r\n",
      "test_image_data_0.parquet  train_image_data_2.parquet\r\n",
      "test_image_data_1.parquet  train_image_data_3.parquet\r\n",
      "test_image_data_2.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls /mnt/chicm/data/bengali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /home/chec/data/bengali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/mnt/chicm/data/bengali'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "class_map_df = pd.read_csv(f'{DATA_DIR}/class_map.csv')\n",
    "sample_sub_df = pd.read_csv(f'{DATA_DIR}/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Train_0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>ক্ট্রো</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Train_1</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>হ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Train_2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>খ্রী</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Train_3</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>র্টি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Train_4</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>থ্রো</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  grapheme_root  vowel_diacritic  consonant_diacritic grapheme\n",
       "0  Train_0             15                9                    5   ক্ট্রো\n",
       "1  Train_1            159                0                    0        হ\n",
       "2  Train_2             22                3                    5     খ্রী\n",
       "3  Train_3             53                2                    2     র্টি\n",
       "4  Train_4             71                9                    5     থ্রো"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as albu\n",
    "\n",
    "def get_train_augs(p=1.):\n",
    "    return albu.Compose([\n",
    "        #albu.HorizontalFlip(.5),\n",
    "        albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.5 ),\n",
    "        albu.Blur(blur_limit=3, p=0.3),\n",
    "        albu.OpticalDistortion(p=0.3),\n",
    "        albu.GaussNoise(p=0.3)\n",
    "        #albu.GridDistortion(p=.33),\n",
    "        #albu.HueSaturationValue(p=.33) # not for grey scale\n",
    "    ], p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "\n",
    "class BengaliDataset(Dataset):\n",
    "    def __init__(self, df, img_df, train_mode=True, test_mode=False):\n",
    "        self.df = df\n",
    "        self.img_df = img_df\n",
    "        self.train_mode = train_mode\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = self.get_img(row.image_id)\n",
    "        #print(img.shape)\n",
    "        if self.train_mode:\n",
    "            augs = get_train_augs()\n",
    "            img = augs(image=img)['image']\n",
    "        \n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "        \n",
    "        img = transforms.functional.to_tensor(img)\n",
    "        #img = transforms.functional.normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "        if self.test_mode:\n",
    "            return img\n",
    "        else:\n",
    "            return img, torch.tensor([row.grapheme_root, row.vowel_diacritic, row.consonant_diacritic])\n",
    "\n",
    "    def get_img(self, img_id):\n",
    "        return 255 - self.img_df.loc[img_id].values.reshape(HEIGHT, WIDTH).astype(np.uint8)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "def get_train_val_loaders(batch_size=4, val_batch_size=4, ifold=0, dev_mode=False):\n",
    "    train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "    train_df = shuffle(train_df, random_state=1234)\n",
    "    print(train_df.shape)\n",
    "\n",
    "    if dev_mode:\n",
    "        img_df = pd.read_parquet(f'{DATA_DIR}/train_image_data_0.parquet').set_index('image_id')\n",
    "        train_df = train_df.iloc[:1000]\n",
    "    else:\n",
    "        img_dfs = [pd.read_parquet(f'{DATA_DIR}/train_image_data_{i}.parquet') for i in range(4)]\n",
    "        img_df = pd.concat(img_dfs, axis=0).set_index('image_id')\n",
    "    print(img_df.shape)\n",
    "    #split_index = int(len(train_df) * 0.9)\n",
    "    \n",
    "    #train = train_df.iloc[:split_index]\n",
    "    #val = train_df.iloc[split_index:]\n",
    "    \n",
    "    kf = StratifiedKFold(5, random_state=1234, shuffle=True)\n",
    "    for i, (train_idx, val_idx) in enumerate(kf.split(train_df, train_df['grapheme_root'].values)):\n",
    "        if i == ifold:\n",
    "            #print(val_idx)\n",
    "            train = train_df.iloc[train_idx]\n",
    "            val = train_df.iloc[val_idx]\n",
    "            break\n",
    "    assert i == ifold\n",
    "    print(train.shape, val.shape)\n",
    "    \n",
    "    train_ds = BengaliDataset(train, img_df, True, False)\n",
    "    val_ds = BengaliDataset(val, img_df, False, False)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "    train_loader.num = len(train_ds)\n",
    "\n",
    "    val_loader = DataLoader(val_ds, batch_size=val_batch_size, shuffle=False, num_workers=8, drop_last=False)\n",
    "    val_loader.num = len(val_ds)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader, val_loader = get_train_val_loaders(dev_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fbresnet152', 'bninception', 'resnext101_32x4d', 'resnext101_64x4d', 'inceptionv4', 'inceptionresnetv2', 'alexnet', 'densenet121', 'densenet169', 'densenet201', 'densenet161', 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'inceptionv3', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19_bn', 'vgg19', 'nasnetamobile', 'nasnetalarge', 'dpn68', 'dpn68b', 'dpn92', 'dpn98', 'dpn131', 'dpn107', 'xception', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152', 'se_resnext50_32x4d', 'se_resnext101_32x4d', 'cafferesnet101', 'pnasnet5large', 'polynet']\n"
     ]
    }
   ],
   "source": [
    "print(pretrainedmodels.model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'resnet50' # could be fbresnet152 or inceptionresnetv2\n",
    "#model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet').cuda()\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained=False).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.features(torch.randn((2, 3, 137, 236)).cuda()).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.last_linear.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliNet_1(nn.Module):\n",
    "    def __init__(self, backbone_name):\n",
    "        super(BengaliNet, self).__init__()\n",
    "        self.n_grapheme = 168\n",
    "        self.n_vowel = 11\n",
    "        self.n_consonant = 7\n",
    "        self.backbone_name = backbone_name\n",
    "        \n",
    "        self.num_classes = self.n_grapheme + self.n_vowel + self.n_consonant\n",
    "        \n",
    "        self.conv0 = nn.Conv2d(1, 3, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        if self.backbone_name.startswith('efficient'):\n",
    "            self.backbone = EfficientNet.from_pretrained(self.backbone_name)\n",
    "            self.fc = nn.Linear(self.backbone._fc.in_features, self.num_classes)\n",
    "        else:\n",
    "            self.backbone = pretrainedmodels.__dict__[self.backbone_name](num_classes=1000, pretrained='imagenet')\n",
    "            self.fc = nn.Linear(self.backbone.last_linear.in_features, self.num_classes)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "    def logits(self, x):\n",
    "        x = self.avg_pool(x)\n",
    "        x = F.dropout2d(x, 0.2, self.training)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        #print(x.size())\n",
    "        if self.backbone_name.startswith('efficient'):\n",
    "            x = self.backbone.extract_features(x)\n",
    "        else:\n",
    "            x = self.backbone.features(x)\n",
    "        x = self.logits(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliNet(nn.Module):\n",
    "    def __init__(self, backbone_name):\n",
    "        super(BengaliNet, self).__init__()\n",
    "        self.n_grapheme = 168\n",
    "        self.n_vowel = 11\n",
    "        self.n_consonant = 7\n",
    "        self.backbone_name = backbone_name\n",
    "        \n",
    "        self.num_classes = self.n_grapheme + self.n_vowel + self.n_consonant\n",
    "        \n",
    "        #self.conv0 = nn.Conv2d(1, 3, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        if self.backbone_name.startswith('efficient'):\n",
    "            self.backbone = EfficientNet.from_pretrained(self.backbone_name)\n",
    "            self.fc = nn.Linear(self.backbone._fc.in_features, self.num_classes)\n",
    "        else:\n",
    "            self.backbone = pretrainedmodels.__dict__[self.backbone_name](num_classes=1000, pretrained='imagenet')\n",
    "            self.fc = nn.Linear(self.backbone.last_linear.in_features, self.num_classes)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        self.fix_input_layer()\n",
    "        \n",
    "    def fix_input_layer(self):\n",
    "        if self.backbone_name in ['se_resnext50_32x4d', 'se_resnext101_32x4d', 'se_resnet50', 'senet154', 'se_resnet152', 'nasnetmobile', 'mobilenet', 'nasnetalarge']:\n",
    "            #self.backbone = eval(backbone_name)()\n",
    "            #print(self.backbone.layer0.conv1)\n",
    "            w = self.backbone.layer0.conv1.weight.data\n",
    "            self.backbone.layer0.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "            #self.backbone.layer0.conv1.weight = torch.nn.Parameter(torch.cat((w, w[:, 2, :, :].unsqueeze(1)), dim=1))\n",
    "            self.backbone.layer0.conv1.weight = torch.nn.Parameter(w[:, 0, :, :].unsqueeze(1))\n",
    "        \n",
    "    def logits(self, x):\n",
    "        x = self.avg_pool(x)\n",
    "        x = F.dropout2d(x, 0.2, self.training)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x = self.conv0(x)\n",
    "        #print(x.size())\n",
    "        if self.backbone_name.startswith('efficient'):\n",
    "            x = self.backbone.extract_features(x)\n",
    "        else:\n",
    "            x = self.backbone.features(x)\n",
    "        x = self.logits(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './models'\n",
    "def create_model(args):\n",
    "    model = BengaliNet(backbone_name=args.backbone)\n",
    "    model_file = os.path.join(MODEL_DIR, args.backbone, args.ckp_name)\n",
    "\n",
    "    parent_dir = os.path.dirname(model_file)\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "\n",
    "    print('model file: {}, exist: {}'.format(model_file, os.path.exists(model_file)))\n",
    "\n",
    "    if args.predict and (not os.path.exists(model_file)):\n",
    "        raise AttributeError('model file does not exist: {}'.format(model_file))\n",
    "\n",
    "    if os.path.exists(model_file):\n",
    "        print('loading {}...'.format(model_file))\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "    \n",
    "    return model, model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bnet = BengaliNet('se_resnext50_32x4d').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bnet(torch.randn((2, 1, 137, 236)).cuda()).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.111111"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(1/9, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "\n",
    "\n",
    "def macro_recall(pred_y, y, n_grapheme=168, n_vowel=11, n_consonant=7):\n",
    "    pred_y = torch.split(pred_y, [n_grapheme, n_vowel, n_consonant], dim=1)\n",
    "    pred_labels = [torch.argmax(py, dim=1).cpu().numpy() for py in pred_y]\n",
    "\n",
    "    #y = y.cpu().numpy()\n",
    "    # pred_y = [p.cpu().numpy() for p in pred_y]\n",
    "\n",
    "    recall_grapheme = sklearn.metrics.recall_score(pred_labels[0], y_grapheme, average='macro')\n",
    "    recall_vowel = sklearn.metrics.recall_score(pred_labels[1], y_vowel, average='macro')\n",
    "    recall_consonant = sklearn.metrics.recall_score(pred_labels[2], y_consonant, average='macro')\n",
    "    scores = [recall_grapheme, recall_vowel, recall_consonant]\n",
    "    final_score = np.average(scores, weights=[2, 1, 1])\n",
    "    # print(f'recall: grapheme {recall_grapheme}, vowel {recall_vowel}, consonant {recall_consonant}, '\n",
    "    #       f'total {final_score}, y {y.shape}')\n",
    "    return final_score\n",
    "\n",
    "def calc_metrics(preds0, preds1, preds2, y):\n",
    "    assert len(y) == len(preds0) == len(preds1) == len(preds2)\n",
    "\n",
    "    recall_grapheme = sklearn.metrics.recall_score(preds0, y[:, 0], average='macro')\n",
    "    recall_vowel = sklearn.metrics.recall_score(preds1, y[:, 1], average='macro')\n",
    "    recall_consonant = sklearn.metrics.recall_score(preds2, y[:, 2], average='macro')\n",
    "    scores = [recall_grapheme, recall_vowel, recall_consonant]\n",
    "    final_recall_score = np.average(scores, weights=[2, 1, 1])\n",
    "    \n",
    "    metrics = {}\n",
    "    metrics['recall'] = round(final_recall_score, 6)\n",
    "    metrics['recall_grapheme'] = round(recall_grapheme, 6)\n",
    "    metrics['recall_vowel'] = round(recall_vowel, 6)\n",
    "    metrics['recall_consonant'] = round(recall_consonant, 6)\n",
    "    \n",
    "    metrics['acc_grapheme'] = round((preds0 == y[:, 0]).sum() / len(y), 6)\n",
    "    metrics['acc_vowel'] = round((preds1 == y[:, 1]).sum() / len(y), 6)\n",
    "    metrics['acc_consonant'] = round((preds2 == y[:, 2]).sum() / len(y), 6)\n",
    "    \n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(outputs, y_true):\n",
    "    # outputs: (N, 182)\n",
    "    # y_true: (N, 3)\n",
    "    \n",
    "    outputs = torch.split(outputs, [168, 11, 7], dim=1)\n",
    "    loss0 = F.cross_entropy(outputs[0], y_true[:, 0], reduction='mean')\n",
    "    loss1 = F.cross_entropy(outputs[1], y_true[:, 1], reduction='mean')\n",
    "    loss2 = F.cross_entropy(outputs[2], y_true[:, 2], reduction='mean')\n",
    "    \n",
    "    return loss0 + loss1 + loss2 #, loss0.item(), loss1.item(), loss2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    loss0, loss1, loss2 = 0., 0., 0.\n",
    "    preds0, preds1,preds2 = [], [], []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            y_true.append(y)\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            outputs = model(x)\n",
    "            outputs = torch.split(outputs, [168, 11, 7], dim=1)\n",
    "            \n",
    "            preds0.append(torch.max(outputs[0], dim=1)[1])\n",
    "            preds1.append(torch.max(outputs[1], dim=1)[1])\n",
    "            preds2.append(torch.max(outputs[2], dim=1)[1])\n",
    "            loss0 += F.cross_entropy(outputs[0], y[:, 0], reduction='sum').item()\n",
    "            loss1 += F.cross_entropy(outputs[1], y[:, 1], reduction='sum').item()\n",
    "            loss2 += F.cross_entropy(outputs[2], y[:, 2], reduction='sum').item()\n",
    "            \n",
    "            # for debug\n",
    "            #metrics = {}\n",
    "            #metrics['loss_grapheme'] =  F.cross_entropy(outputs[0], y[:, 0], reduction='mean').item()\n",
    "            #metrics['loss_vowel'] =  F.cross_entropy(outputs[1], y[:, 1], reduction='mean').item()\n",
    "            #metrics['loss_consonant'] =  F.cross_entropy(outputs[2], y[:, 2], reduction='mean').item()\n",
    "            #return metrics\n",
    "    \n",
    "    preds0 = torch.cat(preds0, 0).cpu().numpy()\n",
    "    preds1 = torch.cat(preds1, 0).cpu().numpy()\n",
    "    preds2 = torch.cat(preds2, 0).cpu().numpy()\n",
    "    y_true = torch.cat(y_true, 0).numpy()\n",
    "    \n",
    "    #print('y_true:', y_true.shape)\n",
    "    #print('preds0:', preds0.shape)\n",
    "    \n",
    "    metrics = calc_metrics(preds0, preds1, preds2, y_true)\n",
    "    metrics['loss_grapheme'] = round(loss0 / val_loader.num, 6)\n",
    "    metrics['loss_vowel'] = round(loss1 / val_loader.num, 6)\n",
    "    metrics['loss_consonant'] = round(loss2 / val_loader.num, 6)\n",
    "    \n",
    "    return metrics\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lrs(optimizer):\n",
    "    lrs = []\n",
    "    for pgs in optimizer.state_dict()['param_groups']:\n",
    "        lrs.append(pgs['lr'])\n",
    "    lrs = ['{:.6f}'.format(x) for x in lrs]\n",
    "    return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_file):\n",
    "    parent_dir = os.path.dirname(model_file)\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        torch.save(model.module.state_dict(), model_file)\n",
    "    else:\n",
    "        torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(data, targets, alpha=1):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets = targets[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    data = data * lam + shuffled_data * (1 - lam)\n",
    "    targets = (targets, shuffled_targets, lam)\n",
    "\n",
    "    return data, targets\n",
    "\n",
    "\n",
    "def mixup_criterion(outputs, targets):\n",
    "    targets1, targets2, lam = targets\n",
    "    #criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    return lam * criterion(outputs, targets1) + (1 - lam) * criterion(outputs, targets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3693146998813017"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from over9000.over9000 import Over9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    global model\n",
    "\n",
    "    if args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=0.0001)\n",
    "    elif args.optim == 'RAdam':\n",
    "        optimizer = RAdam(model.parameters(), lr=args.lr)\n",
    "    elif args.optim == 'Over9000':\n",
    "        optimizer = Over9000(model.parameters(), lr=args.lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "    if args.lrs == 'plateau':\n",
    "        lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=args.factor, patience=args.patience, min_lr=args.min_lr)\n",
    "    else:\n",
    "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, args.t_max, eta_min=args.min_lr)\n",
    "        \n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    best_metrics = 0.\n",
    "    best_key = 'recall'\n",
    "    \n",
    "    val_metrics = validate(model, val_loader)\n",
    "    print(val_metrics)\n",
    "    best_metrics = val_metrics[best_key]\n",
    "    \n",
    "    model.train()\n",
    "    #optimizer.zero_grad()\n",
    "\n",
    "    #if args.lrs == 'plateau':\n",
    "    #    lr_scheduler.step(best_metrics)\n",
    "    #else:\n",
    "    #    lr_scheduler.step()\n",
    "    train_iter = 0\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        train_loss = 0\n",
    "\n",
    "        current_lr = get_lrs(optimizer)\n",
    "        bg = time.time()\n",
    "        for batch_idx, (img, targets) in enumerate(train_loader):\n",
    "            train_iter += 1\n",
    "            img, targets  = img.cuda(), targets.cuda()\n",
    "            #do_mixup = False #(np.random.random() < 0.4)\n",
    "            \n",
    "            #if do_mixup:\n",
    "            #    img, targets = mixup(img, targets)\n",
    "            batch_size = img.size(0)\n",
    "          \n",
    "            \n",
    "            \n",
    "            #if do_mixup:\n",
    "            #    loss = mixup_criterion(outputs, targets)\n",
    "            #else:\n",
    "            #    loss = criterion(outputs, targets)\n",
    "            r = np.random.rand()\n",
    "            #if args.beta > 0 and r < args.cutmix_prob:\n",
    "            if r < 0.5:\n",
    "                # generate mixed sample\n",
    "                lam = np.random.beta(args.beta, args.beta)\n",
    "                rand_index = torch.randperm(img.size()[0]).cuda()\n",
    "                target_a = targets\n",
    "                target_b = targets[rand_index]\n",
    "                bbx1, bby1, bbx2, bby2 = rand_bbox(img.size(), lam)\n",
    "                img[:, :, bbx1:bbx2, bby1:bby2] = img[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "                # adjust lambda to exactly match pixel ratio\n",
    "                lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (img.size()[-1] * img.size()[-2]))\n",
    "                # compute output\n",
    "                outputs = model(img)\n",
    "                loss = criterion(outputs, target_a) * lam + criterion(outputs, target_b) * (1. - lam)\n",
    "            else:\n",
    "                #img, targets = mixup(img, targets)\n",
    "                outputs = model(img)\n",
    "                #loss = mixup_criterion(outputs, targets)\n",
    "                loss = criterion(outputs, targets)\n",
    "            \n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "            \n",
    "            #loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "            #if batch_idx % 4 == 0:\n",
    "            #    optimizer.step()\n",
    "            #    optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            print('\\r {:4d} | {:.6f} | {:06d}/{} | {:.4f} | {:.4f} |'.format(\n",
    "                epoch, float(current_lr[0]), batch_size*(batch_idx+1), train_loader.num, \n",
    "                loss.item(), train_loss/(batch_idx+1)), end='')\n",
    "\n",
    "            if train_iter > 0 and train_iter % args.iter_val == 0:\n",
    "                #outputs = torch.split(outputs, [168, 11, 7], dim=1)\n",
    "            \n",
    "                #preds0 = (torch.max(outputs[0], dim=1)[1]).cpu().numpy()\n",
    "                #preds1 = (torch.max(outputs[1], dim=1)[1]).cpu().numpy()\n",
    "                #preds2 = (torch.max(outputs[2], dim=1)[1]).cpu().numpy()\n",
    "                #train_metrics = calc_metrics(preds0, preds1, preds2, targets.cpu().numpy())\n",
    "                #print('train:', train_metrics)\n",
    "                #save_model(model, model_file+'_latest')\n",
    "                val_metrics = validate(model, val_loader)\n",
    "                print('\\nval:', val_metrics)\n",
    "                \n",
    "                if val_metrics[best_key] > best_metrics:\n",
    "                    best_metrics = val_metrics[best_key]\n",
    "                    save_model(model, model_file)\n",
    "                    print('** saved')\n",
    "                \n",
    "                model.train()\n",
    "                \n",
    "                if args.lrs == 'plateau':\n",
    "                    lr_scheduler.step(best_metrics)\n",
    "                else:\n",
    "                    lr_scheduler.step()\n",
    "                current_lr = get_lrs(optimizer)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.backbone = 'se_resnext50_32x4d'\n",
    "args.ckp_name = 'model2_best_model.pth'\n",
    "args.predict = False\n",
    "args.optim = 'Over9000'\n",
    "args.lr = 1e-4\n",
    "args.lrs = 'cosine'\n",
    "args.t_max = 15\n",
    "args.factor = 0.6\n",
    "args.patience = 2\n",
    "args.min_lr = 2e-6\n",
    "args.iter_val = 200\n",
    "args.num_epochs = 100000\n",
    "args.batch_size = 1024\n",
    "args.val_batch_size = 1024\n",
    "\n",
    "args.beta = 1.0\n",
    "args.cutmix_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200840, 5)\n",
      "(200840, 32332)\n",
      "(160678, 5) (40162, 5)\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_train_val_loaders(batch_size=args.batch_size, val_batch_size=args.val_batch_size, ifold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model file: ./models/se_resnext50_32x4d/model2_best_model.pth, exist: True\n",
      "loading ./models/se_resnext50_32x4d/model2_best_model.pth...\n"
     ]
    }
   ],
   "source": [
    "model, model_file = create_model(args)\n",
    "#if torch.cuda.device_count() > 1:\n",
    "#    model = nn.DataParallel(model)\n",
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'recall': 0.983832, 'recall_grapheme': 0.976587, 'recall_vowel': 0.992964, 'recall_consonant': 0.989192, 'acc_grapheme': 0.975499, 'acc_vowel': 0.993725, 'acc_consonant': 0.992082, 'loss_grapheme': 0.10933, 'loss_vowel': 0.045842, 'loss_consonant': 0.039667}\n",
      "    1 | 0.000100 | 045056/160678 | 3.8738 | 1.4469 |\n",
      "val: {'recall': 0.983691, 'recall_grapheme': 0.976539, 'recall_vowel': 0.993165, 'recall_consonant': 0.988521, 'acc_grapheme': 0.975649, 'acc_vowel': 0.9938, 'acc_consonant': 0.992182, 'loss_grapheme': 0.107471, 'loss_vowel': 0.042411, 'loss_consonant': 0.038076}\n",
      "    2 | 0.000099 | 090112/160678 | 0.2990 | 1.2315 |\n",
      "val: {'recall': 0.983594, 'recall_grapheme': 0.976415, 'recall_vowel': 0.993047, 'recall_consonant': 0.9885, 'acc_grapheme': 0.976022, 'acc_vowel': 0.993775, 'acc_consonant': 0.992381, 'loss_grapheme': 0.105525, 'loss_vowel': 0.033101, 'loss_consonant': 0.033147}\n",
      "    3 | 0.000096 | 135168/160678 | 0.0027 | 1.4807 |\n",
      "val: {'recall': 0.983785, 'recall_grapheme': 0.975925, 'recall_vowel': 0.993226, 'recall_consonant': 0.990063, 'acc_grapheme': 0.975773, 'acc_vowel': 0.993875, 'acc_consonant': 0.992456, 'loss_grapheme': 0.106029, 'loss_vowel': 0.03179, 'loss_consonant': 0.032925}\n",
      "    5 | 0.000091 | 020480/160678 | 3.8960 | 1.6274 |\n",
      "val: {'recall': 0.983875, 'recall_grapheme': 0.976774, 'recall_vowel': 0.993002, 'recall_consonant': 0.988951, 'acc_grapheme': 0.976072, 'acc_vowel': 0.993701, 'acc_consonant': 0.992182, 'loss_grapheme': 0.10723, 'loss_vowel': 0.040027, 'loss_consonant': 0.036568}\n",
      "** saved\n",
      "    6 | 0.000084 | 065536/160678 | 3.3808 | 1.6849 |\n",
      "val: {'recall': 0.983165, 'recall_grapheme': 0.975341, 'recall_vowel': 0.992848, 'recall_consonant': 0.98913, 'acc_grapheme': 0.975051, 'acc_vowel': 0.993551, 'acc_consonant': 0.992007, 'loss_grapheme': 0.119624, 'loss_vowel': 0.059627, 'loss_consonant': 0.04839}\n",
      "    7 | 0.000076 | 110592/160678 | 3.6618 | 1.6470 |\n",
      "val: {'recall': 0.983561, 'recall_grapheme': 0.975901, 'recall_vowel': 0.992998, 'recall_consonant': 0.989445, 'acc_grapheme': 0.975549, 'acc_vowel': 0.993601, 'acc_consonant': 0.992207, 'loss_grapheme': 0.123419, 'loss_vowel': 0.063422, 'loss_consonant': 0.051612}\n",
      "    8 | 0.000066 | 155648/160678 | 1.9458 | 1.6672 |\n",
      "val: {'recall': 0.983681, 'recall_grapheme': 0.976243, 'recall_vowel': 0.992743, 'recall_consonant': 0.989496, 'acc_grapheme': 0.975449, 'acc_vowel': 0.993501, 'acc_consonant': 0.992182, 'loss_grapheme': 0.11389, 'loss_vowel': 0.053901, 'loss_consonant': 0.045248}\n",
      "   10 | 0.000056 | 040960/160678 | 0.0049 | 1.6755 |\n",
      "val: {'recall': 0.983525, 'recall_grapheme': 0.975887, 'recall_vowel': 0.993133, 'recall_consonant': 0.989193, 'acc_grapheme': 0.975449, 'acc_vowel': 0.993576, 'acc_consonant': 0.992182, 'loss_grapheme': 0.114082, 'loss_vowel': 0.053192, 'loss_consonant': 0.044528}\n",
      "   11 | 0.000046 | 086016/160678 | 0.0030 | 1.8414 |\n",
      "val: {'recall': 0.983653, 'recall_grapheme': 0.976296, 'recall_vowel': 0.993173, 'recall_consonant': 0.988848, 'acc_grapheme': 0.975375, 'acc_vowel': 0.993701, 'acc_consonant': 0.992207, 'loss_grapheme': 0.114651, 'loss_vowel': 0.055676, 'loss_consonant': 0.046217}\n",
      "   12 | 0.000036 | 056320/160678 | 4.2781 | 1.4850 |"
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'recall': 0.983759, 'recall_grapheme': 0.976406, 'recall_vowel': 0.992934, 'recall_consonant': 0.98929, 'acc_grapheme': 0.975425, 'acc_vowel': 0.993551, 'acc_consonant': 0.992157, 'loss_grapheme': 0.106273, 'loss_vowel': 0.041701, 'loss_consonant': 0.038099}\n",
      "    1 | 0.000020 | 045056/160678 | 1.2768 | 1.7016 |\n",
      "val: {'recall': 0.983604, 'recall_grapheme': 0.976226, 'recall_vowel': 0.992656, 'recall_consonant': 0.989307, 'acc_grapheme': 0.9754, 'acc_vowel': 0.993377, 'acc_consonant': 0.992256, 'loss_grapheme': 0.107763, 'loss_vowel': 0.044844, 'loss_consonant': 0.039453}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/chicm/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:73: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2 | 0.000020 | 090112/160678 | 3.5262 | 1.3944 |\n",
      "val: {'recall': 0.983226, 'recall_grapheme': 0.97561, 'recall_vowel': 0.992674, 'recall_consonant': 0.989011, 'acc_grapheme': 0.974752, 'acc_vowel': 0.993302, 'acc_consonant': 0.991684, 'loss_grapheme': 0.133167, 'loss_vowel': 0.079155, 'loss_consonant': 0.059539}\n",
      "    3 | 0.000019 | 135168/160678 | 3.4017 | 1.3007 |\n",
      "val: {'recall': 0.983111, 'recall_grapheme': 0.975271, 'recall_vowel': 0.99284, 'recall_consonant': 0.989063, 'acc_grapheme': 0.97525, 'acc_vowel': 0.993651, 'acc_consonant': 0.992231, 'loss_grapheme': 0.104009, 'loss_vowel': 0.034853, 'loss_consonant': 0.033734}\n",
      "    5 | 0.000017 | 020480/160678 | 4.5452 | 1.8092 |\n",
      "val: {'recall': 0.982948, 'recall_grapheme': 0.975272, 'recall_vowel': 0.992556, 'recall_consonant': 0.988692, 'acc_grapheme': 0.974628, 'acc_vowel': 0.993252, 'acc_consonant': 0.992032, 'loss_grapheme': 0.117416, 'loss_vowel': 0.059287, 'loss_consonant': 0.048102}\n",
      "    6 | 0.000015 | 065536/160678 | 0.0066 | 1.5202 |\n",
      "val: {'recall': 0.983382, 'recall_grapheme': 0.975647, 'recall_vowel': 0.992955, 'recall_consonant': 0.989279, 'acc_grapheme': 0.975325, 'acc_vowel': 0.9938, 'acc_consonant': 0.992306, 'loss_grapheme': 0.104475, 'loss_vowel': 0.036597, 'loss_consonant': 0.034975}\n",
      "    7 | 0.000013 | 110592/160678 | 0.0026 | 1.4957 |\n",
      "val: {'recall': 0.983351, 'recall_grapheme': 0.975697, 'recall_vowel': 0.992845, 'recall_consonant': 0.989164, 'acc_grapheme': 0.975051, 'acc_vowel': 0.993651, 'acc_consonant': 0.992356, 'loss_grapheme': 0.104905, 'loss_vowel': 0.038618, 'loss_consonant': 0.035751}\n",
      "    8 | 0.000011 | 155648/160678 | 4.1776 | 1.5610 |\n",
      "val: {'recall': 0.98319, 'recall_grapheme': 0.975727, 'recall_vowel': 0.992928, 'recall_consonant': 0.988378, 'acc_grapheme': 0.975275, 'acc_vowel': 0.99375, 'acc_consonant': 0.992082, 'loss_grapheme': 0.104935, 'loss_vowel': 0.038738, 'loss_consonant': 0.035967}\n",
      "   10 | 0.000008 | 040960/160678 | 0.0030 | 1.7202 |\n",
      "val: {'recall': 0.983415, 'recall_grapheme': 0.975831, 'recall_vowel': 0.992738, 'recall_consonant': 0.98926, 'acc_grapheme': 0.9754, 'acc_vowel': 0.993626, 'acc_consonant': 0.992231, 'loss_grapheme': 0.10516, 'loss_vowel': 0.039951, 'loss_consonant': 0.037057}\n",
      "   11 | 0.000006 | 086016/160678 | 3.0939 | 1.5519 |\n",
      "val: {'recall': 0.983399, 'recall_grapheme': 0.97596, 'recall_vowel': 0.992529, 'recall_consonant': 0.989148, 'acc_grapheme': 0.975176, 'acc_vowel': 0.993427, 'acc_consonant': 0.992157, 'loss_grapheme': 0.110441, 'loss_vowel': 0.048744, 'loss_consonant': 0.041717}\n",
      "   12 | 0.000004 | 131072/160678 | 0.0030 | 1.6255 |\n",
      "val: {'recall': 0.983336, 'recall_grapheme': 0.975767, 'recall_vowel': 0.992624, 'recall_consonant': 0.989185, 'acc_grapheme': 0.975076, 'acc_vowel': 0.993501, 'acc_consonant': 0.992007, 'loss_grapheme': 0.112046, 'loss_vowel': 0.051095, 'loss_consonant': 0.042804}\n",
      "   14 | 0.000002 | 016384/160678 | 0.0021 | 1.5757 |\n",
      "val: {'recall': 0.983318, 'recall_grapheme': 0.975661, 'recall_vowel': 0.992762, 'recall_consonant': 0.989189, 'acc_grapheme': 0.975126, 'acc_vowel': 0.993476, 'acc_consonant': 0.992057, 'loss_grapheme': 0.107589, 'loss_vowel': 0.044383, 'loss_consonant': 0.039541}\n",
      "   15 | 0.000001 | 061440/160678 | 0.0020 | 0.8565 |\n",
      "val: {'recall': 0.983588, 'recall_grapheme': 0.976144, 'recall_vowel': 0.99299, 'recall_consonant': 0.989076, 'acc_grapheme': 0.975947, 'acc_vowel': 0.993701, 'acc_consonant': 0.992306, 'loss_grapheme': 0.104566, 'loss_vowel': 0.031519, 'loss_consonant': 0.032255}\n",
      "   16 | 0.000001 | 106496/160678 | 3.3899 | 1.9004 |\n",
      "val: {'recall': 0.983111, 'recall_grapheme': 0.975497, 'recall_vowel': 0.992386, 'recall_consonant': 0.989067, 'acc_grapheme': 0.974777, 'acc_vowel': 0.993252, 'acc_consonant': 0.992032, 'loss_grapheme': 0.11988, 'loss_vowel': 0.060962, 'loss_consonant': 0.04969}\n",
      "   17 | 0.000001 | 151552/160678 | 3.3714 | 1.3708 |\n",
      "val: {'recall': 0.98323, 'recall_grapheme': 0.975509, 'recall_vowel': 0.992761, 'recall_consonant': 0.989143, 'acc_grapheme': 0.9752, 'acc_vowel': 0.993501, 'acc_consonant': 0.992132, 'loss_grapheme': 0.107633, 'loss_vowel': 0.043771, 'loss_consonant': 0.038955}\n",
      "   19 | 0.000002 | 036864/160678 | 0.0018 | 1.1735 |\n",
      "val: {'recall': 0.983615, 'recall_grapheme': 0.976251, 'recall_vowel': 0.992917, 'recall_consonant': 0.989039, 'acc_grapheme': 0.975823, 'acc_vowel': 0.993701, 'acc_consonant': 0.992281, 'loss_grapheme': 0.106029, 'loss_vowel': 0.031171, 'loss_consonant': 0.032147}\n",
      "   20 | 0.000004 | 081920/160678 | 4.0975 | 1.5898 |\n",
      "val: {'recall': 0.983239, 'recall_grapheme': 0.975884, 'recall_vowel': 0.992759, 'recall_consonant': 0.988428, 'acc_grapheme': 0.974951, 'acc_vowel': 0.993576, 'acc_consonant': 0.992082, 'loss_grapheme': 0.110135, 'loss_vowel': 0.048219, 'loss_consonant': 0.041204}\n",
      "   21 | 0.000006 | 126976/160678 | 3.2304 | 1.4285 |\n",
      "val: {'recall': 0.98292, 'recall_grapheme': 0.975324, 'recall_vowel': 0.992058, 'recall_consonant': 0.988976, 'acc_grapheme': 0.974827, 'acc_vowel': 0.993153, 'acc_consonant': 0.991783, 'loss_grapheme': 0.131132, 'loss_vowel': 0.076353, 'loss_consonant': 0.057412}\n",
      "   23 | 0.000008 | 012288/160678 | 2.2818 | 1.2718 |\n",
      "val: {'recall': 0.983426, 'recall_grapheme': 0.975908, 'recall_vowel': 0.992715, 'recall_consonant': 0.989172, 'acc_grapheme': 0.975026, 'acc_vowel': 0.993551, 'acc_consonant': 0.992157, 'loss_grapheme': 0.10917, 'loss_vowel': 0.046573, 'loss_consonant': 0.040618}\n",
      "   24 | 0.000010 | 057344/160678 | 0.0027 | 2.0059 |\n",
      "val: {'recall': 0.98319, 'recall_grapheme': 0.975539, 'recall_vowel': 0.992446, 'recall_consonant': 0.989234, 'acc_grapheme': 0.974653, 'acc_vowel': 0.993327, 'acc_consonant': 0.992107, 'loss_grapheme': 0.118122, 'loss_vowel': 0.060153, 'loss_consonant': 0.049023}\n",
      "   25 | 0.000013 | 102400/160678 | 3.2032 | 1.5616 |\n",
      "val: {'recall': 0.983247, 'recall_grapheme': 0.97556, 'recall_vowel': 0.992679, 'recall_consonant': 0.98919, 'acc_grapheme': 0.974553, 'acc_vowel': 0.993203, 'acc_consonant': 0.991883, 'loss_grapheme': 0.123759, 'loss_vowel': 0.064743, 'loss_consonant': 0.051194}\n",
      "   26 | 0.000015 | 147456/160678 | 0.0019 | 1.5416 |\n",
      "val: {'recall': 0.983365, 'recall_grapheme': 0.975919, 'recall_vowel': 0.993145, 'recall_consonant': 0.988475, 'acc_grapheme': 0.975723, 'acc_vowel': 0.9938, 'acc_consonant': 0.992231, 'loss_grapheme': 0.104562, 'loss_vowel': 0.031495, 'loss_consonant': 0.031944}\n",
      "   28 | 0.000017 | 032768/160678 | 0.0045 | 1.6766 |\n",
      "val: {'recall': 0.983085, 'recall_grapheme': 0.97553, 'recall_vowel': 0.992724, 'recall_consonant': 0.988555, 'acc_grapheme': 0.974902, 'acc_vowel': 0.993476, 'acc_consonant': 0.992231, 'loss_grapheme': 0.106446, 'loss_vowel': 0.041223, 'loss_consonant': 0.037363}\n",
      "   29 | 0.000019 | 077824/160678 | 0.0021 | 1.7800 |\n",
      "val: {'recall': 0.98346, 'recall_grapheme': 0.976171, 'recall_vowel': 0.992867, 'recall_consonant': 0.988629, 'acc_grapheme': 0.975076, 'acc_vowel': 0.993576, 'acc_consonant': 0.992231, 'loss_grapheme': 0.106225, 'loss_vowel': 0.042122, 'loss_consonant': 0.037691}\n",
      "   30 | 0.000020 | 122880/160678 | 0.0024 | 1.3879 |\n",
      "val: {'recall': 0.983321, 'recall_grapheme': 0.97573, 'recall_vowel': 0.992661, 'recall_consonant': 0.989162, 'acc_grapheme': 0.97535, 'acc_vowel': 0.993476, 'acc_consonant': 0.992207, 'loss_grapheme': 0.108154, 'loss_vowel': 0.043869, 'loss_consonant': 0.039114}\n",
      "   32 | 0.000020 | 008192/160678 | 0.3418 | 2.8001 |\n",
      "val: {'recall': 0.983099, 'recall_grapheme': 0.975403, 'recall_vowel': 0.992268, 'recall_consonant': 0.989324, 'acc_grapheme': 0.974702, 'acc_vowel': 0.993178, 'acc_consonant': 0.991833, 'loss_grapheme': 0.144811, 'loss_vowel': 0.087972, 'loss_consonant': 0.066111}\n",
      "   33 | 0.000020 | 053248/160678 | 4.0966 | 1.4710 |\n",
      "val: {'recall': 0.983467, 'recall_grapheme': 0.976173, 'recall_vowel': 0.99249, 'recall_consonant': 0.989032, 'acc_grapheme': 0.975051, 'acc_vowel': 0.993427, 'acc_consonant': 0.992231, 'loss_grapheme': 0.108466, 'loss_vowel': 0.044597, 'loss_consonant': 0.039202}\n",
      "   34 | 0.000019 | 098304/160678 | 4.2670 | 1.4355 |\n",
      "val: {'recall': 0.983476, 'recall_grapheme': 0.975973, 'recall_vowel': 0.992735, 'recall_consonant': 0.989225, 'acc_grapheme': 0.975001, 'acc_vowel': 0.993377, 'acc_consonant': 0.991982, 'loss_grapheme': 0.11645, 'loss_vowel': 0.055827, 'loss_consonant': 0.046758}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   35 | 0.000017 | 143360/160678 | 0.0019 | 1.5450 |\n",
      "val: {'recall': 0.983287, 'recall_grapheme': 0.975601, 'recall_vowel': 0.992595, 'recall_consonant': 0.989352, 'acc_grapheme': 0.974927, 'acc_vowel': 0.993427, 'acc_consonant': 0.992231, 'loss_grapheme': 0.108547, 'loss_vowel': 0.045038, 'loss_consonant': 0.039963}\n",
      "   37 | 0.000015 | 028672/160678 | 0.0032 | 1.3909 |\n",
      "val: {'recall': 0.983239, 'recall_grapheme': 0.975477, 'recall_vowel': 0.992492, 'recall_consonant': 0.989509, 'acc_grapheme': 0.975425, 'acc_vowel': 0.993601, 'acc_consonant': 0.992381, 'loss_grapheme': 0.104633, 'loss_vowel': 0.034217, 'loss_consonant': 0.034157}\n",
      "   38 | 0.000013 | 073728/160678 | 0.0036 | 1.4285 |\n",
      "val: {'recall': 0.98337, 'recall_grapheme': 0.975778, 'recall_vowel': 0.992514, 'recall_consonant': 0.98941, 'acc_grapheme': 0.975076, 'acc_vowel': 0.993551, 'acc_consonant': 0.992256, 'loss_grapheme': 0.106842, 'loss_vowel': 0.040796, 'loss_consonant': 0.037841}\n",
      "   39 | 0.000011 | 118784/160678 | 0.0037 | 1.6077 |\n",
      "val: {'recall': 0.983196, 'recall_grapheme': 0.97545, 'recall_vowel': 0.992736, 'recall_consonant': 0.989149, 'acc_grapheme': 0.975449, 'acc_vowel': 0.993501, 'acc_consonant': 0.992381, 'loss_grapheme': 0.103694, 'loss_vowel': 0.034477, 'loss_consonant': 0.033526}\n",
      "   41 | 0.000008 | 004096/160678 | 0.0019 | 1.4171 |\n",
      "val: {'recall': 0.983211, 'recall_grapheme': 0.975388, 'recall_vowel': 0.992732, 'recall_consonant': 0.989335, 'acc_grapheme': 0.974852, 'acc_vowel': 0.993476, 'acc_consonant': 0.992231, 'loss_grapheme': 0.106286, 'loss_vowel': 0.041264, 'loss_consonant': 0.037395}\n",
      "   42 | 0.000006 | 049152/160678 | 3.9854 | 1.3697 |\n",
      "val: {'recall': 0.983309, 'recall_grapheme': 0.975788, 'recall_vowel': 0.992535, 'recall_consonant': 0.989128, 'acc_grapheme': 0.97535, 'acc_vowel': 0.993501, 'acc_consonant': 0.992132, 'loss_grapheme': 0.108071, 'loss_vowel': 0.044153, 'loss_consonant': 0.03967}\n",
      "   43 | 0.000004 | 094208/160678 | 0.0031 | 1.4292 |\n",
      "val: {'recall': 0.983425, 'recall_grapheme': 0.975836, 'recall_vowel': 0.992746, 'recall_consonant': 0.989281, 'acc_grapheme': 0.975698, 'acc_vowel': 0.993576, 'acc_consonant': 0.992381, 'loss_grapheme': 0.10445, 'loss_vowel': 0.032695, 'loss_consonant': 0.033009}\n",
      "   44 | 0.000002 | 139264/160678 | 2.2967 | 1.5224 |\n",
      "val: {'recall': 0.983166, 'recall_grapheme': 0.975357, 'recall_vowel': 0.992746, 'recall_consonant': 0.989204, 'acc_grapheme': 0.975126, 'acc_vowel': 0.993501, 'acc_consonant': 0.992231, 'loss_grapheme': 0.109804, 'loss_vowel': 0.04802, 'loss_consonant': 0.041654}\n",
      "   46 | 0.000001 | 024576/160678 | 0.0037 | 1.8788 |\n",
      "val: {'recall': 0.983629, 'recall_grapheme': 0.976496, 'recall_vowel': 0.992404, 'recall_consonant': 0.98912, 'acc_grapheme': 0.974752, 'acc_vowel': 0.993203, 'acc_consonant': 0.991783, 'loss_grapheme': 0.156959, 'loss_vowel': 0.103864, 'loss_consonant': 0.074166}\n",
      "   47 | 0.000001 | 069632/160678 | 0.0028 | 1.5248 |\n",
      "val: {'recall': 0.983707, 'recall_grapheme': 0.976412, 'recall_vowel': 0.992552, 'recall_consonant': 0.989451, 'acc_grapheme': 0.975225, 'acc_vowel': 0.993352, 'acc_consonant': 0.992207, 'loss_grapheme': 0.111315, 'loss_vowel': 0.050663, 'loss_consonant': 0.042697}\n",
      "   48 | 0.000001 | 114688/160678 | 0.0022 | 1.3112 |\n",
      "val: {'recall': 0.983447, 'recall_grapheme': 0.975858, 'recall_vowel': 0.992946, 'recall_consonant': 0.989126, 'acc_grapheme': 0.975674, 'acc_vowel': 0.993775, 'acc_consonant': 0.992331, 'loss_grapheme': 0.105048, 'loss_vowel': 0.032017, 'loss_consonant': 0.032805}\n",
      "   49 | 0.000002 | 159744/160678 | 0.0021 | 1.4978 |\n",
      "val: {'recall': 0.98355, 'recall_grapheme': 0.976262, 'recall_vowel': 0.992428, 'recall_consonant': 0.989249, 'acc_grapheme': 0.975101, 'acc_vowel': 0.993302, 'acc_consonant': 0.992182, 'loss_grapheme': 0.11628, 'loss_vowel': 0.057589, 'loss_consonant': 0.047114}\n",
      "   51 | 0.000004 | 045056/160678 | 0.0045 | 1.4880 |\n",
      "val: {'recall': 0.983434, 'recall_grapheme': 0.976126, 'recall_vowel': 0.99283, 'recall_consonant': 0.988652, 'acc_grapheme': 0.975574, 'acc_vowel': 0.993551, 'acc_consonant': 0.992207, 'loss_grapheme': 0.10525, 'loss_vowel': 0.038795, 'loss_consonant': 0.036375}\n",
      "   52 | 0.000006 | 090112/160678 | 3.9781 | 2.0231 |\n",
      "val: {'recall': 0.983465, 'recall_grapheme': 0.976045, 'recall_vowel': 0.992629, 'recall_consonant': 0.989141, 'acc_grapheme': 0.975001, 'acc_vowel': 0.993476, 'acc_consonant': 0.991833, 'loss_grapheme': 0.146854, 'loss_vowel': 0.090592, 'loss_consonant': 0.067164}\n",
      "   53 | 0.000008 | 135168/160678 | 0.0022 | 1.7029 |\n",
      "val: {'recall': 0.983421, 'recall_grapheme': 0.97588, 'recall_vowel': 0.992746, 'recall_consonant': 0.989179, 'acc_grapheme': 0.975151, 'acc_vowel': 0.993427, 'acc_consonant': 0.992132, 'loss_grapheme': 0.106795, 'loss_vowel': 0.042093, 'loss_consonant': 0.038165}\n",
      "   55 | 0.000010 | 020480/160678 | 0.0028 | 1.3778 |\n",
      "val: {'recall': 0.983305, 'recall_grapheme': 0.975679, 'recall_vowel': 0.992633, 'recall_consonant': 0.989228, 'acc_grapheme': 0.97525, 'acc_vowel': 0.993526, 'acc_consonant': 0.992207, 'loss_grapheme': 0.108639, 'loss_vowel': 0.045803, 'loss_consonant': 0.040363}\n",
      "   56 | 0.000013 | 065536/160678 | 2.4175 | 1.4797 |\n",
      "val: {'recall': 0.983607, 'recall_grapheme': 0.976404, 'recall_vowel': 0.992981, 'recall_consonant': 0.98864, 'acc_grapheme': 0.975674, 'acc_vowel': 0.99375, 'acc_consonant': 0.992381, 'loss_grapheme': 0.103613, 'loss_vowel': 0.034588, 'loss_consonant': 0.033721}\n",
      "   57 | 0.000015 | 110592/160678 | 0.0043 | 1.5284 |\n",
      "val: {'recall': 0.983675, 'recall_grapheme': 0.97636, 'recall_vowel': 0.992578, 'recall_consonant': 0.989402, 'acc_grapheme': 0.976296, 'acc_vowel': 0.993551, 'acc_consonant': 0.99253, 'loss_grapheme': 0.106993, 'loss_vowel': 0.030716, 'loss_consonant': 0.032592}\n",
      "   58 | 0.000017 | 155648/160678 | 3.0773 | 1.7682 |\n",
      "val: {'recall': 0.983506, 'recall_grapheme': 0.976104, 'recall_vowel': 0.992495, 'recall_consonant': 0.989323, 'acc_grapheme': 0.974802, 'acc_vowel': 0.993352, 'acc_consonant': 0.992007, 'loss_grapheme': 0.120615, 'loss_vowel': 0.064557, 'loss_consonant': 0.050798}\n",
      "   60 | 0.000019 | 040960/160678 | 3.4799 | 1.4422 |\n",
      "val: {'recall': 0.983472, 'recall_grapheme': 0.976033, 'recall_vowel': 0.992823, 'recall_consonant': 0.989, 'acc_grapheme': 0.975151, 'acc_vowel': 0.993551, 'acc_consonant': 0.992082, 'loss_grapheme': 0.110634, 'loss_vowel': 0.048607, 'loss_consonant': 0.041506}\n",
      "   61 | 0.000020 | 086016/160678 | 4.4979 | 1.6722 |\n",
      "val: {'recall': 0.983792, 'recall_grapheme': 0.976533, 'recall_vowel': 0.99267, 'recall_consonant': 0.989432, 'acc_grapheme': 0.975325, 'acc_vowel': 0.993551, 'acc_consonant': 0.992356, 'loss_grapheme': 0.113248, 'loss_vowel': 0.054714, 'loss_consonant': 0.04487}\n",
      "** saved\n",
      "   62 | 0.000020 | 131072/160678 | 0.0027 | 1.3083 |\n",
      "val: {'recall': 0.98349, 'recall_grapheme': 0.976186, 'recall_vowel': 0.992532, 'recall_consonant': 0.989058, 'acc_grapheme': 0.9753, 'acc_vowel': 0.993377, 'acc_consonant': 0.992107, 'loss_grapheme': 0.115156, 'loss_vowel': 0.056773, 'loss_consonant': 0.045654}\n",
      "   64 | 0.000020 | 016384/160678 | 1.8405 | 1.1281 |\n",
      "val: {'recall': 0.983389, 'recall_grapheme': 0.975831, 'recall_vowel': 0.992926, 'recall_consonant': 0.988967, 'acc_grapheme': 0.975624, 'acc_vowel': 0.993775, 'acc_consonant': 0.992157, 'loss_grapheme': 0.105047, 'loss_vowel': 0.037842, 'loss_consonant': 0.035183}\n",
      "   65 | 0.000019 | 061440/160678 | 3.7930 | 1.3367 |\n",
      "val: {'recall': 0.983314, 'recall_grapheme': 0.975718, 'recall_vowel': 0.992697, 'recall_consonant': 0.989122, 'acc_grapheme': 0.975449, 'acc_vowel': 0.993626, 'acc_consonant': 0.992082, 'loss_grapheme': 0.104788, 'loss_vowel': 0.03672, 'loss_consonant': 0.034862}\n",
      "   66 | 0.000017 | 106496/160678 | 2.1581 | 1.5760 |\n",
      "val: {'recall': 0.983551, 'recall_grapheme': 0.976024, 'recall_vowel': 0.992932, 'recall_consonant': 0.989223, 'acc_grapheme': 0.975325, 'acc_vowel': 0.993626, 'acc_consonant': 0.992032, 'loss_grapheme': 0.107137, 'loss_vowel': 0.041991, 'loss_consonant': 0.038031}\n",
      "   67 | 0.000015 | 151552/160678 | 0.0032 | 1.5056 |\n",
      "val: {'recall': 0.9834, 'recall_grapheme': 0.976205, 'recall_vowel': 0.992679, 'recall_consonant': 0.98851, 'acc_grapheme': 0.976072, 'acc_vowel': 0.993651, 'acc_consonant': 0.992505, 'loss_grapheme': 0.10873, 'loss_vowel': 0.031054, 'loss_consonant': 0.03212}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   69 | 0.000013 | 036864/160678 | 0.0035 | 2.0978 |\n",
      "val: {'recall': 0.983501, 'recall_grapheme': 0.976078, 'recall_vowel': 0.992889, 'recall_consonant': 0.98896, 'acc_grapheme': 0.975051, 'acc_vowel': 0.993476, 'acc_consonant': 0.991733, 'loss_grapheme': 0.116378, 'loss_vowel': 0.054478, 'loss_consonant': 0.04523}\n",
      "   70 | 0.000011 | 081920/160678 | 0.0020 | 1.4009 |\n",
      "val: {'recall': 0.983453, 'recall_grapheme': 0.975924, 'recall_vowel': 0.992875, 'recall_consonant': 0.98909, 'acc_grapheme': 0.975325, 'acc_vowel': 0.993576, 'acc_consonant': 0.992007, 'loss_grapheme': 0.109994, 'loss_vowel': 0.047794, 'loss_consonant': 0.041347}\n",
      "   71 | 0.000008 | 126976/160678 | 0.0027 | 1.4938 |\n",
      "val: {'recall': 0.983242, 'recall_grapheme': 0.975764, 'recall_vowel': 0.992765, 'recall_consonant': 0.988674, 'acc_grapheme': 0.975126, 'acc_vowel': 0.993476, 'acc_consonant': 0.992032, 'loss_grapheme': 0.108424, 'loss_vowel': 0.044579, 'loss_consonant': 0.039714}\n",
      "   73 | 0.000006 | 012288/160678 | 0.0043 | 1.1447 |\n",
      "val: {'recall': 0.983579, 'recall_grapheme': 0.976153, 'recall_vowel': 0.992848, 'recall_consonant': 0.989163, 'acc_grapheme': 0.975773, 'acc_vowel': 0.993651, 'acc_consonant': 0.992231, 'loss_grapheme': 0.104362, 'loss_vowel': 0.033772, 'loss_consonant': 0.033142}\n",
      "   74 | 0.000004 | 057344/160678 | 0.0027 | 1.7362 |\n",
      "val: {'recall': 0.983449, 'recall_grapheme': 0.97597, 'recall_vowel': 0.992737, 'recall_consonant': 0.989119, 'acc_grapheme': 0.975599, 'acc_vowel': 0.993576, 'acc_consonant': 0.992132, 'loss_grapheme': 0.107193, 'loss_vowel': 0.042827, 'loss_consonant': 0.038498}\n",
      "   75 | 0.000002 | 102400/160678 | 0.0015 | 2.0254 |\n",
      "val: {'recall': 0.983151, 'recall_grapheme': 0.975393, 'recall_vowel': 0.992579, 'recall_consonant': 0.989237, 'acc_grapheme': 0.975001, 'acc_vowel': 0.993277, 'acc_consonant': 0.991982, 'loss_grapheme': 0.125999, 'loss_vowel': 0.067341, 'loss_consonant': 0.053725}\n",
      "   76 | 0.000001 | 147456/160678 | 0.0100 | 1.4845 |\n",
      "val: {'recall': 0.983359, 'recall_grapheme': 0.975742, 'recall_vowel': 0.992857, 'recall_consonant': 0.989095, 'acc_grapheme': 0.97525, 'acc_vowel': 0.993725, 'acc_consonant': 0.992132, 'loss_grapheme': 0.104671, 'loss_vowel': 0.037212, 'loss_consonant': 0.035392}\n",
      "   78 | 0.000001 | 032768/160678 | 0.0019 | 1.3331 |\n",
      "val: {'recall': 0.98348, 'recall_grapheme': 0.976291, 'recall_vowel': 0.992804, 'recall_consonant': 0.988535, 'acc_grapheme': 0.975549, 'acc_vowel': 0.993701, 'acc_consonant': 0.992306, 'loss_grapheme': 0.105205, 'loss_vowel': 0.032564, 'loss_consonant': 0.032615}\n",
      "   79 | 0.000001 | 077824/160678 | 0.0020 | 1.3751 |\n",
      "val: {'recall': 0.983381, 'recall_grapheme': 0.975718, 'recall_vowel': 0.99286, 'recall_consonant': 0.989226, 'acc_grapheme': 0.975499, 'acc_vowel': 0.993626, 'acc_consonant': 0.992132, 'loss_grapheme': 0.105522, 'loss_vowel': 0.039252, 'loss_consonant': 0.03615}\n",
      "   80 | 0.000002 | 122880/160678 | 0.0038 | 1.5959 |\n",
      "val: {'recall': 0.98342, 'recall_grapheme': 0.975904, 'recall_vowel': 0.992654, 'recall_consonant': 0.989216, 'acc_grapheme': 0.975101, 'acc_vowel': 0.993501, 'acc_consonant': 0.992057, 'loss_grapheme': 0.11615, 'loss_vowel': 0.057723, 'loss_consonant': 0.048013}\n",
      "   82 | 0.000004 | 008192/160678 | 2.7519 | 0.5540 |\n",
      "val: {'recall': 0.983223, 'recall_grapheme': 0.975766, 'recall_vowel': 0.992666, 'recall_consonant': 0.988695, 'acc_grapheme': 0.975723, 'acc_vowel': 0.993626, 'acc_consonant': 0.992356, 'loss_grapheme': 0.104908, 'loss_vowel': 0.032453, 'loss_consonant': 0.032595}\n",
      "   83 | 0.000006 | 053248/160678 | 1.5105 | 1.5873 |\n",
      "val: {'recall': 0.983387, 'recall_grapheme': 0.97605, 'recall_vowel': 0.992672, 'recall_consonant': 0.988776, 'acc_grapheme': 0.975325, 'acc_vowel': 0.993526, 'acc_consonant': 0.992132, 'loss_grapheme': 0.107459, 'loss_vowel': 0.04366, 'loss_consonant': 0.038855}\n",
      "   84 | 0.000008 | 098304/160678 | 3.8797 | 1.6943 |\n",
      "val: {'recall': 0.983295, 'recall_grapheme': 0.975876, 'recall_vowel': 0.992754, 'recall_consonant': 0.988672, 'acc_grapheme': 0.974877, 'acc_vowel': 0.993452, 'acc_consonant': 0.991958, 'loss_grapheme': 0.117314, 'loss_vowel': 0.057624, 'loss_consonant': 0.047177}\n",
      "   85 | 0.000011 | 143360/160678 | 0.0051 | 1.7912 |\n",
      "val: {'recall': 0.98359, 'recall_grapheme': 0.976027, 'recall_vowel': 0.993141, 'recall_consonant': 0.989165, 'acc_grapheme': 0.975051, 'acc_vowel': 0.993576, 'acc_consonant': 0.992032, 'loss_grapheme': 0.113877, 'loss_vowel': 0.054177, 'loss_consonant': 0.044952}\n",
      "   87 | 0.000013 | 028672/160678 | 3.0844 | 2.0386 |\n",
      "val: {'recall': 0.983127, 'recall_grapheme': 0.975435, 'recall_vowel': 0.992333, 'recall_consonant': 0.989306, 'acc_grapheme': 0.974578, 'acc_vowel': 0.993227, 'acc_consonant': 0.991933, 'loss_grapheme': 0.156203, 'loss_vowel': 0.101389, 'loss_consonant': 0.072958}\n",
      "   88 | 0.000015 | 073728/160678 | 0.0049 | 1.8625 |\n",
      "val: {'recall': 0.983507, 'recall_grapheme': 0.975926, 'recall_vowel': 0.992884, 'recall_consonant': 0.989293, 'acc_grapheme': 0.9753, 'acc_vowel': 0.993576, 'acc_consonant': 0.992082, 'loss_grapheme': 0.10936, 'loss_vowel': 0.047589, 'loss_consonant': 0.041311}\n",
      "   89 | 0.000017 | 118784/160678 | 3.7065 | 1.5138 |\n",
      "val: {'recall': 0.983307, 'recall_grapheme': 0.975427, 'recall_vowel': 0.992962, 'recall_consonant': 0.989412, 'acc_grapheme': 0.975325, 'acc_vowel': 0.993626, 'acc_consonant': 0.992256, 'loss_grapheme': 0.105998, 'loss_vowel': 0.040572, 'loss_consonant': 0.036967}\n",
      "   91 | 0.000019 | 004096/160678 | 0.0024 | 1.1351 |\n",
      "val: {'recall': 0.98346, 'recall_grapheme': 0.975822, 'recall_vowel': 0.992969, 'recall_consonant': 0.989229, 'acc_grapheme': 0.975698, 'acc_vowel': 0.993576, 'acc_consonant': 0.992231, 'loss_grapheme': 0.108596, 'loss_vowel': 0.045405, 'loss_consonant': 0.039818}\n",
      "   92 | 0.000020 | 049152/160678 | 1.5015 | 1.3098 |\n",
      "val: {'recall': 0.983287, 'recall_grapheme': 0.975876, 'recall_vowel': 0.992552, 'recall_consonant': 0.988844, 'acc_grapheme': 0.975425, 'acc_vowel': 0.993551, 'acc_consonant': 0.991958, 'loss_grapheme': 0.116816, 'loss_vowel': 0.057527, 'loss_consonant': 0.046724}\n",
      "   93 | 0.000020 | 094208/160678 | 0.0020 | 1.3821 |\n",
      "val: {'recall': 0.983095, 'recall_grapheme': 0.975617, 'recall_vowel': 0.992466, 'recall_consonant': 0.98868, 'acc_grapheme': 0.975076, 'acc_vowel': 0.993252, 'acc_consonant': 0.991733, 'loss_grapheme': 0.130457, 'loss_vowel': 0.071285, 'loss_consonant': 0.056032}\n",
      "   94 | 0.000020 | 139264/160678 | 0.0052 | 1.5856 |\n",
      "val: {'recall': 0.983214, 'recall_grapheme': 0.975851, 'recall_vowel': 0.992477, 'recall_consonant': 0.988676, 'acc_grapheme': 0.97525, 'acc_vowel': 0.993427, 'acc_consonant': 0.991808, 'loss_grapheme': 0.121248, 'loss_vowel': 0.064788, 'loss_consonant': 0.05107}\n",
      "   96 | 0.000019 | 024576/160678 | 0.4099 | 0.9855 |\n",
      "val: {'recall': 0.983495, 'recall_grapheme': 0.97632, 'recall_vowel': 0.992842, 'recall_consonant': 0.988499, 'acc_grapheme': 0.975674, 'acc_vowel': 0.993701, 'acc_consonant': 0.992132, 'loss_grapheme': 0.103665, 'loss_vowel': 0.034025, 'loss_consonant': 0.033307}\n",
      "   97 | 0.000017 | 069632/160678 | 0.0023 | 1.6085 |\n",
      "val: {'recall': 0.983539, 'recall_grapheme': 0.976146, 'recall_vowel': 0.992742, 'recall_consonant': 0.98912, 'acc_grapheme': 0.975524, 'acc_vowel': 0.9938, 'acc_consonant': 0.992157, 'loss_grapheme': 0.104664, 'loss_vowel': 0.037592, 'loss_consonant': 0.035401}\n",
      "   98 | 0.000015 | 114688/160678 | 0.0049 | 1.9076 |\n",
      "val: {'recall': 0.982936, 'recall_grapheme': 0.975495, 'recall_vowel': 0.992271, 'recall_consonant': 0.988483, 'acc_grapheme': 0.975001, 'acc_vowel': 0.993277, 'acc_consonant': 0.991609, 'loss_grapheme': 0.140782, 'loss_vowel': 0.085258, 'loss_consonant': 0.06346}\n",
      "   99 | 0.000013 | 159744/160678 | 3.7937 | 1.6201 |\n",
      "val: {'recall': 0.983416, 'recall_grapheme': 0.976172, 'recall_vowel': 0.992644, 'recall_consonant': 0.988675, 'acc_grapheme': 0.975549, 'acc_vowel': 0.993551, 'acc_consonant': 0.992007, 'loss_grapheme': 0.111183, 'loss_vowel': 0.049164, 'loss_consonant': 0.042358}\n",
      "  101 | 0.000011 | 045056/160678 | 0.0026 | 1.5361 |\n",
      "val: {'recall': 0.983225, 'recall_grapheme': 0.975635, 'recall_vowel': 0.992773, 'recall_consonant': 0.988856, 'acc_grapheme': 0.975425, 'acc_vowel': 0.993601, 'acc_consonant': 0.992182, 'loss_grapheme': 0.105607, 'loss_vowel': 0.037791, 'loss_consonant': 0.035624}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  102 | 0.000008 | 090112/160678 | 3.2193 | 1.6613 |\n",
      "val: {'recall': 0.983384, 'recall_grapheme': 0.975771, 'recall_vowel': 0.992706, 'recall_consonant': 0.98929, 'acc_grapheme': 0.975101, 'acc_vowel': 0.993551, 'acc_consonant': 0.991982, 'loss_grapheme': 0.123945, 'loss_vowel': 0.066179, 'loss_consonant': 0.051828}\n",
      "  103 | 0.000006 | 135168/160678 | 3.5503 | 1.6544 |\n",
      "val: {'recall': 0.983264, 'recall_grapheme': 0.975946, 'recall_vowel': 0.992679, 'recall_consonant': 0.988487, 'acc_grapheme': 0.975126, 'acc_vowel': 0.993576, 'acc_consonant': 0.992007, 'loss_grapheme': 0.112679, 'loss_vowel': 0.051317, 'loss_consonant': 0.042975}\n",
      "  105 | 0.000004 | 020480/160678 | 0.1884 | 1.6048 |\n",
      "val: {'recall': 0.983255, 'recall_grapheme': 0.975952, 'recall_vowel': 0.992432, 'recall_consonant': 0.988685, 'acc_grapheme': 0.975151, 'acc_vowel': 0.993452, 'acc_consonant': 0.992032, 'loss_grapheme': 0.124091, 'loss_vowel': 0.065983, 'loss_consonant': 0.051293}\n",
      "  106 | 0.000002 | 065536/160678 | 0.0027 | 1.1472 |\n",
      "val: {'recall': 0.983537, 'recall_grapheme': 0.976489, 'recall_vowel': 0.992634, 'recall_consonant': 0.988539, 'acc_grapheme': 0.976097, 'acc_vowel': 0.993676, 'acc_consonant': 0.99248, 'loss_grapheme': 0.111751, 'loss_vowel': 0.031356, 'loss_consonant': 0.033395}\n",
      "  107 | 0.000001 | 110592/160678 | 2.4676 | 1.1637 |\n",
      "val: {'recall': 0.983478, 'recall_grapheme': 0.976265, 'recall_vowel': 0.992918, 'recall_consonant': 0.988464, 'acc_grapheme': 0.975923, 'acc_vowel': 0.99375, 'acc_consonant': 0.99248, 'loss_grapheme': 0.105172, 'loss_vowel': 0.031758, 'loss_consonant': 0.032412}\n",
      "  108 | 0.000001 | 155648/160678 | 2.8665 | 1.3027 |\n",
      "val: {'recall': 0.983668, 'recall_grapheme': 0.976209, 'recall_vowel': 0.992816, 'recall_consonant': 0.989438, 'acc_grapheme': 0.975674, 'acc_vowel': 0.993651, 'acc_consonant': 0.992331, 'loss_grapheme': 0.105692, 'loss_vowel': 0.039488, 'loss_consonant': 0.036394}\n",
      "  110 | 0.000001 | 040960/160678 | 0.0078 | 1.0877 |\n",
      "val: {'recall': 0.983603, 'recall_grapheme': 0.976275, 'recall_vowel': 0.992804, 'recall_consonant': 0.989058, 'acc_grapheme': 0.975599, 'acc_vowel': 0.993676, 'acc_consonant': 0.992007, 'loss_grapheme': 0.105889, 'loss_vowel': 0.038806, 'loss_consonant': 0.036416}\n",
      "  111 | 0.000002 | 086016/160678 | 0.0015 | 1.2975 |\n",
      "val: {'recall': 0.983374, 'recall_grapheme': 0.976057, 'recall_vowel': 0.992834, 'recall_consonant': 0.988546, 'acc_grapheme': 0.9753, 'acc_vowel': 0.993626, 'acc_consonant': 0.992231, 'loss_grapheme': 0.106427, 'loss_vowel': 0.040609, 'loss_consonant': 0.03691}\n",
      "  112 | 0.000004 | 131072/160678 | 0.0019 | 1.6808 |\n",
      "val: {'recall': 0.983269, 'recall_grapheme': 0.975925, 'recall_vowel': 0.992605, 'recall_consonant': 0.988621, 'acc_grapheme': 0.975026, 'acc_vowel': 0.993402, 'acc_consonant': 0.991808, 'loss_grapheme': 0.119881, 'loss_vowel': 0.061226, 'loss_consonant': 0.049483}\n",
      "  114 | 0.000006 | 016384/160678 | 0.0075 | 1.3386 |\n",
      "val: {'recall': 0.983348, 'recall_grapheme': 0.975943, 'recall_vowel': 0.992908, 'recall_consonant': 0.988597, 'acc_grapheme': 0.97535, 'acc_vowel': 0.993676, 'acc_consonant': 0.992207, 'loss_grapheme': 0.107499, 'loss_vowel': 0.042271, 'loss_consonant': 0.038136}\n",
      "  115 | 0.000008 | 061440/160678 | 0.0039 | 1.8095 |\n",
      "val: {'recall': 0.983559, 'recall_grapheme': 0.976372, 'recall_vowel': 0.992956, 'recall_consonant': 0.988535, 'acc_grapheme': 0.975698, 'acc_vowel': 0.993775, 'acc_consonant': 0.992231, 'loss_grapheme': 0.105071, 'loss_vowel': 0.036511, 'loss_consonant': 0.034657}\n",
      "  116 | 0.000011 | 106496/160678 | 3.5594 | 1.6069 |\n",
      "val: {'recall': 0.983717, 'recall_grapheme': 0.976432, 'recall_vowel': 0.992835, 'recall_consonant': 0.989169, 'acc_grapheme': 0.9752, 'acc_vowel': 0.993651, 'acc_consonant': 0.991958, 'loss_grapheme': 0.11137, 'loss_vowel': 0.050225, 'loss_consonant': 0.042515}\n",
      "  117 | 0.000013 | 151552/160678 | 2.4130 | 1.3666 |\n",
      "val: {'recall': 0.983471, 'recall_grapheme': 0.97606, 'recall_vowel': 0.99305, 'recall_consonant': 0.988713, 'acc_grapheme': 0.975698, 'acc_vowel': 0.993775, 'acc_consonant': 0.992231, 'loss_grapheme': 0.104666, 'loss_vowel': 0.036759, 'loss_consonant': 0.034717}\n",
      "  119 | 0.000015 | 036864/160678 | 1.4353 | 2.0481 |\n",
      "val: {'recall': 0.983366, 'recall_grapheme': 0.976132, 'recall_vowel': 0.992516, 'recall_consonant': 0.988685, 'acc_grapheme': 0.974927, 'acc_vowel': 0.993352, 'acc_consonant': 0.991758, 'loss_grapheme': 0.164789, 'loss_vowel': 0.105517, 'loss_consonant': 0.077108}\n",
      "  120 | 0.000017 | 081920/160678 | 4.4925 | 1.1914 |\n",
      "val: {'recall': 0.983461, 'recall_grapheme': 0.97585, 'recall_vowel': 0.992976, 'recall_consonant': 0.989168, 'acc_grapheme': 0.975549, 'acc_vowel': 0.993875, 'acc_consonant': 0.992306, 'loss_grapheme': 0.104823, 'loss_vowel': 0.033901, 'loss_consonant': 0.032969}\n",
      "  121 | 0.000019 | 126976/160678 | 2.2983 | 1.6210 |\n",
      "val: {'recall': 0.983315, 'recall_grapheme': 0.975891, 'recall_vowel': 0.992729, 'recall_consonant': 0.988748, 'acc_grapheme': 0.975176, 'acc_vowel': 0.993501, 'acc_consonant': 0.992107, 'loss_grapheme': 0.108462, 'loss_vowel': 0.044221, 'loss_consonant': 0.039104}\n",
      "  123 | 0.000020 | 012288/160678 | 4.0240 | 2.1089 |\n",
      "val: {'recall': 0.983194, 'recall_grapheme': 0.975861, 'recall_vowel': 0.992353, 'recall_consonant': 0.988703, 'acc_grapheme': 0.974902, 'acc_vowel': 0.993452, 'acc_consonant': 0.992132, 'loss_grapheme': 0.114677, 'loss_vowel': 0.056085, 'loss_consonant': 0.045902}\n",
      "  124 | 0.000020 | 057344/160678 | 0.5300 | 1.3763 |\n",
      "val: {'recall': 0.983525, 'recall_grapheme': 0.976303, 'recall_vowel': 0.992934, 'recall_consonant': 0.988562, 'acc_grapheme': 0.975499, 'acc_vowel': 0.993701, 'acc_consonant': 0.992555, 'loss_grapheme': 0.104817, 'loss_vowel': 0.033145, 'loss_consonant': 0.032644}\n",
      "  125 | 0.000020 | 102400/160678 | 0.0023 | 1.4069 |\n",
      "val: {'recall': 0.98343, 'recall_grapheme': 0.97578, 'recall_vowel': 0.992959, 'recall_consonant': 0.989201, 'acc_grapheme': 0.975798, 'acc_vowel': 0.99375, 'acc_consonant': 0.992431, 'loss_grapheme': 0.105453, 'loss_vowel': 0.031785, 'loss_consonant': 0.032183}\n",
      "  126 | 0.000019 | 147456/160678 | 4.0786 | 1.6669 |\n",
      "val: {'recall': 0.983115, 'recall_grapheme': 0.97558, 'recall_vowel': 0.992488, 'recall_consonant': 0.988811, 'acc_grapheme': 0.974852, 'acc_vowel': 0.993551, 'acc_consonant': 0.991958, 'loss_grapheme': 0.127174, 'loss_vowel': 0.071567, 'loss_consonant': 0.056192}\n",
      "  128 | 0.000017 | 032768/160678 | 3.7940 | 2.1346 |\n",
      "val: {'recall': 0.982879, 'recall_grapheme': 0.975329, 'recall_vowel': 0.992211, 'recall_consonant': 0.988647, 'acc_grapheme': 0.974603, 'acc_vowel': 0.993327, 'acc_consonant': 0.991883, 'loss_grapheme': 0.151127, 'loss_vowel': 0.095031, 'loss_consonant': 0.06942}\n",
      "  129 | 0.000015 | 077824/160678 | 0.0026 | 1.2522 |\n",
      "val: {'recall': 0.983328, 'recall_grapheme': 0.975779, 'recall_vowel': 0.99302, 'recall_consonant': 0.988731, 'acc_grapheme': 0.975773, 'acc_vowel': 0.9939, 'acc_consonant': 0.992431, 'loss_grapheme': 0.104978, 'loss_vowel': 0.032121, 'loss_consonant': 0.032466}\n",
      "  130 | 0.000013 | 122880/160678 | 0.0034 | 1.6834 |\n",
      "val: {'recall': 0.983682, 'recall_grapheme': 0.976215, 'recall_vowel': 0.992982, 'recall_consonant': 0.989318, 'acc_grapheme': 0.975624, 'acc_vowel': 0.99375, 'acc_consonant': 0.992182, 'loss_grapheme': 0.105085, 'loss_vowel': 0.035285, 'loss_consonant': 0.034422}\n",
      "  132 | 0.000011 | 008192/160678 | 2.2574 | 1.3663 |\n",
      "val: {'recall': 0.983371, 'recall_grapheme': 0.975931, 'recall_vowel': 0.992866, 'recall_consonant': 0.988753, 'acc_grapheme': 0.97535, 'acc_vowel': 0.993651, 'acc_consonant': 0.992032, 'loss_grapheme': 0.113208, 'loss_vowel': 0.052764, 'loss_consonant': 0.043945}\n",
      "  133 | 0.000008 | 053248/160678 | 3.2625 | 1.6818 |\n",
      "val: {'recall': 0.983529, 'recall_grapheme': 0.976016, 'recall_vowel': 0.992766, 'recall_consonant': 0.98932, 'acc_grapheme': 0.975176, 'acc_vowel': 0.993576, 'acc_consonant': 0.991958, 'loss_grapheme': 0.114332, 'loss_vowel': 0.052941, 'loss_consonant': 0.044601}\n",
      "  134 | 0.000006 | 098304/160678 | 3.3615 | 1.3337 |\n",
      "val: {'recall': 0.983712, 'recall_grapheme': 0.976314, 'recall_vowel': 0.992947, 'recall_consonant': 0.989272, 'acc_grapheme': 0.975499, 'acc_vowel': 0.99375, 'acc_consonant': 0.992256, 'loss_grapheme': 0.105666, 'loss_vowel': 0.037937, 'loss_consonant': 0.035477}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  135 | 0.000004 | 143360/160678 | 0.0017 | 1.4419 |\n",
      "val: {'recall': 0.983316, 'recall_grapheme': 0.975753, 'recall_vowel': 0.992939, 'recall_consonant': 0.988818, 'acc_grapheme': 0.975449, 'acc_vowel': 0.993701, 'acc_consonant': 0.992406, 'loss_grapheme': 0.105309, 'loss_vowel': 0.036222, 'loss_consonant': 0.034523}\n",
      "  137 | 0.000002 | 028672/160678 | 2.1682 | 1.4081 |\n",
      "val: {'recall': 0.983511, 'recall_grapheme': 0.975977, 'recall_vowel': 0.992775, 'recall_consonant': 0.989315, 'acc_grapheme': 0.975549, 'acc_vowel': 0.993601, 'acc_consonant': 0.992182, 'loss_grapheme': 0.108885, 'loss_vowel': 0.044763, 'loss_consonant': 0.039639}\n",
      "  138 | 0.000001 | 073728/160678 | 0.0079 | 1.4058 |\n",
      "val: {'recall': 0.983533, 'recall_grapheme': 0.976217, 'recall_vowel': 0.992976, 'recall_consonant': 0.988723, 'acc_grapheme': 0.975574, 'acc_vowel': 0.993725, 'acc_consonant': 0.992207, 'loss_grapheme': 0.106639, 'loss_vowel': 0.03966, 'loss_consonant': 0.036478}\n",
      "  139 | 0.000001 | 118784/160678 | 0.0021 | 1.4227 |\n",
      "val: {'recall': 0.98354, 'recall_grapheme': 0.975871, 'recall_vowel': 0.993061, 'recall_consonant': 0.989357, 'acc_grapheme': 0.9752, 'acc_vowel': 0.993775, 'acc_consonant': 0.992231, 'loss_grapheme': 0.10591, 'loss_vowel': 0.037617, 'loss_consonant': 0.035751}\n",
      "  141 | 0.000001 | 004096/160678 | 0.0020 | 0.0023 |\n",
      "val: {'recall': 0.983681, 'recall_grapheme': 0.976652, 'recall_vowel': 0.992972, 'recall_consonant': 0.988448, 'acc_grapheme': 0.976172, 'acc_vowel': 0.9938, 'acc_consonant': 0.992406, 'loss_grapheme': 0.109449, 'loss_vowel': 0.030694, 'loss_consonant': 0.032478}\n",
      "  142 | 0.000002 | 049152/160678 | 2.6205 | 2.0025 |\n",
      "val: {'recall': 0.983549, 'recall_grapheme': 0.975986, 'recall_vowel': 0.992899, 'recall_consonant': 0.989322, 'acc_grapheme': 0.9752, 'acc_vowel': 0.993701, 'acc_consonant': 0.992207, 'loss_grapheme': 0.107722, 'loss_vowel': 0.042637, 'loss_consonant': 0.038133}\n",
      "  143 | 0.000004 | 094208/160678 | 4.4630 | 1.5201 |\n",
      "val: {'recall': 0.983276, 'recall_grapheme': 0.975891, 'recall_vowel': 0.992621, 'recall_consonant': 0.988701, 'acc_grapheme': 0.975001, 'acc_vowel': 0.993526, 'acc_consonant': 0.991833, 'loss_grapheme': 0.126204, 'loss_vowel': 0.068813, 'loss_consonant': 0.05374}\n",
      "  144 | 0.000006 | 139264/160678 | 3.6950 | 1.7547 |\n",
      "val: {'recall': 0.983397, 'recall_grapheme': 0.975929, 'recall_vowel': 0.992569, 'recall_consonant': 0.989162, 'acc_grapheme': 0.975225, 'acc_vowel': 0.993601, 'acc_consonant': 0.991958, 'loss_grapheme': 0.126302, 'loss_vowel': 0.067046, 'loss_consonant': 0.052577}\n",
      "  146 | 0.000008 | 024576/160678 | 4.3076 | 1.2472 |\n",
      "val: {'recall': 0.983372, 'recall_grapheme': 0.975992, 'recall_vowel': 0.992781, 'recall_consonant': 0.988724, 'acc_grapheme': 0.975275, 'acc_vowel': 0.993725, 'acc_consonant': 0.992306, 'loss_grapheme': 0.106375, 'loss_vowel': 0.039701, 'loss_consonant': 0.036206}\n",
      "  147 | 0.000010 | 069632/160678 | 1.3509 | 1.7234 |\n",
      "val: {'recall': 0.983254, 'recall_grapheme': 0.975926, 'recall_vowel': 0.992563, 'recall_consonant': 0.988602, 'acc_grapheme': 0.975176, 'acc_vowel': 0.993402, 'acc_consonant': 0.991833, 'loss_grapheme': 0.125212, 'loss_vowel': 0.065295, 'loss_consonant': 0.0517}\n",
      "  148 | 0.000013 | 114688/160678 | 4.1273 | 1.6907 |\n",
      "val: {'recall': 0.983272, 'recall_grapheme': 0.975974, 'recall_vowel': 0.992359, 'recall_consonant': 0.988782, 'acc_grapheme': 0.974877, 'acc_vowel': 0.993352, 'acc_consonant': 0.991908, 'loss_grapheme': 0.15285, 'loss_vowel': 0.099976, 'loss_consonant': 0.071852}\n",
      "  149 | 0.000015 | 159744/160678 | 0.0034 | 1.4092 |\n",
      "val: {'recall': 0.9834, 'recall_grapheme': 0.975936, 'recall_vowel': 0.993024, 'recall_consonant': 0.988705, 'acc_grapheme': 0.975574, 'acc_vowel': 0.993825, 'acc_consonant': 0.992331, 'loss_grapheme': 0.105203, 'loss_vowel': 0.037183, 'loss_consonant': 0.034716}\n",
      "  151 | 0.000017 | 045056/160678 | 0.0048 | 1.5369 |\n",
      "val: {'recall': 0.983511, 'recall_grapheme': 0.976216, 'recall_vowel': 0.993007, 'recall_consonant': 0.988605, 'acc_grapheme': 0.975449, 'acc_vowel': 0.99375, 'acc_consonant': 0.992157, 'loss_grapheme': 0.106716, 'loss_vowel': 0.041268, 'loss_consonant': 0.037182}\n",
      "  152 | 0.000019 | 090112/160678 | 2.1068 | 1.4484 |\n",
      "val: {'recall': 0.983295, 'recall_grapheme': 0.97594, 'recall_vowel': 0.992531, 'recall_consonant': 0.98877, 'acc_grapheme': 0.97535, 'acc_vowel': 0.993526, 'acc_consonant': 0.991982, 'loss_grapheme': 0.109269, 'loss_vowel': 0.045445, 'loss_consonant': 0.040337}\n",
      "  153 | 0.000020 | 135168/160678 | 3.2091 | 1.7013 |\n",
      "val: {'recall': 0.983081, 'recall_grapheme': 0.975593, 'recall_vowel': 0.992333, 'recall_consonant': 0.988807, 'acc_grapheme': 0.974877, 'acc_vowel': 0.993452, 'acc_consonant': 0.992057, 'loss_grapheme': 0.124659, 'loss_vowel': 0.067193, 'loss_consonant': 0.052289}\n",
      "  155 | 0.000020 | 020480/160678 | 3.6174 | 2.1333 |\n",
      "val: {'recall': 0.983087, 'recall_grapheme': 0.975618, 'recall_vowel': 0.992379, 'recall_consonant': 0.988731, 'acc_grapheme': 0.974702, 'acc_vowel': 0.993302, 'acc_consonant': 0.991933, 'loss_grapheme': 0.151168, 'loss_vowel': 0.097487, 'loss_consonant': 0.071502}\n",
      "  156 | 0.000020 | 065536/160678 | 3.9698 | 1.3944 |\n",
      "val: {'recall': 0.983483, 'recall_grapheme': 0.975842, 'recall_vowel': 0.993055, 'recall_consonant': 0.989193, 'acc_grapheme': 0.975474, 'acc_vowel': 0.993576, 'acc_consonant': 0.991883, 'loss_grapheme': 0.117751, 'loss_vowel': 0.057125, 'loss_consonant': 0.046961}\n",
      "  157 | 0.000019 | 110592/160678 | 4.0916 | 1.6491 |\n",
      "val: {'recall': 0.983098, 'recall_grapheme': 0.975195, 'recall_vowel': 0.992888, 'recall_consonant': 0.989115, 'acc_grapheme': 0.975076, 'acc_vowel': 0.993626, 'acc_consonant': 0.991958, 'loss_grapheme': 0.116623, 'loss_vowel': 0.05701, 'loss_consonant': 0.046909}\n",
      "  158 | 0.000017 | 155648/160678 | 0.0014 | 1.6783 |\n",
      "val: {'recall': 0.983551, 'recall_grapheme': 0.976, 'recall_vowel': 0.993014, 'recall_consonant': 0.989191, 'acc_grapheme': 0.975524, 'acc_vowel': 0.993701, 'acc_consonant': 0.992082, 'loss_grapheme': 0.108621, 'loss_vowel': 0.044429, 'loss_consonant': 0.039615}\n",
      "  160 | 0.000015 | 040960/160678 | 0.0051 | 1.8263 |\n",
      "val: {'recall': 0.983223, 'recall_grapheme': 0.975731, 'recall_vowel': 0.993067, 'recall_consonant': 0.988361, 'acc_grapheme': 0.974976, 'acc_vowel': 0.993626, 'acc_consonant': 0.991982, 'loss_grapheme': 0.112102, 'loss_vowel': 0.049858, 'loss_consonant': 0.04293}\n",
      "  161 | 0.000013 | 086016/160678 | 2.9037 | 1.4646 |\n",
      "val: {'recall': 0.983537, 'recall_grapheme': 0.976301, 'recall_vowel': 0.993141, 'recall_consonant': 0.988405, 'acc_grapheme': 0.975474, 'acc_vowel': 0.993725, 'acc_consonant': 0.992306, 'loss_grapheme': 0.107711, 'loss_vowel': 0.041974, 'loss_consonant': 0.037473}\n",
      "  162 | 0.000010 | 131072/160678 | 0.0031 | 1.2038 |\n",
      "val: {'recall': 0.98346, 'recall_grapheme': 0.97573, 'recall_vowel': 0.993116, 'recall_consonant': 0.989265, 'acc_grapheme': 0.975325, 'acc_vowel': 0.993875, 'acc_consonant': 0.992132, 'loss_grapheme': 0.106098, 'loss_vowel': 0.038701, 'loss_consonant': 0.036051}\n",
      "  164 | 0.000008 | 016384/160678 | 2.6546 | 2.5800 |\n",
      "val: {'recall': 0.983246, 'recall_grapheme': 0.975861, 'recall_vowel': 0.992458, 'recall_consonant': 0.988803, 'acc_grapheme': 0.974752, 'acc_vowel': 0.993501, 'acc_consonant': 0.991958, 'loss_grapheme': 0.187118, 'loss_vowel': 0.127386, 'loss_consonant': 0.0876}\n",
      "  165 | 0.000006 | 061440/160678 | 1.6027 | 1.7762 |\n",
      "val: {'recall': 0.98357, 'recall_grapheme': 0.976032, 'recall_vowel': 0.992982, 'recall_consonant': 0.989235, 'acc_grapheme': 0.975499, 'acc_vowel': 0.993701, 'acc_consonant': 0.992032, 'loss_grapheme': 0.106665, 'loss_vowel': 0.041227, 'loss_consonant': 0.037477}\n",
      "  166 | 0.000004 | 106496/160678 | 3.6294 | 1.2582 |\n",
      "val: {'recall': 0.983435, 'recall_grapheme': 0.976097, 'recall_vowel': 0.992859, 'recall_consonant': 0.988686, 'acc_grapheme': 0.975549, 'acc_vowel': 0.993701, 'acc_consonant': 0.992306, 'loss_grapheme': 0.104739, 'loss_vowel': 0.036475, 'loss_consonant': 0.034673}\n",
      "  167 | 0.000002 | 151552/160678 | 4.1773 | 1.4944 |\n",
      "val: {'recall': 0.983494, 'recall_grapheme': 0.976161, 'recall_vowel': 0.992872, 'recall_consonant': 0.988783, 'acc_grapheme': 0.975449, 'acc_vowel': 0.993676, 'acc_consonant': 0.992082, 'loss_grapheme': 0.11525, 'loss_vowel': 0.055067, 'loss_consonant': 0.04548}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  169 | 0.000001 | 036864/160678 | 0.0110 | 1.4008 |\n",
      "val: {'recall': 0.983516, 'recall_grapheme': 0.976102, 'recall_vowel': 0.993104, 'recall_consonant': 0.988754, 'acc_grapheme': 0.975748, 'acc_vowel': 0.9939, 'acc_consonant': 0.992057, 'loss_grapheme': 0.106091, 'loss_vowel': 0.040274, 'loss_consonant': 0.037128}\n",
      "  170 | 0.000001 | 081920/160678 | 0.0018 | 1.4808 |\n",
      "val: {'recall': 0.983635, 'recall_grapheme': 0.976406, 'recall_vowel': 0.993101, 'recall_consonant': 0.988626, 'acc_grapheme': 0.975898, 'acc_vowel': 0.993875, 'acc_consonant': 0.992207, 'loss_grapheme': 0.104631, 'loss_vowel': 0.033412, 'loss_consonant': 0.033421}\n",
      "  171 | 0.000001 | 126976/160678 | 2.7245 | 1.5091 |\n",
      "val: {'recall': 0.983376, 'recall_grapheme': 0.97611, 'recall_vowel': 0.992797, 'recall_consonant': 0.988488, 'acc_grapheme': 0.975549, 'acc_vowel': 0.993725, 'acc_consonant': 0.992057, 'loss_grapheme': 0.110463, 'loss_vowel': 0.048373, 'loss_consonant': 0.041056}\n",
      "  173 | 0.000002 | 012288/160678 | 0.0029 | 0.7979 |\n",
      "val: {'recall': 0.983411, 'recall_grapheme': 0.976093, 'recall_vowel': 0.992963, 'recall_consonant': 0.988493, 'acc_grapheme': 0.975898, 'acc_vowel': 0.99375, 'acc_consonant': 0.992281, 'loss_grapheme': 0.105676, 'loss_vowel': 0.031557, 'loss_consonant': 0.032759}\n",
      "  174 | 0.000004 | 057344/160678 | 3.8666 | 1.5683 |\n",
      "val: {'recall': 0.983386, 'recall_grapheme': 0.976094, 'recall_vowel': 0.992574, 'recall_consonant': 0.988782, 'acc_grapheme': 0.975026, 'acc_vowel': 0.993501, 'acc_consonant': 0.992032, 'loss_grapheme': 0.123762, 'loss_vowel': 0.068041, 'loss_consonant': 0.05295}\n",
      "  175 | 0.000006 | 102400/160678 | 3.8756 | 1.6319 |\n",
      "val: {'recall': 0.983439, 'recall_grapheme': 0.976299, 'recall_vowel': 0.992295, 'recall_consonant': 0.988862, 'acc_grapheme': 0.974877, 'acc_vowel': 0.993352, 'acc_consonant': 0.992007, 'loss_grapheme': 0.183649, 'loss_vowel': 0.129797, 'loss_consonant': 0.087704}\n",
      "  176 | 0.000008 | 147456/160678 | 0.0021 | 1.7482 |\n",
      "val: {'recall': 0.983431, 'recall_grapheme': 0.976003, 'recall_vowel': 0.993075, 'recall_consonant': 0.988643, 'acc_grapheme': 0.975524, 'acc_vowel': 0.99385, 'acc_consonant': 0.992157, 'loss_grapheme': 0.105156, 'loss_vowel': 0.034894, 'loss_consonant': 0.034089}\n",
      "  178 | 0.000010 | 032768/160678 | 0.0013 | 1.4567 |\n",
      "val: {'recall': 0.983267, 'recall_grapheme': 0.975862, 'recall_vowel': 0.992785, 'recall_consonant': 0.988558, 'acc_grapheme': 0.975325, 'acc_vowel': 0.993775, 'acc_consonant': 0.992356, 'loss_grapheme': 0.105401, 'loss_vowel': 0.035159, 'loss_consonant': 0.034057}\n",
      "  179 | 0.000013 | 077824/160678 | 4.1796 | 1.6869 |\n",
      "val: {'recall': 0.982913, 'recall_grapheme': 0.975166, 'recall_vowel': 0.992689, 'recall_consonant': 0.98863, 'acc_grapheme': 0.974304, 'acc_vowel': 0.993501, 'acc_consonant': 0.991684, 'loss_grapheme': 0.169705, 'loss_vowel': 0.117473, 'loss_consonant': 0.083806}\n",
      "  180 | 0.000015 | 122880/160678 | 4.1932 | 1.6308 |\n",
      "val: {'recall': 0.983192, 'recall_grapheme': 0.975583, 'recall_vowel': 0.992926, 'recall_consonant': 0.988676, 'acc_grapheme': 0.974951, 'acc_vowel': 0.993626, 'acc_consonant': 0.991958, 'loss_grapheme': 0.11591, 'loss_vowel': 0.0554, 'loss_consonant': 0.045844}\n",
      "  182 | 0.000017 | 008192/160678 | 2.0935 | 1.2873 |\n",
      "val: {'recall': 0.983347, 'recall_grapheme': 0.97584, 'recall_vowel': 0.992988, 'recall_consonant': 0.988721, 'acc_grapheme': 0.975449, 'acc_vowel': 0.99375, 'acc_consonant': 0.992281, 'loss_grapheme': 0.110733, 'loss_vowel': 0.047849, 'loss_consonant': 0.041402}\n",
      "  183 | 0.000019 | 053248/160678 | 0.0017 | 1.3625 |\n",
      "val: {'recall': 0.983581, 'recall_grapheme': 0.976092, 'recall_vowel': 0.992921, 'recall_consonant': 0.989217, 'acc_grapheme': 0.975375, 'acc_vowel': 0.993725, 'acc_consonant': 0.992082, 'loss_grapheme': 0.105241, 'loss_vowel': 0.036791, 'loss_consonant': 0.035045}\n",
      "  184 | 0.000020 | 098304/160678 | 0.0016 | 1.5351 |\n",
      "val: {'recall': 0.983548, 'recall_grapheme': 0.976276, 'recall_vowel': 0.992905, 'recall_consonant': 0.988736, 'acc_grapheme': 0.975524, 'acc_vowel': 0.993626, 'acc_consonant': 0.992381, 'loss_grapheme': 0.105403, 'loss_vowel': 0.032832, 'loss_consonant': 0.032811}\n",
      "  185 | 0.000020 | 143360/160678 | 4.7011 | 1.4088 |\n",
      "val: {'recall': 0.983277, 'recall_grapheme': 0.975886, 'recall_vowel': 0.992724, 'recall_consonant': 0.988612, 'acc_grapheme': 0.975524, 'acc_vowel': 0.993526, 'acc_consonant': 0.992107, 'loss_grapheme': 0.10678, 'loss_vowel': 0.042111, 'loss_consonant': 0.038212}\n",
      "  187 | 0.000020 | 028672/160678 | 4.4689 | 1.3892 |\n",
      "val: {'recall': 0.983278, 'recall_grapheme': 0.975897, 'recall_vowel': 0.992671, 'recall_consonant': 0.988647, 'acc_grapheme': 0.975325, 'acc_vowel': 0.993526, 'acc_consonant': 0.992057, 'loss_grapheme': 0.107238, 'loss_vowel': 0.042421, 'loss_consonant': 0.03843}\n",
      "  188 | 0.000019 | 073728/160678 | 1.9787 | 1.5340 |\n",
      "val: {'recall': 0.98357, 'recall_grapheme': 0.976471, 'recall_vowel': 0.99256, 'recall_consonant': 0.988776, 'acc_grapheme': 0.9753, 'acc_vowel': 0.993476, 'acc_consonant': 0.992132, 'loss_grapheme': 0.126544, 'loss_vowel': 0.068733, 'loss_consonant': 0.053139}\n",
      "  189 | 0.000017 | 118784/160678 | 4.2535 | 1.3499 |\n",
      "val: {'recall': 0.983534, 'recall_grapheme': 0.976468, 'recall_vowel': 0.992608, 'recall_consonant': 0.988593, 'acc_grapheme': 0.975375, 'acc_vowel': 0.993476, 'acc_consonant': 0.992057, 'loss_grapheme': 0.108257, 'loss_vowel': 0.044234, 'loss_consonant': 0.03921}\n",
      "  191 | 0.000015 | 004096/160678 | 2.9291 | 1.7444 |\n",
      "val: {'recall': 0.983227, 'recall_grapheme': 0.975836, 'recall_vowel': 0.992654, 'recall_consonant': 0.988582, 'acc_grapheme': 0.974976, 'acc_vowel': 0.993551, 'acc_consonant': 0.991958, 'loss_grapheme': 0.11769, 'loss_vowel': 0.059005, 'loss_consonant': 0.047552}\n",
      "  192 | 0.000013 | 049152/160678 | 3.8215 | 1.5262 |\n",
      "val: {'recall': 0.983362, 'recall_grapheme': 0.976029, 'recall_vowel': 0.992777, 'recall_consonant': 0.98861, 'acc_grapheme': 0.974802, 'acc_vowel': 0.993551, 'acc_consonant': 0.992057, 'loss_grapheme': 0.112552, 'loss_vowel': 0.050271, 'loss_consonant': 0.042884}\n",
      "  193 | 0.000010 | 094208/160678 | 0.0018 | 1.3892 |\n",
      "val: {'recall': 0.983278, 'recall_grapheme': 0.975904, 'recall_vowel': 0.99277, 'recall_consonant': 0.988533, 'acc_grapheme': 0.975474, 'acc_vowel': 0.993701, 'acc_consonant': 0.992207, 'loss_grapheme': 0.10611, 'loss_vowel': 0.038874, 'loss_consonant': 0.035901}\n",
      "  194 | 0.000008 | 139264/160678 | 3.4429 | 1.2170 |\n",
      "val: {'recall': 0.983463, 'recall_grapheme': 0.976223, 'recall_vowel': 0.992796, 'recall_consonant': 0.988611, 'acc_grapheme': 0.974951, 'acc_vowel': 0.993651, 'acc_consonant': 0.991933, 'loss_grapheme': 0.112257, 'loss_vowel': 0.05089, 'loss_consonant': 0.043249}\n",
      "  196 | 0.000006 | 024576/160678 | 0.6270 | 1.7887 |\n",
      "val: {'recall': 0.983238, 'recall_grapheme': 0.975687, 'recall_vowel': 0.992901, 'recall_consonant': 0.988677, 'acc_grapheme': 0.97525, 'acc_vowel': 0.993626, 'acc_consonant': 0.992032, 'loss_grapheme': 0.115865, 'loss_vowel': 0.055774, 'loss_consonant': 0.045736}\n",
      "  197 | 0.000004 | 069632/160678 | 1.0858 | 1.3545 |\n",
      "val: {'recall': 0.983568, 'recall_grapheme': 0.976319, 'recall_vowel': 0.992993, 'recall_consonant': 0.988643, 'acc_grapheme': 0.975698, 'acc_vowel': 0.99375, 'acc_consonant': 0.992107, 'loss_grapheme': 0.107598, 'loss_vowel': 0.042633, 'loss_consonant': 0.038441}\n",
      "  198 | 0.000002 | 114688/160678 | 3.2123 | 1.4263 |\n",
      "val: {'recall': 0.983334, 'recall_grapheme': 0.976102, 'recall_vowel': 0.992573, 'recall_consonant': 0.988559, 'acc_grapheme': 0.9752, 'acc_vowel': 0.993551, 'acc_consonant': 0.991982, 'loss_grapheme': 0.108869, 'loss_vowel': 0.045786, 'loss_consonant': 0.040098}\n",
      "  199 | 0.000001 | 159744/160678 | 3.3111 | 1.7829 |\n",
      "val: {'recall': 0.983573, 'recall_grapheme': 0.9764, 'recall_vowel': 0.992811, 'recall_consonant': 0.988683, 'acc_grapheme': 0.975101, 'acc_vowel': 0.993601, 'acc_consonant': 0.991982, 'loss_grapheme': 0.124215, 'loss_vowel': 0.065783, 'loss_consonant': 0.051714}\n",
      "  201 | 0.000001 | 045056/160678 | 3.2226 | 1.8006 |\n",
      "val: {'recall': 0.983555, 'recall_grapheme': 0.976464, 'recall_vowel': 0.992686, 'recall_consonant': 0.988606, 'acc_grapheme': 0.975225, 'acc_vowel': 0.993626, 'acc_consonant': 0.992007, 'loss_grapheme': 0.121478, 'loss_vowel': 0.064189, 'loss_consonant': 0.05049}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  202 | 0.000001 | 090112/160678 | 0.0037 | 1.3208 |\n",
      "val: {'recall': 0.983565, 'recall_grapheme': 0.976315, 'recall_vowel': 0.993049, 'recall_consonant': 0.988583, 'acc_grapheme': 0.975698, 'acc_vowel': 0.993775, 'acc_consonant': 0.992207, 'loss_grapheme': 0.105218, 'loss_vowel': 0.032946, 'loss_consonant': 0.033024}\n",
      "  203 | 0.000002 | 135168/160678 | 0.0018 | 1.4626 |\n",
      "val: {'recall': 0.98321, 'recall_grapheme': 0.97565, 'recall_vowel': 0.992999, 'recall_consonant': 0.988543, 'acc_grapheme': 0.9753, 'acc_vowel': 0.9938, 'acc_consonant': 0.992032, 'loss_grapheme': 0.106336, 'loss_vowel': 0.039182, 'loss_consonant': 0.036255}\n",
      "  205 | 0.000004 | 020480/160678 | 0.0045 | 1.8145 |\n",
      "val: {'recall': 0.98352, 'recall_grapheme': 0.976401, 'recall_vowel': 0.992716, 'recall_consonant': 0.988564, 'acc_grapheme': 0.975425, 'acc_vowel': 0.993651, 'acc_consonant': 0.991982, 'loss_grapheme': 0.109923, 'loss_vowel': 0.047053, 'loss_consonant': 0.04117}\n",
      "  206 | 0.000006 | 065536/160678 | 3.1242 | 1.5675 |\n",
      "val: {'recall': 0.983631, 'recall_grapheme': 0.976473, 'recall_vowel': 0.992866, 'recall_consonant': 0.988709, 'acc_grapheme': 0.975151, 'acc_vowel': 0.993601, 'acc_consonant': 0.992057, 'loss_grapheme': 0.112823, 'loss_vowel': 0.051175, 'loss_consonant': 0.043685}\n",
      "  207 | 0.000008 | 110592/160678 | 0.0027 | 1.5942 |\n",
      "val: {'recall': 0.983518, 'recall_grapheme': 0.976303, 'recall_vowel': 0.992889, 'recall_consonant': 0.988577, 'acc_grapheme': 0.975624, 'acc_vowel': 0.993725, 'acc_consonant': 0.992331, 'loss_grapheme': 0.104522, 'loss_vowel': 0.035327, 'loss_consonant': 0.034014}\n",
      "  208 | 0.000010 | 155648/160678 | 0.0025 | 1.7009 |\n",
      "val: {'recall': 0.983832, 'recall_grapheme': 0.976587, 'recall_vowel': 0.992964, 'recall_consonant': 0.989192, 'acc_grapheme': 0.975499, 'acc_vowel': 0.993725, 'acc_consonant': 0.992082, 'loss_grapheme': 0.10933, 'loss_vowel': 0.045842, 'loss_consonant': 0.039667}\n",
      "** saved\n",
      "  210 | 0.000013 | 040960/160678 | 3.3896 | 1.7702 |\n",
      "val: {'recall': 0.983546, 'recall_grapheme': 0.976052, 'recall_vowel': 0.992739, 'recall_consonant': 0.989342, 'acc_grapheme': 0.975101, 'acc_vowel': 0.993626, 'acc_consonant': 0.992132, 'loss_grapheme': 0.120093, 'loss_vowel': 0.062449, 'loss_consonant': 0.049653}\n",
      "  211 | 0.000015 | 086016/160678 | 0.0035 | 0.8647 |\n",
      "val: {'recall': 0.983385, 'recall_grapheme': 0.975788, 'recall_vowel': 0.993041, 'recall_consonant': 0.988922, 'acc_grapheme': 0.976097, 'acc_vowel': 0.9938, 'acc_consonant': 0.992655, 'loss_grapheme': 0.104845, 'loss_vowel': 0.031892, 'loss_consonant': 0.032482}\n",
      "  212 | 0.000017 | 131072/160678 | 0.0022 | 1.4260 |\n",
      "val: {'recall': 0.983517, 'recall_grapheme': 0.976155, 'recall_vowel': 0.993139, 'recall_consonant': 0.988618, 'acc_grapheme': 0.975499, 'acc_vowel': 0.993875, 'acc_consonant': 0.992456, 'loss_grapheme': 0.106772, 'loss_vowel': 0.041248, 'loss_consonant': 0.037196}\n",
      "  214 | 0.000019 | 016384/160678 | 0.0034 | 1.6873 |\n",
      "val: {'recall': 0.983575, 'recall_grapheme': 0.976269, 'recall_vowel': 0.993092, 'recall_consonant': 0.988669, 'acc_grapheme': 0.975773, 'acc_vowel': 0.993875, 'acc_consonant': 0.992331, 'loss_grapheme': 0.105832, 'loss_vowel': 0.040015, 'loss_consonant': 0.036855}\n",
      "  215 | 0.000020 | 061440/160678 | 0.0025 | 1.6982 |\n",
      "val: {'recall': 0.983352, 'recall_grapheme': 0.975958, 'recall_vowel': 0.992977, 'recall_consonant': 0.988515, 'acc_grapheme': 0.975151, 'acc_vowel': 0.993725, 'acc_consonant': 0.992306, 'loss_grapheme': 0.113833, 'loss_vowel': 0.052157, 'loss_consonant': 0.043535}\n",
      "  216 | 0.000020 | 106496/160678 | 2.7402 | 1.6183 |\n",
      "val: {'recall': 0.983532, 'recall_grapheme': 0.9758, 'recall_vowel': 0.993502, 'recall_consonant': 0.989028, 'acc_grapheme': 0.975051, 'acc_vowel': 0.993974, 'acc_consonant': 0.992207, 'loss_grapheme': 0.107521, 'loss_vowel': 0.042901, 'loss_consonant': 0.038145}\n",
      "  217 | 0.000020 | 151552/160678 | 0.0015 | 1.2890 |\n",
      "val: {'recall': 0.983391, 'recall_grapheme': 0.975496, 'recall_vowel': 0.993182, 'recall_consonant': 0.989392, 'acc_grapheme': 0.9752, 'acc_vowel': 0.993825, 'acc_consonant': 0.992331, 'loss_grapheme': 0.107716, 'loss_vowel': 0.04354, 'loss_consonant': 0.038809}\n",
      "  219 | 0.000019 | 036864/160678 | 0.0020 | 1.5759 |\n",
      "val: {'recall': 0.983672, 'recall_grapheme': 0.976236, 'recall_vowel': 0.993152, 'recall_consonant': 0.989066, 'acc_grapheme': 0.975649, 'acc_vowel': 0.99375, 'acc_consonant': 0.99248, 'loss_grapheme': 0.104705, 'loss_vowel': 0.035538, 'loss_consonant': 0.034016}\n",
      "  220 | 0.000017 | 081920/160678 | 2.2647 | 1.5476 |"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-54080d9d42c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-a8db2b1e1d0d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;31m#print('train:', train_metrics)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;31m#save_model(model, model_file+'_latest')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nval:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-0e344cb1f778>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, val_loader)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mpreds1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mpreds2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss0\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mloss1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mloss2\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_model(model, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
