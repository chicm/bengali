{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time, gc\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pretrainedmodels\n",
    "from argparse import Namespace\n",
    "from sklearn.utils import shuffle\n",
    "from apex import amp\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from cvcore.data.auto_augment import RandAugment\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_map.csv\t\t       train.csv\r\n",
      "sample_submission.csv\t       train.csv.zip\r\n",
      "test.csv\t\t       train_image_data_0.parquet\r\n",
      "test_image_data_0.parquet      train_image_data_0.parquet.zip\r\n",
      "test_image_data_0.parquet.zip  train_image_data_1.parquet\r\n",
      "test_image_data_1.parquet      train_image_data_1.parquet.zip\r\n",
      "test_image_data_1.parquet.zip  train_image_data_2.parquet\r\n",
      "test_image_data_2.parquet      train_image_data_2.parquet.zip\r\n",
      "test_image_data_2.parquet.zip  train_image_data_3.parquet\r\n",
      "test_image_data_3.parquet      train_image_data_3.parquet.zip\r\n",
      "test_image_data_3.parquet.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/chec/data/bengali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /home/chec/data/bengali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/chec/data/bengali'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "class_map_df = pd.read_csv(f'{DATA_DIR}/class_map.csv')\n",
    "sample_sub_df = pd.read_csv(f'{DATA_DIR}/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>ক্ট্রো</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_1</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>হ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>খ্রী</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_3</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>র্টি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_4</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>থ্রো</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  grapheme_root  vowel_diacritic  consonant_diacritic grapheme\n",
       "0  Train_0             15                9                    5   ক্ট্রো\n",
       "1  Train_1            159                0                    0        হ\n",
       "2  Train_2             22                3                    5     খ্রী\n",
       "3  Train_3             53                2                    2     র্টি\n",
       "4  Train_4             71                9                    5     থ্রো"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 137\n",
    "WIDTH = 236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import albumentations as albu\n",
    "def get_train_augs():\n",
    "    return RandAugment(n=2, m=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class BengaliDataset(Dataset):\n",
    "    def __init__(self, df, img_df, train_mode=True, test_mode=False):\n",
    "        self.df = df\n",
    "        self.img_df = img_df\n",
    "        self.train_mode = train_mode\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = self.get_img(row.image_id)\n",
    "        #print(img.shape)\n",
    "        if self.train_mode:\n",
    "            augs = get_train_augs()\n",
    "            #img = augs(image=img)['image']\n",
    "            img = np.asarray(augs(Image.fromarray(img)))\n",
    "        \n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "        #print('###', img.shape)\n",
    "        #img = np.concatenate([img, img, img], 2)\n",
    "        #print('>>>', img.shape)\n",
    "        \n",
    "        # taken from https://www.kaggle.com/iafoss/image-preprocessing-128x128\n",
    "        #MEAN = [ 0.06922848809290576,  0.06922848809290576,  0.06922848809290576]\n",
    "        #STD = [ 0.20515700083327537,  0.20515700083327537,  0.20515700083327537]\n",
    "        \n",
    "        img = transforms.functional.to_tensor(img)\n",
    "        #img = transforms.functional.normalize(img, mean=MEAN, std=STD)\n",
    "        \n",
    "        if self.test_mode:\n",
    "            return img\n",
    "        else:\n",
    "            return img, torch.tensor([row.grapheme_root, row.vowel_diacritic, row.consonant_diacritic, row.word_label])\n",
    "\n",
    "    def get_img(self, img_id):\n",
    "        return 255 - self.img_df.loc[img_id].values.reshape(HEIGHT, WIDTH).astype(np.uint8)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "def get_train_val_loaders(batch_size=4, val_batch_size=4, ifold=0, dev_mode=False):\n",
    "    train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "\n",
    "    train_df = shuffle(train_df, random_state=1234)\n",
    "\n",
    "    grapheme_words = np.unique(train_df.grapheme.values)\n",
    "    grapheme_words_dict = {grapheme: i for i, grapheme in enumerate(grapheme_words)}\n",
    "    train_df['word_label'] = train_df['grapheme'].map(lambda x: grapheme_words_dict[x])\n",
    "\n",
    "    print(train_df.shape)\n",
    "\n",
    "    if dev_mode:\n",
    "        img_df = pd.read_parquet(f'{DATA_DIR}/train_image_data_0.parquet').set_index('image_id')\n",
    "        train_df = train_df.iloc[:1000]\n",
    "    else:\n",
    "        img_dfs = [pd.read_parquet(f'{DATA_DIR}/train_image_data_{i}.parquet') for i in range(4)]\n",
    "        img_df = pd.concat(img_dfs, axis=0).set_index('image_id')\n",
    "    print(img_df.shape)\n",
    "    #split_index = int(len(train_df) * 0.9)\n",
    "    \n",
    "    #train = train_df.iloc[:split_index]\n",
    "    #val = train_df.iloc[split_index:]\n",
    "    \n",
    "    kf = StratifiedKFold(5, random_state=1234, shuffle=True)\n",
    "    for i, (train_idx, val_idx) in enumerate(kf.split(train_df, train_df['grapheme_root'].values)):\n",
    "        if i == ifold:\n",
    "            #print(val_idx)\n",
    "            train = train_df.iloc[train_idx]\n",
    "            val = train_df.iloc[val_idx]\n",
    "            break\n",
    "    assert i == ifold\n",
    "    print(train.shape, val.shape)\n",
    "    \n",
    "    train_ds = BengaliDataset(train, img_df, True, False)\n",
    "    val_ds = BengaliDataset(val, img_df, False, False)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "    train_loader.num = len(train_ds)\n",
    "\n",
    "    val_loader = DataLoader(val_ds, batch_size=val_batch_size, shuffle=False, num_workers=8, drop_last=False)\n",
    "    val_loader.num = len(val_ds)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader, val_loader = get_train_val_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x in train_loader:\n",
    "#    print(x)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'resnet50' # could be fbresnet152 or inceptionresnetv2\n",
    "#model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet').cuda()\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import timm\n",
    "from timm.models.activations import Swish, Mish\n",
    "from timm.models.adaptive_avgmax_pool import SelectAdaptivePool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = [ 0.06922848809290576 ]\n",
    "STD = [ 0.20515700083327537 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliNet(nn.Module):\n",
    "    \"\"\"\n",
    "    EfficientNet B0-B8.\n",
    "    Args:\n",
    "        cfg (CfgNode): configs\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super(BengaliNet, self).__init__()\n",
    "        model_name = cfg.MODEL_NAME\n",
    "        pretrained = cfg.PRETRAINED\n",
    "        input_channels = cfg.IN_CHANNELS\n",
    "        pool_type = cfg.POOL_TYPE\n",
    "        drop_connect_rate = cfg.DROP_CONNECT\n",
    "        self.drop_rate = cfg.DROPOUT\n",
    "        cls_head = cfg.CLS_HEAD\n",
    "        num_total_classes = cfg.NUM_GRAPHEME_CLASSES + cfg.NUM_VOWEL_CLASSES + cfg.NUM_CONSONANT_CLASSES \\\n",
    "            + cfg.NUM_WORD_CLASSES\n",
    "\n",
    "        backbone = timm.create_model(\n",
    "            model_name=model_name,\n",
    "            pretrained=pretrained,\n",
    "            in_chans=input_channels,\n",
    "            drop_connect_rate=drop_connect_rate,\n",
    "        )\n",
    "        self.conv_stem = backbone.conv_stem\n",
    "        self.bn1 = backbone.bn1\n",
    "        self.act1 = backbone.act1\n",
    "        ### Original blocks ###\n",
    "        for i in range(len((backbone.blocks))):\n",
    "            setattr(self, \"block{}\".format(str(i)), backbone.blocks[i])\n",
    "        self.conv_head = backbone.conv_head\n",
    "        self.bn2 = backbone.bn2\n",
    "        self.act2 = backbone.act2\n",
    "        self.aux_block5 = backbone.blocks[5]\n",
    "        self.aux_num_features = self.block5[-1].bn3.num_features\n",
    "        self.aux_head4 = nn.Conv2d(self.aux_num_features, self.aux_num_features * 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(self.aux_num_features * 4)\n",
    "        self.act4 = Swish()\n",
    "        self.aux_head5 = nn.Conv2d(self.aux_num_features, self.aux_num_features * 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(self.aux_num_features * 4)\n",
    "        self.act5 = Swish()\n",
    "        self.global_pool = SelectAdaptivePool2d(pool_type=pool_type)\n",
    "        self.num_features = backbone.num_features * self.global_pool.feat_mult()\n",
    "        assert cls_head == 'linear'\n",
    "        if cls_head == \"linear\":\n",
    "            ### Baseline head ###\n",
    "            #self.grapheme_fc = nn.Linear(self.num_features, num_grapheme_classes)\n",
    "            #self.consonant_fc = nn.Linear(self.num_features, num_consonant_classes)\n",
    "            #self.vowel_fc = nn.Linear(self.num_features, num_vowel_classes)\n",
    "            #self.word_fc = nn.Linear(self.num_features, num_word_classes)\n",
    "            self.fc = nn.Linear(self.num_features, num_total_classes)\n",
    "            \n",
    "            #self.grapheme_fc_1 = nn.Linear(self.aux_num_features * 4, num_grapheme_classes)\n",
    "            #self.consonant_fc_1 = nn.Linear(self.aux_num_features * 4, num_consonant_classes)\n",
    "            #self.vowel_fc_1 = nn.Linear(self.aux_num_features * 4, num_vowel_classes)\n",
    "            #self.word_fc_1 = nn.Linear(self.aux_num_features * 4, num_word_classes)\n",
    "            \n",
    "            self.aux_fc1 = nn.Linear(self.aux_num_features*4, num_total_classes)\n",
    "            \n",
    "            #self.grapheme_fc_2 = nn.Linear(self.aux_num_features * 4, num_grapheme_classes)\n",
    "            #self.consonant_fc_2 = nn.Linear(self.aux_num_features * 4, num_consonant_classes)\n",
    "            #self.vowel_fc_2 = nn.Linear(self.aux_num_features * 4, num_vowel_classes)\n",
    "            #self.word_fc_2 = nn.Linear(self.aux_num_features * 4, num_word_classes)\n",
    "            \n",
    "            self.aux_fc2 = nn.Linear(self.aux_num_features*4, num_total_classes)\n",
    "            \n",
    "            for fc in [self.fc, self.aux_fc1, self.aux_fc2]:\n",
    "                nn.init.zeros_(fc.bias.data)\n",
    "        elif cls_head == \"norm_softmax\":\n",
    "            ### NormSoftmax ###\n",
    "            self.grapheme_fc = NormSoftmax(self.num_features, num_grapheme_classes)\n",
    "            self.consonant_fc = NormSoftmax(self.num_features, num_consonant_classes)\n",
    "            self.vowel_fc = NormSoftmax(self.num_features, num_vowel_classes)\n",
    "        # Replace with Mish activation\n",
    "        if cfg.MODEL_ACTIVATION == \"mish\":\n",
    "            convert_swish_to_mish(self)\n",
    "        del backbone\n",
    "\n",
    "    def _features(self, x):\n",
    "        x = self.conv_stem(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.block0(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x); b4 = x\n",
    "        x = self.block5(x); b4 = self.aux_block5(b4); b5 = x\n",
    "        x = self.block6(x)\n",
    "        x = self.conv_head(x); b4 = self.aux_head4(b4); b5 = self.aux_head5(b5)\n",
    "        x = self.bn2(x); b4 = self.bn4(b4); b5 = self.bn5(b5)\n",
    "        x = self.act2(x); b4 = self.act4(b4); b5 = self.act5(b5)\n",
    "        return b4, b5, x\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(x)):\n",
    "            transforms.functional.normalize(x[i], mean=MEAN, std=STD, inplace=True)\n",
    "\n",
    "        # _, _, x = self._features(x)\n",
    "        b4, b5, x = self._features(x)\n",
    "        x = self.global_pool(x); b4 = self.global_pool(b4); b5 = self.global_pool(b5)\n",
    "        x = torch.flatten(x, 1); b4 = torch.flatten(b4, 1); b5 = torch.flatten(b5, 1)\n",
    "        if self.drop_rate > 0.:\n",
    "            x = F.dropout(x, p=self.drop_rate, training=self.training)\n",
    "        #grapheme_logits = self.grapheme_fc(x)\n",
    "        #vowel_logits = self.vowel_fc(x)\n",
    "        #consonant_logits = self.consonant_fc(x)\n",
    "        #word_logits = self.word_fc(x)\n",
    "        logits = self.fc(x)\n",
    "        \n",
    "        #grapheme_logits_1 = self.grapheme_fc_1(b4)\n",
    "        #vowel_logits_1 = self.vowel_fc_1(b4)\n",
    "        #consonant_logits_1 = self.consonant_fc_1(b4)\n",
    "        #word_logits_1 = self.word_fc_1(b4)\n",
    "        aux_logits1 = self.aux_fc1(b4)\n",
    "        \n",
    "        #grapheme_logits_2 = self.grapheme_fc_2(b5)\n",
    "        #vowel_logits_2 = self.vowel_fc_2(b5)\n",
    "        #consonant_logits_2 = self.consonant_fc_2(b5)\n",
    "        #word_logits_2 = self.word_fc_2(b5)\n",
    "        aux_logits2 = self.aux_fc2(b5)\n",
    "        \n",
    "        #return grapheme_logits, vowel_logits, consonant_logits, word_logits, \\\n",
    "        #       grapheme_logits_1, vowel_logits_1, consonant_logits_1, word_logits_1, \\\n",
    "        #       grapheme_logits_2, vowel_logits_2, consonant_logits_2, word_logits_2\n",
    "        return logits, aux_logits1, aux_logits2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './model4-ckps'\n",
    "def create_model(cfg):\n",
    "    model = BengaliNet(cfg)\n",
    "    model_file = os.path.join(MODEL_DIR, cfg.MODEL_NAME, cfg.CKP_NAME)\n",
    "\n",
    "    parent_dir = os.path.dirname(model_file)\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "\n",
    "    print('model file: {}, exist: {}'.format(model_file, os.path.exists(model_file)))\n",
    "\n",
    "    if os.path.exists(model_file):\n",
    "        print('loading {}...'.format(model_file))\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "    \n",
    "    return model, model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bnet = BengaliNet('se_resnext50_32x4d').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bnet(torch.randn((2, 1, 137, 236)).cuda()).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.111111"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(1/9, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "\n",
    "\n",
    "def calc_metrics(preds0, preds1, preds2, preds3, y):\n",
    "    assert len(y) == len(preds0) == len(preds1) == len(preds2) == len(preds3)\n",
    "\n",
    "    recall_grapheme = sklearn.metrics.recall_score(preds0, y[:, 0], average='macro')\n",
    "    recall_vowel = sklearn.metrics.recall_score(preds1, y[:, 1], average='macro')\n",
    "    recall_consonant = sklearn.metrics.recall_score(preds2, y[:, 2], average='macro')\n",
    "    recall_word = sklearn.metrics.recall_score(preds3, y[:, 3], average='macro')\n",
    "    \n",
    "    scores = [recall_grapheme, recall_vowel, recall_consonant]\n",
    "    final_recall_score = np.average(scores, weights=[2, 1, 1])\n",
    "    \n",
    "    metrics = {}\n",
    "    metrics['recall'] = round(final_recall_score, 6)\n",
    "    metrics['recall_grapheme'] = round(recall_grapheme, 6)\n",
    "    metrics['recall_vowel'] = round(recall_vowel, 6)\n",
    "    metrics['recall_consonant'] = round(recall_consonant, 6)\n",
    "    metrics['recall_word'] = round(recall_word, 6)\n",
    "    \n",
    "    metrics['acc_grapheme'] = round((preds0 == y[:, 0]).sum() / len(y), 6)\n",
    "    metrics['acc_vowel'] = round((preds1 == y[:, 1]).sum() / len(y), 6)\n",
    "    metrics['acc_consonant'] = round((preds2 == y[:, 2]).sum() / len(y), 6)\n",
    "    metrics['acc_word'] = round((preds3 == y[:, 3]).sum() / len(y), 6)    \n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(outputs, y_true):\n",
    "    # outputs: (N, 182)\n",
    "    # y_true: (N, 3)\n",
    "    \n",
    "    outputs = torch.split(outputs, [168, 11, 7, 1295], dim=1)\n",
    "    loss0 = F.cross_entropy(outputs[0], y_true[:, 0], reduction='mean')\n",
    "    loss1 = F.cross_entropy(outputs[1], y_true[:, 1], reduction='mean')\n",
    "    loss2 = F.cross_entropy(outputs[2], y_true[:, 2], reduction='mean')\n",
    "    loss3 = F.cross_entropy(outputs[3], y_true[:, 3], reduction='mean')\n",
    "    \n",
    "    return loss0 + loss1 + loss2 + loss3 #, loss0.item(), loss1.item(), loss2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    loss0, loss1, loss2, loss3 = 0., 0., 0., 0.\n",
    "    preds0, preds1, preds2, preds3 = [], [], [], []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            y_true.append(y)\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            outputs, _, _ = model(x)\n",
    "            outputs = torch.split(outputs, [168, 11, 7, 1295], dim=1)\n",
    "            \n",
    "            preds0.append(torch.max(outputs[0], dim=1)[1])\n",
    "            preds1.append(torch.max(outputs[1], dim=1)[1])\n",
    "            preds2.append(torch.max(outputs[2], dim=1)[1])\n",
    "            preds3.append(torch.max(outputs[3], dim=1)[1])\n",
    "            loss0 += F.cross_entropy(outputs[0], y[:, 0], reduction='sum').item()\n",
    "            loss1 += F.cross_entropy(outputs[1], y[:, 1], reduction='sum').item()\n",
    "            loss2 += F.cross_entropy(outputs[2], y[:, 2], reduction='sum').item()\n",
    "            loss3 += F.cross_entropy(outputs[3], y[:, 3], reduction='sum').item()\n",
    "            \n",
    "            # for debug\n",
    "            #metrics = {}\n",
    "            #metrics['loss_grapheme'] =  F.cross_entropy(outputs[0], y[:, 0], reduction='mean').item()\n",
    "            #metrics['loss_vowel'] =  F.cross_entropy(outputs[1], y[:, 1], reduction='mean').item()\n",
    "            #metrics['loss_consonant'] =  F.cross_entropy(outputs[2], y[:, 2], reduction='mean').item()\n",
    "            #return metrics\n",
    "    \n",
    "    preds0 = torch.cat(preds0, 0).cpu().numpy()\n",
    "    preds1 = torch.cat(preds1, 0).cpu().numpy()\n",
    "    preds2 = torch.cat(preds2, 0).cpu().numpy()\n",
    "    preds3 = torch.cat(preds3, 0).cpu().numpy()\n",
    "    \n",
    "    y_true = torch.cat(y_true, 0).numpy()\n",
    "    \n",
    "    #print('y_true:', y_true.shape)\n",
    "    #print('preds0:', preds0.shape)\n",
    "    \n",
    "    metrics = calc_metrics(preds0, preds1, preds2, preds3, y_true)\n",
    "    metrics['loss_grapheme'] = round(loss0 / val_loader.num, 6)\n",
    "    metrics['loss_vowel'] = round(loss1 / val_loader.num, 6)\n",
    "    metrics['loss_consonant'] = round(loss2 / val_loader.num, 6)\n",
    "    metrics['loss_word'] = round(loss3 / val_loader.num, 6)\n",
    "    \n",
    "    return metrics\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lrs(optimizer):\n",
    "    lrs = []\n",
    "    for pgs in optimizer.state_dict()['param_groups']:\n",
    "        lrs.append(pgs['lr'])\n",
    "    lrs = ['{:.6f}'.format(x) for x in lrs]\n",
    "    return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_file):\n",
    "    parent_dir = os.path.dirname(model_file)\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        torch.save(model.module.state_dict(), model_file)\n",
    "    else:\n",
    "        torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(data, targets, alpha=1):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets = targets[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    data = data * lam + shuffled_data * (1 - lam)\n",
    "    targets = (targets, shuffled_targets, lam)\n",
    "\n",
    "    return data, targets\n",
    "\n",
    "\n",
    "def mixup_criterion(outputs, targets):\n",
    "    targets1, targets2, lam = targets\n",
    "    #criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    return lam * criterion(outputs, targets1) + (1 - lam) * criterion(outputs, targets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8841070185469359"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from over9000.over9000 import Over9000\n",
    "from over9000.radam import RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvcore.solver import WarmupCyclicalLR\n",
    "def make_optimizer(model, base_lr=4e-4, weight_decay=0., weight_decay_bias=0., epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    Create optimizer with per-layer learning rate and weight decay.\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    for key, value in model.named_parameters():\n",
    "        if not value.requires_grad:\n",
    "            continue\n",
    "        lr = base_lr\n",
    "        params += [{\"params\": [value], \"lr\": lr, \"weight_decay\": weight_decay_bias if 'bias' in key else weight_decay}]\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(params, lr, eps=epsilon)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metrics = 0.\n",
    "\n",
    "def train(args, model):\n",
    "    optimizer = make_optimizer(model)\n",
    "    scheduler = WarmupCyclicalLR(\n",
    "        \"cos\", args.base_lr, args.num_epochs, iters_per_epoch=len(train_loader), warmup_epochs=args.warmup_epochs)\n",
    "    \n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    for cycle in range(1, args.num_cycles+1):\n",
    "        print('CYCLE:', cycle)\n",
    "        train_cycle(args, model, optimizer, scheduler)\n",
    "        args.base_lr = 4e-4\n",
    "        args.num_epochs = 100\n",
    "        args.warmup_epochs = 10\n",
    "        scheduler = WarmupCyclicalLR(\n",
    "            \"cos\", args.base_lr, args.num_epochs, iters_per_epoch=len(train_loader), warmup_epochs=args.warmup_epochs)\n",
    "\n",
    "def train_cycle(args, model, optimizer, lr_scheduler):\n",
    "    global best_metrics\n",
    "    best_key = 'recall'\n",
    "    \n",
    "    val_metrics = validate(model, val_loader)\n",
    "    print(val_metrics)\n",
    "\n",
    "    if val_metrics[best_key] > best_metrics:\n",
    "        best_metrics = val_metrics[best_key]\n",
    "    \n",
    "    model.train()\n",
    "    train_iter = 0\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        train_loss = 0\n",
    "\n",
    "        bg = time.time()\n",
    "        for batch_idx, (img, targets) in enumerate(train_loader):\n",
    "            train_iter += 1\n",
    "            img, targets  = img.cuda(), targets.cuda()\n",
    "            batch_size = img.size(0)\n",
    "            r = np.random.rand()\n",
    "\n",
    "            if True:\n",
    "                # generate mixed sample\n",
    "                lam = np.random.beta(args.beta, args.beta)\n",
    "                rand_index = torch.randperm(img.size()[0]).cuda()\n",
    "                target_a = targets\n",
    "                target_b = targets[rand_index]\n",
    "                bbx1, bby1, bbx2, bby2 = rand_bbox(img.size(), lam)\n",
    "                img[:, :, bbx1:bbx2, bby1:bby2] = img[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "                # adjust lambda to exactly match pixel ratio\n",
    "                lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (img.size()[-1] * img.size()[-2]))\n",
    "                # compute output\n",
    "                outputs, outputs_aux1, outputs_aux2 = model(img)\n",
    "                loss_primary = criterion(outputs, target_a) * lam + criterion(outputs, target_b) * (1. - lam)\n",
    "                loss_aux1 = criterion(outputs_aux1, target_a) * lam + criterion(outputs_aux1, target_b) * (1. - lam)\n",
    "                loss_aux2 = criterion(outputs_aux2, target_a) * lam + criterion(outputs_aux2, target_b) * (1. - lam)\n",
    "                loss = loss_primary + loss_aux1 + loss_aux2\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "            #loss.backward()\n",
    "            lr_scheduler(optimizer, batch_idx, epoch)\n",
    "            optimizer.step()            \n",
    "            \n",
    "            current_lr = get_lrs(optimizer)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            print('\\r {:4d} | {:.6f} | {:06d}/{} | {:.4f} | {:.4f} |'.format(\n",
    "                epoch, float(current_lr[0]), batch_size*(batch_idx+1), train_loader.num, \n",
    "                loss.item(), train_loss/(batch_idx+1)), end='')\n",
    "\n",
    "        if True:#train_iter > 0 and train_iter % args.iter_val == 0:\n",
    "            val_metrics = validate(model, val_loader)\n",
    "            print('\\nval:', val_metrics)\n",
    "                \n",
    "            if val_metrics[best_key] > best_metrics:\n",
    "                best_metrics = val_metrics[best_key]\n",
    "                save_model(model, model_file)\n",
    "                print('###>>>>> saved')\n",
    "                \n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Namespace()\n",
    "cfg.MODEL_NAME = 'tf_efficientnet_b4'\n",
    "cfg.PRETRAINED = True\n",
    "cfg.IN_CHANNELS = 1\n",
    "cfg.POOL_TYPE = 'avg'\n",
    "cfg.CLS_HEAD = 'linear'\n",
    "cfg.MODEL_ACTIVATION = 'swish'\n",
    "cfg.DROP_CONNECT = 0.2\n",
    "cfg.DROPOUT= 0.\n",
    "cfg.NUM_WORD_CLASSES = 1295\n",
    "cfg.NUM_GRAPHEME_CLASSES = 168\n",
    "cfg.NUM_VOWEL_CLASSES = 11\n",
    "cfg.NUM_CONSONANT_CLASSES = 7\n",
    "cfg.CKP_NAME = 'model4_eb4_fold1.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model, model_file = create_model(cfg)\n",
    "#model(torch.randn(2,1,137,236))[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "\n",
    "args.base_lr = 4e-4\n",
    "args.num_epochs = 100\n",
    "args.warmup_epochs = 5\n",
    "args.num_cycles = 100\n",
    "args.batch_size = 640\n",
    "args.val_batch_size = 1024\n",
    "args.st_epochs = 10\n",
    "\n",
    "args.beta = 1.0\n",
    "args.cutmix_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200840, 6)\n",
      "(200840, 32332)\n",
      "(160635, 6) (40205, 6)\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_train_val_loaders(batch_size=args.batch_size, val_batch_size=args.val_batch_size, ifold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model file: ./model4-ckps/tf_efficientnet_b4/model4_eb4_fold1.pth, exist: True\n",
      "loading ./model4-ckps/tf_efficientnet_b4/model4_eb4_fold1.pth...\n"
     ]
    }
   ],
   "source": [
    "model, model_file = create_model(cfg)\n",
    "#if torch.cuda.device_count() > 1:\n",
    "#    model = nn.DataParallel(model)\n",
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CYCLE: 1\n",
      "{'recall': 0.992515, 'recall_grapheme': 0.990199, 'recall_vowel': 0.995289, 'recall_consonant': 0.994371, 'recall_word': 0.989434, 'acc_grapheme': 0.989379, 'acc_vowel': 0.995249, 'acc_consonant': 0.996219, 'acc_word': 0.989504, 'loss_grapheme': 0.180598, 'loss_vowel': 0.129995, 'loss_consonant': 0.083757, 'loss_word': 0.13174}\n",
      "    0 | 0.000080 | 160000/160635 | 19.4096 | 15.3317 |\n",
      "val: {'recall': 0.991266, 'recall_grapheme': 0.987342, 'recall_vowel': 0.9943, 'recall_consonant': 0.99608, 'recall_word': 0.989049, 'acc_grapheme': 0.986693, 'acc_vowel': 0.99423, 'acc_consonant': 0.994976, 'acc_word': 0.989056, 'loss_grapheme': 0.318078, 'loss_vowel': 0.260641, 'loss_consonant': 0.165991, 'loss_word': 0.172336}\n",
      "    1 | 0.000160 | 160000/160635 | 20.6334 | 14.9313 |\n",
      "val: {'recall': 0.992183, 'recall_grapheme': 0.988759, 'recall_vowel': 0.994917, 'recall_consonant': 0.996296, 'recall_word': 0.989278, 'acc_grapheme': 0.987862, 'acc_vowel': 0.994777, 'acc_consonant': 0.995821, 'acc_word': 0.98933, 'loss_grapheme': 0.27214, 'loss_vowel': 0.212054, 'loss_consonant': 0.135736, 'loss_word': 0.170706}\n",
      "    2 | 0.000239 | 160000/160635 | 14.6376 | 16.0644 |\n",
      "val: {'recall': 0.99169, 'recall_grapheme': 0.988753, 'recall_vowel': 0.995096, 'recall_consonant': 0.994159, 'recall_word': 0.98929, 'acc_grapheme': 0.988061, 'acc_vowel': 0.99515, 'acc_consonant': 0.995846, 'acc_word': 0.98928, 'loss_grapheme': 0.245744, 'loss_vowel': 0.182551, 'loss_consonant': 0.119266, 'loss_word': 0.15662}\n",
      "    3 | 0.000318 | 160000/160635 | 11.8358 | 14.9480 |\n",
      "val: {'recall': 0.991285, 'recall_grapheme': 0.98765, 'recall_vowel': 0.994497, 'recall_consonant': 0.995344, 'recall_word': 0.989085, 'acc_grapheme': 0.987116, 'acc_vowel': 0.994578, 'acc_consonant': 0.9951, 'acc_word': 0.989131, 'loss_grapheme': 0.289973, 'loss_vowel': 0.240823, 'loss_consonant': 0.160085, 'loss_word': 0.161942}\n",
      "    4 | 0.000347 | 058240/160635 | 20.4257 | 15.7570 |"
     ]
    }
   ],
   "source": [
    "train(args, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CYCLE: 1\n",
      "{'recall': 0.984603, 'recall_grapheme': 0.97713, 'recall_vowel': 0.991312, 'recall_consonant': 0.992841, 'recall_word': 0.97772, 'acc_grapheme': 0.975376, 'acc_vowel': 0.990822, 'acc_consonant': 0.990349, 'acc_word': 0.977764, 'loss_grapheme': 0.377051, 'loss_vowel': 0.251561, 'loss_consonant': 0.166439, 'loss_word': 0.327534}\n",
      "    0 | 0.000080 | 160000/160635 | 25.9052 | 18.1899 |\n",
      "val: {'recall': 0.984696, 'recall_grapheme': 0.977376, 'recall_vowel': 0.991461, 'recall_consonant': 0.992571, 'recall_word': 0.978015, 'acc_grapheme': 0.9756, 'acc_vowel': 0.991195, 'acc_consonant': 0.991245, 'acc_word': 0.978038, 'loss_grapheme': 0.340236, 'loss_vowel': 0.240668, 'loss_consonant': 0.161181, 'loss_word': 0.29083}\n",
      "###>>>>> saved\n",
      "    1 | 0.000160 | 160000/160635 | 2.3221 | 18.9110 ||\n",
      "val: {'recall': 0.984781, 'recall_grapheme': 0.978174, 'recall_vowel': 0.991073, 'recall_consonant': 0.991703, 'recall_word': 0.97805, 'acc_grapheme': 0.976694, 'acc_vowel': 0.991319, 'acc_consonant': 0.99117, 'acc_word': 0.978162, 'loss_grapheme': 0.293794, 'loss_vowel': 0.198103, 'loss_consonant': 0.139734, 'loss_word': 0.244299}\n",
      "###>>>>> saved\n",
      "    2 | 0.000239 | 160000/160635 | 14.3098 | 18.1875 |\n",
      "val: {'recall': 0.984629, 'recall_grapheme': 0.977931, 'recall_vowel': 0.99116, 'recall_consonant': 0.991494, 'recall_word': 0.977285, 'acc_grapheme': 0.976023, 'acc_vowel': 0.991195, 'acc_consonant': 0.991145, 'acc_word': 0.977316, 'loss_grapheme': 0.33478, 'loss_vowel': 0.241768, 'loss_consonant': 0.16355, 'loss_word': 0.277588}\n",
      "    3 | 0.000318 | 160000/160635 | 19.2009 | 18.2638 |\n",
      "val: {'recall': 0.984571, 'recall_grapheme': 0.977557, 'recall_vowel': 0.990713, 'recall_consonant': 0.992457, 'recall_word': 0.977443, 'acc_grapheme': 0.976048, 'acc_vowel': 0.990673, 'acc_consonant': 0.991096, 'acc_word': 0.97749, 'loss_grapheme': 0.333418, 'loss_vowel': 0.23937, 'loss_consonant': 0.163375, 'loss_word': 0.279844}\n",
      "    4 | 0.000397 | 160000/160635 | 19.7396 | 17.9612 |\n",
      "val: {'recall': 0.983972, 'recall_grapheme': 0.976882, 'recall_vowel': 0.99079, 'recall_consonant': 0.991335, 'recall_word': 0.977321, 'acc_grapheme': 0.974456, 'acc_vowel': 0.990847, 'acc_consonant': 0.99122, 'acc_word': 0.977416, 'loss_grapheme': 0.376483, 'loss_vowel': 0.277006, 'loss_consonant': 0.177096, 'loss_word': 0.319178}\n",
      "    5 | 0.000396 | 160000/160635 | 25.1112 | 18.2098 |\n",
      "val: {'recall': 0.984144, 'recall_grapheme': 0.97701, 'recall_vowel': 0.991161, 'recall_consonant': 0.991393, 'recall_word': 0.977308, 'acc_grapheme': 0.975177, 'acc_vowel': 0.990971, 'acc_consonant': 0.991245, 'acc_word': 0.977416, 'loss_grapheme': 0.360193, 'loss_vowel': 0.283771, 'loss_consonant': 0.177305, 'loss_word': 0.326814}\n",
      "    6 | 0.000395 | 160000/160635 | 22.4892 | 18.4964 |\n",
      "val: {'recall': 0.985197, 'recall_grapheme': 0.978447, 'recall_vowel': 0.991417, 'recall_consonant': 0.992477, 'recall_word': 0.978321, 'acc_grapheme': 0.977416, 'acc_vowel': 0.991469, 'acc_consonant': 0.991991, 'acc_word': 0.97851, 'loss_grapheme': 0.298504, 'loss_vowel': 0.201846, 'loss_consonant': 0.141152, 'loss_word': 0.243685}\n",
      "###>>>>> saved\n",
      "    7 | 0.000394 | 160000/160635 | 22.6900 | 18.1613 |\n",
      "val: {'recall': 0.984958, 'recall_grapheme': 0.978691, 'recall_vowel': 0.991695, 'recall_consonant': 0.990753, 'recall_word': 0.978879, 'acc_grapheme': 0.977167, 'acc_vowel': 0.991394, 'acc_consonant': 0.991444, 'acc_word': 0.978958, 'loss_grapheme': 0.315668, 'loss_vowel': 0.217791, 'loss_consonant': 0.149168, 'loss_word': 0.268986}\n",
      "    8 | 0.000392 | 160000/160635 | 18.0319 | 17.6471 |\n",
      "val: {'recall': 0.985911, 'recall_grapheme': 0.980308, 'recall_vowel': 0.991334, 'recall_consonant': 0.991695, 'recall_word': 0.979779, 'acc_grapheme': 0.977938, 'acc_vowel': 0.991693, 'acc_consonant': 0.991916, 'acc_word': 0.979804, 'loss_grapheme': 0.332507, 'loss_vowel': 0.224873, 'loss_consonant': 0.154374, 'loss_word': 0.277954}\n",
      "###>>>>> saved\n",
      "    9 | 0.000390 | 160000/160635 | 7.3551 | 17.6395 ||\n",
      "val: {'recall': 0.98574, 'recall_grapheme': 0.980065, 'recall_vowel': 0.991982, 'recall_consonant': 0.990848, 'recall_word': 0.979325, 'acc_grapheme': 0.978684, 'acc_vowel': 0.991892, 'acc_consonant': 0.992016, 'acc_word': 0.979306, 'loss_grapheme': 0.251469, 'loss_vowel': 0.158123, 'loss_consonant': 0.112822, 'loss_word': 0.201565}\n",
      "   11 | 0.000386 | 160000/160635 | 22.4904 | 17.7227 |\n",
      "val: {'recall': 0.98606, 'recall_grapheme': 0.980194, 'recall_vowel': 0.991895, 'recall_consonant': 0.991959, 'recall_word': 0.980569, 'acc_grapheme': 0.978261, 'acc_vowel': 0.991344, 'acc_consonant': 0.991916, 'acc_word': 0.980351, 'loss_grapheme': 0.363433, 'loss_vowel': 0.268398, 'loss_consonant': 0.169744, 'loss_word': 0.3045}\n",
      "###>>>>> saved\n",
      "   12 | 0.000384 | 160000/160635 | 17.1256 | 17.5252 |\n",
      "val: {'recall': 0.986659, 'recall_grapheme': 0.980851, 'recall_vowel': 0.992543, 'recall_consonant': 0.992391, 'recall_word': 0.981032, 'acc_grapheme': 0.979629, 'acc_vowel': 0.99219, 'acc_consonant': 0.992513, 'acc_word': 0.980948, 'loss_grapheme': 0.323615, 'loss_vowel': 0.230281, 'loss_consonant': 0.14762, 'loss_word': 0.250576}\n",
      "###>>>>> saved\n",
      "   13 | 0.000381 | 160000/160635 | 12.9224 | 17.2048 |\n",
      "val: {'recall': 0.985883, 'recall_grapheme': 0.97967, 'recall_vowel': 0.991736, 'recall_consonant': 0.992456, 'recall_word': 0.980127, 'acc_grapheme': 0.97759, 'acc_vowel': 0.990996, 'acc_consonant': 0.991419, 'acc_word': 0.980077, 'loss_grapheme': 0.384961, 'loss_vowel': 0.283676, 'loss_consonant': 0.183022, 'loss_word': 0.306478}\n",
      "   14 | 0.000378 | 160000/160635 | 12.6175 | 17.8544 |\n",
      "val: {'recall': 0.986564, 'recall_grapheme': 0.9807, 'recall_vowel': 0.992481, 'recall_consonant': 0.992375, 'recall_word': 0.980794, 'acc_grapheme': 0.979157, 'acc_vowel': 0.99224, 'acc_consonant': 0.99224, 'acc_word': 0.980898, 'loss_grapheme': 0.315242, 'loss_vowel': 0.210297, 'loss_consonant': 0.144708, 'loss_word': 0.240964}\n",
      "   15 | 0.000375 | 160000/160635 | 22.7105 | 17.4718 |\n",
      "val: {'recall': 0.987583, 'recall_grapheme': 0.981534, 'recall_vowel': 0.992481, 'recall_consonant': 0.994785, 'recall_word': 0.981849, 'acc_grapheme': 0.979406, 'acc_vowel': 0.991991, 'acc_consonant': 0.992041, 'acc_word': 0.981943, 'loss_grapheme': 0.373212, 'loss_vowel': 0.276412, 'loss_consonant': 0.17435, 'loss_word': 0.29317}\n",
      "###>>>>> saved\n",
      "   16 | 0.000372 | 160000/160635 | 10.8118 | 17.3184 |\n",
      "val: {'recall': 0.987173, 'recall_grapheme': 0.982443, 'recall_vowel': 0.993072, 'recall_consonant': 0.990735, 'recall_word': 0.981047, 'acc_grapheme': 0.980798, 'acc_vowel': 0.993011, 'acc_consonant': 0.992787, 'acc_word': 0.981147, 'loss_grapheme': 0.199111, 'loss_vowel': 0.125772, 'loss_consonant': 0.093307, 'loss_word': 0.162389}\n",
      "   17 | 0.000369 | 160000/160635 | 23.9006 | 16.8475 |\n",
      "val: {'recall': 0.986276, 'recall_grapheme': 0.980408, 'recall_vowel': 0.991932, 'recall_consonant': 0.992358, 'recall_word': 0.979612, 'acc_grapheme': 0.978261, 'acc_vowel': 0.991916, 'acc_consonant': 0.992687, 'acc_word': 0.979828, 'loss_grapheme': 0.3389, 'loss_vowel': 0.244647, 'loss_consonant': 0.159519, 'loss_word': 0.266583}\n",
      "   18 | 0.000365 | 160000/160635 | 20.5498 | 17.3063 |\n",
      "val: {'recall': 0.987655, 'recall_grapheme': 0.9823, 'recall_vowel': 0.992976, 'recall_consonant': 0.993044, 'recall_word': 0.981673, 'acc_grapheme': 0.980276, 'acc_vowel': 0.992812, 'acc_consonant': 0.992862, 'acc_word': 0.981694, 'loss_grapheme': 0.337896, 'loss_vowel': 0.24476, 'loss_consonant': 0.169639, 'loss_word': 0.257741}\n",
      "###>>>>> saved\n",
      "   19 | 0.000362 | 160000/160635 | 2.4900 | 17.0661 ||\n",
      "val: {'recall': 0.987694, 'recall_grapheme': 0.982341, 'recall_vowel': 0.992259, 'recall_consonant': 0.993835, 'recall_word': 0.981039, 'acc_grapheme': 0.980798, 'acc_vowel': 0.99229, 'acc_consonant': 0.992538, 'acc_word': 0.981221, 'loss_grapheme': 0.28009, 'loss_vowel': 0.204381, 'loss_consonant': 0.136422, 'loss_word': 0.213273}\n",
      "###>>>>> saved\n",
      "   20 | 0.000358 | 160000/160635 | 24.8392 | 16.8766 |\n",
      "val: {'recall': 0.988367, 'recall_grapheme': 0.983538, 'recall_vowel': 0.992502, 'recall_consonant': 0.99389, 'recall_word': 0.981545, 'acc_grapheme': 0.9804, 'acc_vowel': 0.992762, 'acc_consonant': 0.992936, 'acc_word': 0.981644, 'loss_grapheme': 0.282382, 'loss_vowel': 0.189099, 'loss_consonant': 0.129681, 'loss_word': 0.211628}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###>>>>> saved\n",
      "   21 | 0.000354 | 160000/160635 | 15.5916 | 16.6565 |\n",
      "val: {'recall': 0.98801, 'recall_grapheme': 0.983662, 'recall_vowel': 0.992418, 'recall_consonant': 0.992298, 'recall_word': 0.982332, 'acc_grapheme': 0.981445, 'acc_vowel': 0.992812, 'acc_consonant': 0.992886, 'acc_word': 0.982365, 'loss_grapheme': 0.324178, 'loss_vowel': 0.234505, 'loss_consonant': 0.151151, 'loss_word': 0.243136}\n",
      "   22 | 0.000350 | 160000/160635 | 23.2937 | 17.4513 |\n",
      "val: {'recall': 0.988243, 'recall_grapheme': 0.98287, 'recall_vowel': 0.993232, 'recall_consonant': 0.993999, 'recall_word': 0.982236, 'acc_grapheme': 0.981022, 'acc_vowel': 0.993011, 'acc_consonant': 0.993235, 'acc_word': 0.98239, 'loss_grapheme': 0.34441, 'loss_vowel': 0.244334, 'loss_consonant': 0.157811, 'loss_word': 0.282093}\n",
      "   23 | 0.000346 | 160000/160635 | 16.0969 | 17.3355 |\n",
      "val: {'recall': 0.987697, 'recall_grapheme': 0.983472, 'recall_vowel': 0.991819, 'recall_consonant': 0.992025, 'recall_word': 0.981513, 'acc_grapheme': 0.981619, 'acc_vowel': 0.992862, 'acc_consonant': 0.992911, 'acc_word': 0.981619, 'loss_grapheme': 0.236622, 'loss_vowel': 0.16262, 'loss_consonant': 0.107342, 'loss_word': 0.180174}\n",
      "   24 | 0.000341 | 160000/160635 | 21.3979 | 17.1300 |\n",
      "val: {'recall': 0.987971, 'recall_grapheme': 0.983033, 'recall_vowel': 0.991145, 'recall_consonant': 0.994674, 'recall_word': 0.982577, 'acc_grapheme': 0.981495, 'acc_vowel': 0.99224, 'acc_consonant': 0.992488, 'acc_word': 0.982639, 'loss_grapheme': 0.350988, 'loss_vowel': 0.253262, 'loss_consonant': 0.168491, 'loss_word': 0.243213}\n",
      "   25 | 0.000337 | 160000/160635 | 20.9972 | 17.1177 |\n",
      "val: {'recall': 0.988682, 'recall_grapheme': 0.983704, 'recall_vowel': 0.993359, 'recall_consonant': 0.993961, 'recall_word': 0.982753, 'acc_grapheme': 0.981719, 'acc_vowel': 0.992812, 'acc_consonant': 0.993359, 'acc_word': 0.982838, 'loss_grapheme': 0.275113, 'loss_vowel': 0.199015, 'loss_consonant': 0.129291, 'loss_word': 0.200743}\n",
      "###>>>>> saved\n",
      "   26 | 0.000332 | 160000/160635 | 23.1804 | 17.8410 |\n",
      "val: {'recall': 0.987352, 'recall_grapheme': 0.982601, 'recall_vowel': 0.991852, 'recall_consonant': 0.992353, 'recall_word': 0.982703, 'acc_grapheme': 0.980724, 'acc_vowel': 0.991643, 'acc_consonant': 0.992538, 'acc_word': 0.982689, 'loss_grapheme': 0.376371, 'loss_vowel': 0.276725, 'loss_consonant': 0.177767, 'loss_word': 0.26244}\n",
      "   27 | 0.000328 | 160000/160635 | 14.4470 | 17.1053 |\n",
      "val: {'recall': 0.987737, 'recall_grapheme': 0.983324, 'recall_vowel': 0.992391, 'recall_consonant': 0.99191, 'recall_word': 0.982666, 'acc_grapheme': 0.981992, 'acc_vowel': 0.992389, 'acc_consonant': 0.99316, 'acc_word': 0.982738, 'loss_grapheme': 0.337984, 'loss_vowel': 0.243071, 'loss_consonant': 0.155784, 'loss_word': 0.241117}\n",
      "   28 | 0.000323 | 160000/160635 | 15.3128 | 16.8768 |\n",
      "val: {'recall': 0.987468, 'recall_grapheme': 0.981863, 'recall_vowel': 0.992131, 'recall_consonant': 0.994016, 'recall_word': 0.982283, 'acc_grapheme': 0.980823, 'acc_vowel': 0.992464, 'acc_consonant': 0.992687, 'acc_word': 0.982365, 'loss_grapheme': 0.353332, 'loss_vowel': 0.259585, 'loss_consonant': 0.166046, 'loss_word': 0.248285}\n",
      "   29 | 0.000318 | 160000/160635 | 18.8869 | 17.0821 |\n",
      "val: {'recall': 0.988192, 'recall_grapheme': 0.98301, 'recall_vowel': 0.992732, 'recall_consonant': 0.994017, 'recall_word': 0.983027, 'acc_grapheme': 0.981171, 'acc_vowel': 0.992439, 'acc_consonant': 0.992663, 'acc_word': 0.983037, 'loss_grapheme': 0.366401, 'loss_vowel': 0.265626, 'loss_consonant': 0.172656, 'loss_word': 0.253463}\n",
      "   30 | 0.000312 | 160000/160635 | 19.8744 | 16.2851 |\n",
      "val: {'recall': 0.988903, 'recall_grapheme': 0.985533, 'recall_vowel': 0.993452, 'recall_consonant': 0.991094, 'recall_word': 0.984282, 'acc_grapheme': 0.983808, 'acc_vowel': 0.993707, 'acc_consonant': 0.993906, 'acc_word': 0.98438, 'loss_grapheme': 0.210391, 'loss_vowel': 0.142074, 'loss_consonant': 0.098753, 'loss_word': 0.147085}\n",
      "###>>>>> saved\n",
      "   31 | 0.000307 | 160000/160635 | 17.6695 | 16.7389 |\n",
      "val: {'recall': 0.988771, 'recall_grapheme': 0.98466, 'recall_vowel': 0.993055, 'recall_consonant': 0.992708, 'recall_word': 0.98416, 'acc_grapheme': 0.983733, 'acc_vowel': 0.993135, 'acc_consonant': 0.993906, 'acc_word': 0.984181, 'loss_grapheme': 0.231817, 'loss_vowel': 0.162561, 'loss_consonant': 0.104803, 'loss_word': 0.171621}\n",
      "   32 | 0.000302 | 160000/160635 | 15.1674 | 16.1244 |\n",
      "val: {'recall': 0.988153, 'recall_grapheme': 0.983805, 'recall_vowel': 0.993202, 'recall_consonant': 0.991799, 'recall_word': 0.983206, 'acc_grapheme': 0.982191, 'acc_vowel': 0.99311, 'acc_consonant': 0.993558, 'acc_word': 0.983261, 'loss_grapheme': 0.287124, 'loss_vowel': 0.214051, 'loss_consonant': 0.135744, 'loss_word': 0.221832}\n",
      "   33 | 0.000296 | 160000/160635 | 3.2225 | 16.6551 ||\n",
      "val: {'recall': 0.989356, 'recall_grapheme': 0.984897, 'recall_vowel': 0.993613, 'recall_consonant': 0.994017, 'recall_word': 0.984893, 'acc_grapheme': 0.984007, 'acc_vowel': 0.993459, 'acc_consonant': 0.994105, 'acc_word': 0.984902, 'loss_grapheme': 0.229957, 'loss_vowel': 0.160528, 'loss_consonant': 0.110729, 'loss_word': 0.160413}\n",
      "###>>>>> saved\n",
      "   34 | 0.000291 | 160000/160635 | 6.7783 | 16.5986 ||\n",
      "val: {'recall': 0.989073, 'recall_grapheme': 0.985332, 'recall_vowel': 0.99323, 'recall_consonant': 0.992399, 'recall_word': 0.984917, 'acc_grapheme': 0.983286, 'acc_vowel': 0.993284, 'acc_consonant': 0.993583, 'acc_word': 0.984927, 'loss_grapheme': 0.307985, 'loss_vowel': 0.228772, 'loss_consonant': 0.14896, 'loss_word': 0.207988}\n",
      "   35 | 0.000285 | 160000/160635 | 16.9726 | 16.7386 |\n",
      "val: {'recall': 0.989106, 'recall_grapheme': 0.98541, 'recall_vowel': 0.993667, 'recall_consonant': 0.991935, 'recall_word': 0.985188, 'acc_grapheme': 0.983932, 'acc_vowel': 0.993832, 'acc_consonant': 0.993658, 'acc_word': 0.985226, 'loss_grapheme': 0.298242, 'loss_vowel': 0.224771, 'loss_consonant': 0.151185, 'loss_word': 0.191105}\n",
      "   36 | 0.000279 | 160000/160635 | 20.4818 | 15.8777 |\n",
      "val: {'recall': 0.989696, 'recall_grapheme': 0.98552, 'recall_vowel': 0.993535, 'recall_consonant': 0.994209, 'recall_word': 0.985104, 'acc_grapheme': 0.984106, 'acc_vowel': 0.993583, 'acc_consonant': 0.994105, 'acc_word': 0.985076, 'loss_grapheme': 0.260517, 'loss_vowel': 0.183752, 'loss_consonant': 0.121131, 'loss_word': 0.181001}\n",
      "###>>>>> saved\n",
      "   37 | 0.000274 | 160000/160635 | 12.8417 | 16.3575 |\n",
      "val: {'recall': 0.989469, 'recall_grapheme': 0.985637, 'recall_vowel': 0.993301, 'recall_consonant': 0.993301, 'recall_word': 0.984352, 'acc_grapheme': 0.983659, 'acc_vowel': 0.993533, 'acc_consonant': 0.99418, 'acc_word': 0.984281, 'loss_grapheme': 0.247068, 'loss_vowel': 0.173272, 'loss_consonant': 0.119867, 'loss_word': 0.183487}\n",
      "   38 | 0.000268 | 160000/160635 | 8.3735 | 16.4500 ||\n",
      "val: {'recall': 0.99035, 'recall_grapheme': 0.986916, 'recall_vowel': 0.993812, 'recall_consonant': 0.993757, 'recall_word': 0.986069, 'acc_grapheme': 0.985698, 'acc_vowel': 0.994006, 'acc_consonant': 0.994702, 'acc_word': 0.986121, 'loss_grapheme': 0.175668, 'loss_vowel': 0.115907, 'loss_consonant': 0.081551, 'loss_word': 0.131107}\n",
      "###>>>>> saved\n",
      "   39 | 0.000262 | 160000/160635 | 18.9201 | 16.1510 |\n",
      "val: {'recall': 0.99007, 'recall_grapheme': 0.98546, 'recall_vowel': 0.994376, 'recall_consonant': 0.994983, 'recall_word': 0.985825, 'acc_grapheme': 0.98443, 'acc_vowel': 0.993881, 'acc_consonant': 0.994254, 'acc_word': 0.985823, 'loss_grapheme': 0.299035, 'loss_vowel': 0.223968, 'loss_consonant': 0.150113, 'loss_word': 0.192397}\n",
      "   40 | 0.000256 | 160000/160635 | 14.1863 | 15.9034 |\n",
      "val: {'recall': 0.990124, 'recall_grapheme': 0.985754, 'recall_vowel': 0.993971, 'recall_consonant': 0.995017, 'recall_word': 0.985857, 'acc_grapheme': 0.984082, 'acc_vowel': 0.993757, 'acc_consonant': 0.994503, 'acc_word': 0.985798, 'loss_grapheme': 0.27831, 'loss_vowel': 0.202457, 'loss_consonant': 0.134884, 'loss_word': 0.196146}\n",
      "   41 | 0.000250 | 160000/160635 | 21.7496 | 16.0394 |\n",
      "val: {'recall': 0.990527, 'recall_grapheme': 0.985961, 'recall_vowel': 0.994229, 'recall_consonant': 0.995955, 'recall_word': 0.986343, 'acc_grapheme': 0.984853, 'acc_vowel': 0.994105, 'acc_consonant': 0.994677, 'acc_word': 0.986395, 'loss_grapheme': 0.262223, 'loss_vowel': 0.201857, 'loss_consonant': 0.125975, 'loss_word': 0.179742}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###>>>>> saved\n",
      "   42 | 0.000244 | 160000/160635 | 21.5886 | 16.1747 |\n",
      "val: {'recall': 0.990179, 'recall_grapheme': 0.986513, 'recall_vowel': 0.993908, 'recall_consonant': 0.993783, 'recall_word': 0.986363, 'acc_grapheme': 0.984853, 'acc_vowel': 0.993583, 'acc_consonant': 0.994429, 'acc_word': 0.98637, 'loss_grapheme': 0.29636, 'loss_vowel': 0.225799, 'loss_consonant': 0.150296, 'loss_word': 0.187128}\n",
      "   43 | 0.000238 | 160000/160635 | 0.9276 | 15.5587 ||\n",
      "val: {'recall': 0.989957, 'recall_grapheme': 0.986283, 'recall_vowel': 0.993358, 'recall_consonant': 0.993906, 'recall_word': 0.985797, 'acc_grapheme': 0.985126, 'acc_vowel': 0.993856, 'acc_consonant': 0.994478, 'acc_word': 0.985897, 'loss_grapheme': 0.200493, 'loss_vowel': 0.146154, 'loss_consonant': 0.099368, 'loss_word': 0.133185}\n",
      "   44 | 0.000231 | 160000/160635 | 0.9186 | 15.4935 ||\n",
      "val: {'recall': 0.989522, 'recall_grapheme': 0.985582, 'recall_vowel': 0.993522, 'recall_consonant': 0.993404, 'recall_word': 0.986566, 'acc_grapheme': 0.985027, 'acc_vowel': 0.993856, 'acc_consonant': 0.994404, 'acc_word': 0.986569, 'loss_grapheme': 0.230975, 'loss_vowel': 0.165821, 'loss_consonant': 0.111825, 'loss_word': 0.14875}\n",
      "   45 | 0.000225 | 160000/160635 | 20.2045 | 15.9692 |\n",
      "val: {'recall': 0.991009, 'recall_grapheme': 0.987102, 'recall_vowel': 0.993937, 'recall_consonant': 0.995895, 'recall_word': 0.986755, 'acc_grapheme': 0.985325, 'acc_vowel': 0.993732, 'acc_consonant': 0.994528, 'acc_word': 0.986718, 'loss_grapheme': 0.326273, 'loss_vowel': 0.242942, 'loss_consonant': 0.159322, 'loss_word': 0.208526}\n",
      "###>>>>> saved\n",
      "   46 | 0.000219 | 160000/160635 | 19.1707 | 15.9692 |\n",
      "val: {'recall': 0.990872, 'recall_grapheme': 0.986588, 'recall_vowel': 0.994551, 'recall_consonant': 0.99576, 'recall_word': 0.987132, 'acc_grapheme': 0.985698, 'acc_vowel': 0.994031, 'acc_consonant': 0.994727, 'acc_word': 0.987216, 'loss_grapheme': 0.255246, 'loss_vowel': 0.190221, 'loss_consonant': 0.121737, 'loss_word': 0.168415}\n",
      "   47 | 0.000213 | 160000/160635 | 13.4044 | 16.2878 |\n",
      "val: {'recall': 0.99105, 'recall_grapheme': 0.987167, 'recall_vowel': 0.994005, 'recall_consonant': 0.995861, 'recall_word': 0.987187, 'acc_grapheme': 0.985425, 'acc_vowel': 0.994006, 'acc_consonant': 0.994677, 'acc_word': 0.987216, 'loss_grapheme': 0.313882, 'loss_vowel': 0.230896, 'loss_consonant': 0.153054, 'loss_word': 0.214174}\n",
      "###>>>>> saved\n",
      "   48 | 0.000206 | 160000/160635 | 17.8309 | 15.7753 |\n",
      "val: {'recall': 0.990196, 'recall_grapheme': 0.986337, 'recall_vowel': 0.993732, 'recall_consonant': 0.994377, 'recall_word': 0.986937, 'acc_grapheme': 0.985251, 'acc_vowel': 0.993608, 'acc_consonant': 0.994578, 'acc_word': 0.986867, 'loss_grapheme': 0.318983, 'loss_vowel': 0.235957, 'loss_consonant': 0.153987, 'loss_word': 0.196371}\n",
      "   49 | 0.000200 | 160000/160635 | 20.2042 | 16.1598 |\n",
      "val: {'recall': 0.990818, 'recall_grapheme': 0.987193, 'recall_vowel': 0.994288, 'recall_consonant': 0.994597, 'recall_word': 0.987778, 'acc_grapheme': 0.985773, 'acc_vowel': 0.994279, 'acc_consonant': 0.994951, 'acc_word': 0.987837, 'loss_grapheme': 0.274768, 'loss_vowel': 0.204044, 'loss_consonant': 0.133328, 'loss_word': 0.171912}\n",
      "   50 | 0.000194 | 160000/160635 | 12.6637 | 15.3295 |\n",
      "val: {'recall': 0.99084, 'recall_grapheme': 0.987309, 'recall_vowel': 0.993815, 'recall_consonant': 0.994925, 'recall_word': 0.987989, 'acc_grapheme': 0.98627, 'acc_vowel': 0.99408, 'acc_consonant': 0.995001, 'acc_word': 0.987987, 'loss_grapheme': 0.2962, 'loss_vowel': 0.219603, 'loss_consonant': 0.139805, 'loss_word': 0.193349}\n",
      "   51 | 0.000187 | 160000/160635 | 15.1974 | 16.3897 |\n",
      "val: {'recall': 0.990961, 'recall_grapheme': 0.987545, 'recall_vowel': 0.994668, 'recall_consonant': 0.994088, 'recall_word': 0.987692, 'acc_grapheme': 0.986121, 'acc_vowel': 0.994379, 'acc_consonant': 0.994951, 'acc_word': 0.987738, 'loss_grapheme': 0.258702, 'loss_vowel': 0.193913, 'loss_consonant': 0.12452, 'loss_word': 0.170629}\n",
      "   52 | 0.000181 | 160000/160635 | 15.5097 | 15.8344 |\n",
      "val: {'recall': 0.990461, 'recall_grapheme': 0.986435, 'recall_vowel': 0.993856, 'recall_consonant': 0.995116, 'recall_word': 0.987904, 'acc_grapheme': 0.985425, 'acc_vowel': 0.993832, 'acc_consonant': 0.994677, 'acc_word': 0.987837, 'loss_grapheme': 0.30679, 'loss_vowel': 0.227848, 'loss_consonant': 0.154195, 'loss_word': 0.18213}\n",
      "   53 | 0.000175 | 160000/160635 | 18.6329 | 15.9160 |\n",
      "val: {'recall': 0.9909, 'recall_grapheme': 0.986836, 'recall_vowel': 0.994525, 'recall_consonant': 0.995404, 'recall_word': 0.988036, 'acc_grapheme': 0.985897, 'acc_vowel': 0.99423, 'acc_consonant': 0.994926, 'acc_word': 0.988061, 'loss_grapheme': 0.275607, 'loss_vowel': 0.211016, 'loss_consonant': 0.141818, 'loss_word': 0.162223}\n",
      "   54 | 0.000169 | 160000/160635 | 18.2592 | 15.5298 |\n",
      "val: {'recall': 0.990996, 'recall_grapheme': 0.986984, 'recall_vowel': 0.994301, 'recall_consonant': 0.995716, 'recall_word': 0.988333, 'acc_grapheme': 0.985798, 'acc_vowel': 0.994354, 'acc_consonant': 0.994901, 'acc_word': 0.98831, 'loss_grapheme': 0.30841, 'loss_vowel': 0.237427, 'loss_consonant': 0.153178, 'loss_word': 0.182059}\n",
      "   55 | 0.000163 | 160000/160635 | 20.9746 | 15.5560 |\n",
      "val: {'recall': 0.990341, 'recall_grapheme': 0.98616, 'recall_vowel': 0.993766, 'recall_consonant': 0.995278, 'recall_word': 0.987536, 'acc_grapheme': 0.984927, 'acc_vowel': 0.993931, 'acc_consonant': 0.994478, 'acc_word': 0.987415, 'loss_grapheme': 0.329232, 'loss_vowel': 0.257921, 'loss_consonant': 0.168136, 'loss_word': 0.186577}\n",
      "   56 | 0.000156 | 160000/160635 | 10.1844 | 15.5165 |\n",
      "val: {'recall': 0.990997, 'recall_grapheme': 0.987578, 'recall_vowel': 0.99452, 'recall_consonant': 0.994314, 'recall_word': 0.988144, 'acc_grapheme': 0.986121, 'acc_vowel': 0.994304, 'acc_consonant': 0.994976, 'acc_word': 0.988136, 'loss_grapheme': 0.273579, 'loss_vowel': 0.212448, 'loss_consonant': 0.141618, 'loss_word': 0.163656}\n",
      "   57 | 0.000150 | 160000/160635 | 7.2618 | 16.3662 ||\n",
      "val: {'recall': 0.99126, 'recall_grapheme': 0.987626, 'recall_vowel': 0.994343, 'recall_consonant': 0.995444, 'recall_word': 0.988735, 'acc_grapheme': 0.986867, 'acc_vowel': 0.994404, 'acc_consonant': 0.995249, 'acc_word': 0.988758, 'loss_grapheme': 0.244243, 'loss_vowel': 0.184031, 'loss_consonant': 0.1213, 'loss_word': 0.147677}\n",
      "###>>>>> saved\n",
      "   58 | 0.000144 | 160000/160635 | 15.2638 | 15.3465 |\n",
      "val: {'recall': 0.99164, 'recall_grapheme': 0.988478, 'recall_vowel': 0.99497, 'recall_consonant': 0.994634, 'recall_word': 0.988096, 'acc_grapheme': 0.987315, 'acc_vowel': 0.994827, 'acc_consonant': 0.995647, 'acc_word': 0.988111, 'loss_grapheme': 0.217732, 'loss_vowel': 0.160714, 'loss_consonant': 0.108939, 'loss_word': 0.147056}\n",
      "###>>>>> saved\n",
      "   59 | 0.000138 | 160000/160635 | 10.6169 | 15.6226 |\n",
      "val: {'recall': 0.991297, 'recall_grapheme': 0.987611, 'recall_vowel': 0.994734, 'recall_consonant': 0.99523, 'recall_word': 0.988544, 'acc_grapheme': 0.98724, 'acc_vowel': 0.994578, 'acc_consonant': 0.995473, 'acc_word': 0.988534, 'loss_grapheme': 0.251801, 'loss_vowel': 0.186986, 'loss_consonant': 0.12689, 'loss_word': 0.15499}\n",
      "   60 | 0.000132 | 160000/160635 | 17.1215 | 15.3304 |\n",
      "val: {'recall': 0.991613, 'recall_grapheme': 0.988125, 'recall_vowel': 0.994797, 'recall_consonant': 0.995405, 'recall_word': 0.988305, 'acc_grapheme': 0.986768, 'acc_vowel': 0.994578, 'acc_consonant': 0.995075, 'acc_word': 0.988335, 'loss_grapheme': 0.301748, 'loss_vowel': 0.247315, 'loss_consonant': 0.15799, 'loss_word': 0.178363}\n",
      "   61 | 0.000126 | 160000/160635 | 20.7184 | 15.2986 |\n",
      "val: {'recall': 0.991165, 'recall_grapheme': 0.988065, 'recall_vowel': 0.994295, 'recall_consonant': 0.994237, 'recall_word': 0.988379, 'acc_grapheme': 0.98739, 'acc_vowel': 0.994379, 'acc_consonant': 0.995324, 'acc_word': 0.988434, 'loss_grapheme': 0.24155, 'loss_vowel': 0.190274, 'loss_consonant': 0.122931, 'loss_word': 0.143479}\n",
      "   62 | 0.000121 | 160000/160635 | 5.6094 | 15.2014 ||\n",
      "val: {'recall': 0.991934, 'recall_grapheme': 0.988712, 'recall_vowel': 0.99493, 'recall_consonant': 0.995383, 'recall_word': 0.98844, 'acc_grapheme': 0.987116, 'acc_vowel': 0.994528, 'acc_consonant': 0.995647, 'acc_word': 0.988509, 'loss_grapheme': 0.244574, 'loss_vowel': 0.189367, 'loss_consonant': 0.122491, 'loss_word': 0.155123}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###>>>>> saved\n",
      "   63 | 0.000115 | 160000/160635 | 21.0869 | 15.4909 |\n",
      "val: {'recall': 0.991303, 'recall_grapheme': 0.987797, 'recall_vowel': 0.994674, 'recall_consonant': 0.994944, 'recall_word': 0.988234, 'acc_grapheme': 0.987191, 'acc_vowel': 0.994652, 'acc_consonant': 0.99515, 'acc_word': 0.988285, 'loss_grapheme': 0.248291, 'loss_vowel': 0.195017, 'loss_consonant': 0.12395, 'loss_word': 0.15407}\n",
      "   64 | 0.000109 | 160000/160635 | 17.6079 | 15.3241 |\n",
      "val: {'recall': 0.991486, 'recall_grapheme': 0.987871, 'recall_vowel': 0.994552, 'recall_consonant': 0.995651, 'recall_word': 0.989058, 'acc_grapheme': 0.986818, 'acc_vowel': 0.994354, 'acc_consonant': 0.99505, 'acc_word': 0.989006, 'loss_grapheme': 0.301503, 'loss_vowel': 0.234748, 'loss_consonant': 0.153272, 'loss_word': 0.16906}\n",
      "   65 | 0.000104 | 160000/160635 | 11.3319 | 15.2315 |\n",
      "val: {'recall': 0.991995, 'recall_grapheme': 0.988611, 'recall_vowel': 0.995208, 'recall_consonant': 0.99555, 'recall_word': 0.988736, 'acc_grapheme': 0.987837, 'acc_vowel': 0.995075, 'acc_consonant': 0.995523, 'acc_word': 0.988807, 'loss_grapheme': 0.232567, 'loss_vowel': 0.180341, 'loss_consonant': 0.113221, 'loss_word': 0.153469}\n",
      "###>>>>> saved\n",
      "   66 | 0.000098 | 160000/160635 | 20.4229 | 15.2405 |\n",
      "val: {'recall': 0.991734, 'recall_grapheme': 0.988498, 'recall_vowel': 0.994864, 'recall_consonant': 0.995076, 'recall_word': 0.988369, 'acc_grapheme': 0.987489, 'acc_vowel': 0.994702, 'acc_consonant': 0.995448, 'acc_word': 0.988409, 'loss_grapheme': 0.229002, 'loss_vowel': 0.175597, 'loss_consonant': 0.115152, 'loss_word': 0.145149}\n",
      "   67 | 0.000093 | 160000/160635 | 11.9867 | 15.1450 |\n",
      "val: {'recall': 0.991328, 'recall_grapheme': 0.987367, 'recall_vowel': 0.99494, 'recall_consonant': 0.995638, 'recall_word': 0.988767, 'acc_grapheme': 0.986643, 'acc_vowel': 0.994652, 'acc_consonant': 0.995423, 'acc_word': 0.988807, 'loss_grapheme': 0.28292, 'loss_vowel': 0.219256, 'loss_consonant': 0.142424, 'loss_word': 0.168302}\n",
      "   68 | 0.000088 | 160000/160635 | 11.6757 | 15.0957 |\n",
      "val: {'recall': 0.991239, 'recall_grapheme': 0.987707, 'recall_vowel': 0.993935, 'recall_consonant': 0.995607, 'recall_word': 0.989133, 'acc_grapheme': 0.986544, 'acc_vowel': 0.994155, 'acc_consonant': 0.994951, 'acc_word': 0.98918, 'loss_grapheme': 0.322736, 'loss_vowel': 0.264851, 'loss_consonant': 0.164949, 'loss_word': 0.174399}\n",
      "   69 | 0.000082 | 160000/160635 | 11.5914 | 15.4230 |\n",
      "val: {'recall': 0.992347, 'recall_grapheme': 0.989214, 'recall_vowel': 0.995092, 'recall_consonant': 0.995866, 'recall_word': 0.989102, 'acc_grapheme': 0.988186, 'acc_vowel': 0.994851, 'acc_consonant': 0.995871, 'acc_word': 0.989156, 'loss_grapheme': 0.229701, 'loss_vowel': 0.165878, 'loss_consonant': 0.110527, 'loss_word': 0.156143}\n",
      "###>>>>> saved\n",
      "   70 | 0.000077 | 160000/160635 | 17.5724 | 15.6579 |\n",
      "val: {'recall': 0.991741, 'recall_grapheme': 0.988223, 'recall_vowel': 0.995016, 'recall_consonant': 0.995504, 'recall_word': 0.989427, 'acc_grapheme': 0.987415, 'acc_vowel': 0.994851, 'acc_consonant': 0.995672, 'acc_word': 0.989454, 'loss_grapheme': 0.240913, 'loss_vowel': 0.18645, 'loss_consonant': 0.125173, 'loss_word': 0.142162}\n",
      "   71 | 0.000073 | 160000/160635 | 13.8454 | 15.9541 |\n",
      "val: {'recall': 0.991832, 'recall_grapheme': 0.988204, 'recall_vowel': 0.994876, 'recall_consonant': 0.996043, 'recall_word': 0.988445, 'acc_grapheme': 0.98734, 'acc_vowel': 0.994677, 'acc_consonant': 0.995797, 'acc_word': 0.988459, 'loss_grapheme': 0.252039, 'loss_vowel': 0.193635, 'loss_consonant': 0.125448, 'loss_word': 0.161492}\n",
      "   72 | 0.000068 | 160000/160635 | 5.1014 | 15.0514 ||\n",
      "val: {'recall': 0.992212, 'recall_grapheme': 0.989558, 'recall_vowel': 0.995012, 'recall_consonant': 0.99472, 'recall_word': 0.989107, 'acc_grapheme': 0.988559, 'acc_vowel': 0.994976, 'acc_consonant': 0.995946, 'acc_word': 0.98918, 'loss_grapheme': 0.206889, 'loss_vowel': 0.146962, 'loss_consonant': 0.101167, 'loss_word': 0.133665}\n",
      "   73 | 0.000063 | 160000/160635 | 8.4648 | 15.2737 ||\n",
      "val: {'recall': 0.992082, 'recall_grapheme': 0.988763, 'recall_vowel': 0.994924, 'recall_consonant': 0.995877, 'recall_word': 0.98944, 'acc_grapheme': 0.988086, 'acc_vowel': 0.994951, 'acc_consonant': 0.995896, 'acc_word': 0.989429, 'loss_grapheme': 0.226571, 'loss_vowel': 0.16748, 'loss_consonant': 0.110369, 'loss_word': 0.14597}\n",
      "   74 | 0.000059 | 160000/160635 | 13.8177 | 14.9581 |\n",
      "val: {'recall': 0.991834, 'recall_grapheme': 0.988681, 'recall_vowel': 0.995155, 'recall_consonant': 0.99482, 'recall_word': 0.989176, 'acc_grapheme': 0.987887, 'acc_vowel': 0.994926, 'acc_consonant': 0.995821, 'acc_word': 0.989255, 'loss_grapheme': 0.241058, 'loss_vowel': 0.187461, 'loss_consonant': 0.119594, 'loss_word': 0.153814}\n",
      "   75 | 0.000054 | 160000/160635 | 15.0491 | 15.3840 |\n",
      "val: {'recall': 0.991671, 'recall_grapheme': 0.988075, 'recall_vowel': 0.994825, 'recall_consonant': 0.99571, 'recall_word': 0.989163, 'acc_grapheme': 0.98734, 'acc_vowel': 0.994652, 'acc_consonant': 0.995672, 'acc_word': 0.989205, 'loss_grapheme': 0.269059, 'loss_vowel': 0.218702, 'loss_consonant': 0.141698, 'loss_word': 0.162969}\n",
      "   76 | 0.000050 | 160000/160635 | 13.6407 | 14.8288 |\n",
      "val: {'recall': 0.992383, 'recall_grapheme': 0.989255, 'recall_vowel': 0.995139, 'recall_consonant': 0.995884, 'recall_word': 0.989408, 'acc_grapheme': 0.989081, 'acc_vowel': 0.995175, 'acc_consonant': 0.996145, 'acc_word': 0.989454, 'loss_grapheme': 0.167321, 'loss_vowel': 0.120949, 'loss_consonant': 0.078694, 'loss_word': 0.113641}\n",
      "###>>>>> saved\n",
      "   77 | 0.000046 | 160000/160635 | 18.1327 | 15.3466 |\n",
      "val: {'recall': 0.991606, 'recall_grapheme': 0.987872, 'recall_vowel': 0.994912, 'recall_consonant': 0.99577, 'recall_word': 0.988761, 'acc_grapheme': 0.987489, 'acc_vowel': 0.994727, 'acc_consonant': 0.995672, 'acc_word': 0.988807, 'loss_grapheme': 0.276516, 'loss_vowel': 0.218427, 'loss_consonant': 0.140244, 'loss_word': 0.16081}\n",
      "   78 | 0.000042 | 160000/160635 | 16.5099 | 14.8519 |\n",
      "val: {'recall': 0.99127, 'recall_grapheme': 0.988098, 'recall_vowel': 0.994802, 'recall_consonant': 0.994084, 'recall_word': 0.989157, 'acc_grapheme': 0.987589, 'acc_vowel': 0.994802, 'acc_consonant': 0.995598, 'acc_word': 0.98918, 'loss_grapheme': 0.299609, 'loss_vowel': 0.232466, 'loss_consonant': 0.151835, 'loss_word': 0.176826}\n",
      "   79 | 0.000038 | 160000/160635 | 15.8550 | 14.9167 |\n",
      "val: {'recall': 0.991789, 'recall_grapheme': 0.988314, 'recall_vowel': 0.994754, 'recall_consonant': 0.995773, 'recall_word': 0.989347, 'acc_grapheme': 0.987887, 'acc_vowel': 0.994727, 'acc_consonant': 0.995821, 'acc_word': 0.989379, 'loss_grapheme': 0.256434, 'loss_vowel': 0.206098, 'loss_consonant': 0.13201, 'loss_word': 0.146109}\n",
      "   80 | 0.000035 | 160000/160635 | 11.9438 | 14.9235 |\n",
      "val: {'recall': 0.992275, 'recall_grapheme': 0.988983, 'recall_vowel': 0.995245, 'recall_consonant': 0.995892, 'recall_word': 0.989615, 'acc_grapheme': 0.988409, 'acc_vowel': 0.99505, 'acc_consonant': 0.99607, 'acc_word': 0.989653, 'loss_grapheme': 0.212035, 'loss_vowel': 0.156516, 'loss_consonant': 0.101814, 'loss_word': 0.13834}\n",
      "   81 | 0.000031 | 160000/160635 | 7.6754 | 14.8821 ||\n",
      "val: {'recall': 0.992515, 'recall_grapheme': 0.990199, 'recall_vowel': 0.995289, 'recall_consonant': 0.994371, 'recall_word': 0.989434, 'acc_grapheme': 0.989379, 'acc_vowel': 0.995249, 'acc_consonant': 0.996219, 'acc_word': 0.989504, 'loss_grapheme': 0.180598, 'loss_vowel': 0.129995, 'loss_consonant': 0.083757, 'loss_word': 0.13174}\n",
      "###>>>>> saved\n",
      "   82 | 0.000028 | 160000/160635 | 14.7509 | 15.4956 |\n",
      "val: {'recall': 0.991362, 'recall_grapheme': 0.987695, 'recall_vowel': 0.994347, 'recall_consonant': 0.995711, 'recall_word': 0.989126, 'acc_grapheme': 0.987017, 'acc_vowel': 0.99423, 'acc_consonant': 0.995399, 'acc_word': 0.989131, 'loss_grapheme': 0.320093, 'loss_vowel': 0.268967, 'loss_consonant': 0.172667, 'loss_word': 0.176597}\n",
      "   83 | 0.000025 | 160000/160635 | 21.1181 | 15.9073 |\n",
      "val: {'recall': 0.991696, 'recall_grapheme': 0.98814, 'recall_vowel': 0.994602, 'recall_consonant': 0.995901, 'recall_word': 0.989227, 'acc_grapheme': 0.987713, 'acc_vowel': 0.994677, 'acc_consonant': 0.995772, 'acc_word': 0.98928, 'loss_grapheme': 0.25926, 'loss_vowel': 0.198457, 'loss_consonant': 0.125538, 'loss_word': 0.150843}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   84 | 0.000022 | 160000/160635 | 19.3180 | 14.6824 |\n",
      "val: {'recall': 0.991856, 'recall_grapheme': 0.9882, 'recall_vowel': 0.99511, 'recall_consonant': 0.995914, 'recall_word': 0.989211, 'acc_grapheme': 0.987663, 'acc_vowel': 0.995001, 'acc_consonant': 0.995722, 'acc_word': 0.98923, 'loss_grapheme': 0.262447, 'loss_vowel': 0.205642, 'loss_consonant': 0.128688, 'loss_word': 0.160564}\n",
      "   85 | 0.000019 | 160000/160635 | 16.4641 | 15.8382 |\n",
      "val: {'recall': 0.99163, 'recall_grapheme': 0.988013, 'recall_vowel': 0.994678, 'recall_consonant': 0.995817, 'recall_word': 0.989342, 'acc_grapheme': 0.987191, 'acc_vowel': 0.994503, 'acc_consonant': 0.995399, 'acc_word': 0.989379, 'loss_grapheme': 0.314966, 'loss_vowel': 0.257951, 'loss_consonant': 0.16325, 'loss_word': 0.172734}\n",
      "   86 | 0.000016 | 160000/160635 | 13.6084 | 15.3989 |\n",
      "val: {'recall': 0.991518, 'recall_grapheme': 0.988468, 'recall_vowel': 0.994388, 'recall_consonant': 0.994748, 'recall_word': 0.989141, 'acc_grapheme': 0.987862, 'acc_vowel': 0.994603, 'acc_consonant': 0.995647, 'acc_word': 0.989156, 'loss_grapheme': 0.240468, 'loss_vowel': 0.190033, 'loss_consonant': 0.124846, 'loss_word': 0.131344}\n",
      "   87 | 0.000014 | 160000/160635 | 20.0424 | 14.9108 |\n",
      "val: {'recall': 0.991971, 'recall_grapheme': 0.988647, 'recall_vowel': 0.994802, 'recall_consonant': 0.995789, 'recall_word': 0.989524, 'acc_grapheme': 0.988186, 'acc_vowel': 0.994827, 'acc_consonant': 0.995846, 'acc_word': 0.989529, 'loss_grapheme': 0.229895, 'loss_vowel': 0.181676, 'loss_consonant': 0.115813, 'loss_word': 0.134058}\n",
      "   88 | 0.000012 | 160000/160635 | 13.9081 | 14.5555 |\n",
      "val: {'recall': 0.992253, 'recall_grapheme': 0.988847, 'recall_vowel': 0.994874, 'recall_consonant': 0.996445, 'recall_word': 0.989735, 'acc_grapheme': 0.988509, 'acc_vowel': 0.994802, 'acc_consonant': 0.996045, 'acc_word': 0.989753, 'loss_grapheme': 0.225989, 'loss_vowel': 0.179977, 'loss_consonant': 0.118972, 'loss_word': 0.126985}\n",
      "   89 | 0.000010 | 160000/160635 | 5.6774 | 15.4479 ||\n",
      "val: {'recall': 0.992362, 'recall_grapheme': 0.989112, 'recall_vowel': 0.995245, 'recall_consonant': 0.99598, 'recall_word': 0.989539, 'acc_grapheme': 0.988658, 'acc_vowel': 0.995001, 'acc_consonant': 0.996095, 'acc_word': 0.989578, 'loss_grapheme': 0.239795, 'loss_vowel': 0.180544, 'loss_consonant': 0.11578, 'loss_word': 0.150779}\n",
      "   90 | 0.000008 | 160000/160635 | 9.2286 | 15.0841 ||\n",
      "val: {'recall': 0.992018, 'recall_grapheme': 0.988729, 'recall_vowel': 0.994807, 'recall_consonant': 0.995808, 'recall_word': 0.989698, 'acc_grapheme': 0.988036, 'acc_vowel': 0.994976, 'acc_consonant': 0.996045, 'acc_word': 0.989728, 'loss_grapheme': 0.240017, 'loss_vowel': 0.185679, 'loss_consonant': 0.123295, 'loss_word': 0.142788}\n",
      "   91 | 0.000006 | 160000/160635 | 6.3401 | 14.7057 ||\n",
      "val: {'recall': 0.991902, 'recall_grapheme': 0.988716, 'recall_vowel': 0.995259, 'recall_consonant': 0.994916, 'recall_word': 0.989348, 'acc_grapheme': 0.988111, 'acc_vowel': 0.995025, 'acc_consonant': 0.996095, 'acc_word': 0.98933, 'loss_grapheme': 0.229332, 'loss_vowel': 0.173922, 'loss_consonant': 0.110303, 'loss_word': 0.146055}\n",
      "   92 | 0.000005 | 160000/160635 | 15.2676 | 15.0994 |\n",
      "val: {'recall': 0.99244, 'recall_grapheme': 0.989274, 'recall_vowel': 0.9952, 'recall_consonant': 0.996014, 'recall_word': 0.98951, 'acc_grapheme': 0.988658, 'acc_vowel': 0.995125, 'acc_consonant': 0.996045, 'acc_word': 0.989554, 'loss_grapheme': 0.220525, 'loss_vowel': 0.168363, 'loss_consonant': 0.109643, 'loss_word': 0.13928}\n",
      "   93 | 0.000004 | 160000/160635 | 14.1091 | 15.6024 |\n",
      "val: {'recall': 0.991921, 'recall_grapheme': 0.988499, 'recall_vowel': 0.994738, 'recall_consonant': 0.995949, 'recall_word': 0.989582, 'acc_grapheme': 0.988061, 'acc_vowel': 0.994827, 'acc_consonant': 0.995871, 'acc_word': 0.989628, 'loss_grapheme': 0.247576, 'loss_vowel': 0.192146, 'loss_consonant': 0.123482, 'loss_word': 0.144189}\n",
      "   94 | 0.000002 | 160000/160635 | 19.0936 | 15.3838 |\n",
      "val: {'recall': 0.991857, 'recall_grapheme': 0.988404, 'recall_vowel': 0.994806, 'recall_consonant': 0.995815, 'recall_word': 0.989413, 'acc_grapheme': 0.987688, 'acc_vowel': 0.994777, 'acc_consonant': 0.995598, 'acc_word': 0.989404, 'loss_grapheme': 0.284055, 'loss_vowel': 0.232263, 'loss_consonant': 0.146886, 'loss_word': 0.158985}\n",
      "   95 | 0.000002 | 088960/160635 | 22.1310 | 15.2445 |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc7c8eb40d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chec/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/chec/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/chec/anaconda3/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/home/chec/anaconda3/lib/python3.7/multiprocessing/popen_fork.py\", line 48, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/home/chec/anaconda3/lib/python3.7/multiprocessing/popen_fork.py\", line 28, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-6df1386d1bbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-f52fdf61a139>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcycle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_cycles\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CYCLE:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtrain_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-f52fdf61a139>\u001b[0m in \u001b[0;36mtrain_cycle\u001b[0;34m(args, model, optimizer, lr_scheduler)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0;31m#loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(args, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_model(model, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
