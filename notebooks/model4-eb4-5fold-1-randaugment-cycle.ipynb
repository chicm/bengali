{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time, gc\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pretrainedmodels\n",
    "from argparse import Namespace\n",
    "from sklearn.utils import shuffle\n",
    "from apex import amp\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from cvcore.data.auto_augment import RandAugment\n",
    "from PIL import Image\n",
    "from utils import bn_update, moving_average, copy_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_map.csv\t\t       train.csv\r\n",
      "sample_submission.csv\t       train.csv.zip\r\n",
      "test.csv\t\t       train_image_data_0.parquet\r\n",
      "test_image_data_0.parquet      train_image_data_0.parquet.zip\r\n",
      "test_image_data_0.parquet.zip  train_image_data_1.parquet\r\n",
      "test_image_data_1.parquet      train_image_data_1.parquet.zip\r\n",
      "test_image_data_1.parquet.zip  train_image_data_2.parquet\r\n",
      "test_image_data_2.parquet      train_image_data_2.parquet.zip\r\n",
      "test_image_data_2.parquet.zip  train_image_data_3.parquet\r\n",
      "test_image_data_3.parquet      train_image_data_3.parquet.zip\r\n",
      "test_image_data_3.parquet.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/chec/data/bengali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /home/chec/data/bengali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/chec/data/bengali'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "class_map_df = pd.read_csv(f'{DATA_DIR}/class_map.csv')\n",
    "sample_sub_df = pd.read_csv(f'{DATA_DIR}/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>ক্ট্রো</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_1</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>হ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>খ্রী</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_3</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>র্টি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_4</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>থ্রো</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  grapheme_root  vowel_diacritic  consonant_diacritic grapheme\n",
       "0  Train_0             15                9                    5   ক্ট্রো\n",
       "1  Train_1            159                0                    0        হ\n",
       "2  Train_2             22                3                    5     খ্রী\n",
       "3  Train_3             53                2                    2     র্টি\n",
       "4  Train_4             71                9                    5     থ্রো"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 137\n",
    "WIDTH = 236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import albumentations as albu\n",
    "def get_train_augs():\n",
    "    return RandAugment(n=2, m=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class BengaliDataset(Dataset):\n",
    "    def __init__(self, df, img_df, train_mode=True, test_mode=False):\n",
    "        self.df = df\n",
    "        self.img_df = img_df\n",
    "        self.train_mode = train_mode\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = self.get_img(row.image_id)\n",
    "        orig_img = img.copy()\n",
    "        #print(img.shape)\n",
    "        if self.train_mode:\n",
    "            augs = get_train_augs()\n",
    "            #img = augs(image=img)['image']\n",
    "            img = np.asarray(augs(Image.fromarray(img)))\n",
    "        \n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "        orig_img = np.expand_dims(orig_img, axis=-1)\n",
    "        \n",
    "        #print('###', img.shape)\n",
    "        #img = np.concatenate([img, img, img], 2)\n",
    "        #print('>>>', img.shape)\n",
    "        \n",
    "        # taken from https://www.kaggle.com/iafoss/image-preprocessing-128x128\n",
    "        #MEAN = [ 0.06922848809290576,  0.06922848809290576,  0.06922848809290576]\n",
    "        #STD = [ 0.20515700083327537,  0.20515700083327537,  0.20515700083327537]\n",
    "        \n",
    "        img = transforms.functional.to_tensor(img)\n",
    "        orig_img = transforms.functional.to_tensor(orig_img)\n",
    "        \n",
    "        #img = transforms.functional.normalize(img, mean=MEAN, std=STD)\n",
    "        \n",
    "        if self.test_mode:\n",
    "            return img\n",
    "        elif self.train_mode:\n",
    "            return img, orig_img, torch.tensor([row.grapheme_root, row.vowel_diacritic, row.consonant_diacritic, row.word_label])\n",
    "        else:\n",
    "            return img, torch.tensor([row.grapheme_root, row.vowel_diacritic, row.consonant_diacritic, row.word_label])\n",
    "                    \n",
    "    def get_img(self, img_id):\n",
    "        return 255 - self.img_df.loc[img_id].values.reshape(HEIGHT, WIDTH).astype(np.uint8)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "def get_train_val_loaders(batch_size=4, val_batch_size=4, ifold=0, dev_mode=False):\n",
    "    train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "\n",
    "    train_df = shuffle(train_df, random_state=1234)\n",
    "\n",
    "    grapheme_words = np.unique(train_df.grapheme.values)\n",
    "    grapheme_words_dict = {grapheme: i for i, grapheme in enumerate(grapheme_words)}\n",
    "    train_df['word_label'] = train_df['grapheme'].map(lambda x: grapheme_words_dict[x])\n",
    "\n",
    "    print(train_df.shape)\n",
    "\n",
    "    if dev_mode:\n",
    "        img_df = pd.read_parquet(f'{DATA_DIR}/train_image_data_0.parquet').set_index('image_id')\n",
    "        train_df = train_df.iloc[:1000]\n",
    "    else:\n",
    "        img_dfs = [pd.read_parquet(f'{DATA_DIR}/train_image_data_{i}.parquet') for i in range(4)]\n",
    "        img_df = pd.concat(img_dfs, axis=0).set_index('image_id')\n",
    "    print(img_df.shape)\n",
    "    #split_index = int(len(train_df) * 0.9)\n",
    "    \n",
    "    #train = train_df.iloc[:split_index]\n",
    "    #val = train_df.iloc[split_index:]\n",
    "    \n",
    "    kf = StratifiedKFold(5, random_state=1234, shuffle=True)\n",
    "    for i, (train_idx, val_idx) in enumerate(kf.split(train_df, train_df['grapheme_root'].values)):\n",
    "        if i == ifold:\n",
    "            #print(val_idx)\n",
    "            train = train_df.iloc[train_idx]\n",
    "            val = train_df.iloc[val_idx]\n",
    "            break\n",
    "    assert i == ifold\n",
    "    print(train.shape, val.shape)\n",
    "    \n",
    "    train_ds = BengaliDataset(train, img_df, True, False)\n",
    "    val_ds = BengaliDataset(val, img_df, False, False)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "    train_loader.num = len(train_ds)\n",
    "\n",
    "    val_loader = DataLoader(val_ds, batch_size=val_batch_size, shuffle=False, num_workers=8, drop_last=False)\n",
    "    val_loader.num = len(val_ds)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader, val_loader = get_train_val_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x in train_loader:\n",
    "#    print(x)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'resnet50' # could be fbresnet152 or inceptionresnetv2\n",
    "#model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet').cuda()\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import timm\n",
    "from timm.models.activations import Swish, Mish\n",
    "from timm.models.adaptive_avgmax_pool import SelectAdaptivePool2d\n",
    "\n",
    "MEAN = [ 0.06922848809290576 ]\n",
    "STD = [ 0.20515700083327537 ]\n",
    "\n",
    "class BengaliNet(nn.Module):\n",
    "    \"\"\"\n",
    "    EfficientNet B0-B8.\n",
    "    Args:\n",
    "        cfg (CfgNode): configs\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super(BengaliNet, self).__init__()\n",
    "        model_name = cfg.MODEL_NAME\n",
    "        pretrained = cfg.PRETRAINED\n",
    "        input_channels = cfg.IN_CHANNELS\n",
    "        pool_type = cfg.POOL_TYPE\n",
    "        drop_connect_rate = cfg.DROP_CONNECT\n",
    "        self.drop_rate = cfg.DROPOUT\n",
    "        cls_head = cfg.CLS_HEAD\n",
    "        num_total_classes = cfg.NUM_GRAPHEME_CLASSES + cfg.NUM_VOWEL_CLASSES + cfg.NUM_CONSONANT_CLASSES \\\n",
    "            + cfg.NUM_WORD_CLASSES\n",
    "\n",
    "        backbone = timm.create_model(\n",
    "            model_name=model_name,\n",
    "            pretrained=pretrained,\n",
    "            in_chans=input_channels,\n",
    "            drop_connect_rate=drop_connect_rate,\n",
    "        )\n",
    "        self.conv_stem = backbone.conv_stem\n",
    "        self.bn1 = backbone.bn1\n",
    "        self.act1 = backbone.act1\n",
    "        ### Original blocks ###\n",
    "        for i in range(len((backbone.blocks))):\n",
    "            setattr(self, \"block{}\".format(str(i)), backbone.blocks[i])\n",
    "        self.conv_head = backbone.conv_head\n",
    "        self.bn2 = backbone.bn2\n",
    "        self.act2 = backbone.act2\n",
    "        self.aux_block5 = backbone.blocks[5]\n",
    "        self.aux_num_features = self.block5[-1].bn3.num_features\n",
    "        self.aux_head4 = nn.Conv2d(self.aux_num_features, self.aux_num_features * 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(self.aux_num_features * 4)\n",
    "        self.act4 = Swish()\n",
    "        self.aux_head5 = nn.Conv2d(self.aux_num_features, self.aux_num_features * 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(self.aux_num_features * 4)\n",
    "        self.act5 = Swish()\n",
    "        self.global_pool = SelectAdaptivePool2d(pool_type=pool_type)\n",
    "        self.num_features = backbone.num_features * self.global_pool.feat_mult()\n",
    "        assert cls_head == 'linear'\n",
    "        if cls_head == \"linear\":\n",
    "            ### Baseline head ###\n",
    "            self.fc = nn.Linear(self.num_features, num_total_classes)            \n",
    "            self.aux_fc1 = nn.Linear(self.aux_num_features*4, num_total_classes)\n",
    "            self.aux_fc2 = nn.Linear(self.aux_num_features*4, num_total_classes)\n",
    "            \n",
    "            for fc in [self.fc, self.aux_fc1, self.aux_fc2]:\n",
    "                nn.init.zeros_(fc.bias.data)\n",
    "        elif cls_head == \"norm_softmax\":\n",
    "            ### NormSoftmax ###\n",
    "            self.grapheme_fc = NormSoftmax(self.num_features, num_grapheme_classes)\n",
    "            self.consonant_fc = NormSoftmax(self.num_features, num_consonant_classes)\n",
    "            self.vowel_fc = NormSoftmax(self.num_features, num_vowel_classes)\n",
    "        # Replace with Mish activation\n",
    "        if cfg.MODEL_ACTIVATION == \"mish\":\n",
    "            convert_swish_to_mish(self)\n",
    "        del backbone\n",
    "\n",
    "    def _features(self, x):\n",
    "        x = self.conv_stem(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.block0(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x); b4 = x\n",
    "        x = self.block5(x); b4 = self.aux_block5(b4); b5 = x\n",
    "        x = self.block6(x)\n",
    "        x = self.conv_head(x); b4 = self.aux_head4(b4); b5 = self.aux_head5(b5)\n",
    "        x = self.bn2(x); b4 = self.bn4(b4); b5 = self.bn5(b5)\n",
    "        x = self.act2(x); b4 = self.act4(b4); b5 = self.act5(b5)\n",
    "        return b4, b5, x\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(x)):\n",
    "            transforms.functional.normalize(x[i], mean=MEAN, std=STD, inplace=True)\n",
    "\n",
    "        # _, _, x = self._features(x)\n",
    "        b4, b5, x = self._features(x)\n",
    "        x = self.global_pool(x); b4 = self.global_pool(b4); b5 = self.global_pool(b5)\n",
    "        x = torch.flatten(x, 1); b4 = torch.flatten(b4, 1); b5 = torch.flatten(b5, 1)\n",
    "        if self.drop_rate > 0.:\n",
    "            x = F.dropout(x, p=self.drop_rate, training=self.training)\n",
    "        logits = self.fc(x)\n",
    "        \n",
    "        aux_logits1 = self.aux_fc1(b4)\n",
    "        aux_logits2 = self.aux_fc2(b5)\n",
    "        \n",
    "        return logits, aux_logits1, aux_logits2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './model4-ckps'\n",
    "def create_model(cfg):\n",
    "    model = BengaliNet(cfg)\n",
    "    model_file = os.path.join(MODEL_DIR, cfg.MODEL_NAME, cfg.CKP_NAME)\n",
    "\n",
    "    parent_dir = os.path.dirname(model_file)\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "\n",
    "    print('model file: {}, exist: {}'.format(model_file, os.path.exists(model_file)))\n",
    "\n",
    "    if os.path.exists(model_file):\n",
    "        print('loading {}...'.format(model_file))\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "    \n",
    "    return model, model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bnet = BengaliNet('se_resnext50_32x4d').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bnet(torch.randn((2, 1, 137, 236)).cuda()).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.111111"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(1/9, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "\n",
    "\n",
    "def calc_metrics(preds0, preds1, preds2, preds3, y):\n",
    "    assert len(y) == len(preds0) == len(preds1) == len(preds2) == len(preds3)\n",
    "\n",
    "    recall_grapheme = sklearn.metrics.recall_score(preds0, y[:, 0], average='macro')\n",
    "    recall_vowel = sklearn.metrics.recall_score(preds1, y[:, 1], average='macro')\n",
    "    recall_consonant = sklearn.metrics.recall_score(preds2, y[:, 2], average='macro')\n",
    "    recall_word = sklearn.metrics.recall_score(preds3, y[:, 3], average='macro')\n",
    "    \n",
    "    scores = [recall_grapheme, recall_vowel, recall_consonant]\n",
    "    final_recall_score = np.average(scores, weights=[2, 1, 1])\n",
    "    \n",
    "    metrics = {}\n",
    "    metrics['recall'] = round(final_recall_score, 6)\n",
    "    metrics['recall_grapheme'] = round(recall_grapheme, 6)\n",
    "    metrics['recall_vowel'] = round(recall_vowel, 6)\n",
    "    metrics['recall_consonant'] = round(recall_consonant, 6)\n",
    "    metrics['recall_word'] = round(recall_word, 6)\n",
    "    \n",
    "    metrics['acc_grapheme'] = round((preds0 == y[:, 0]).sum() / len(y), 6)\n",
    "    metrics['acc_vowel'] = round((preds1 == y[:, 1]).sum() / len(y), 6)\n",
    "    metrics['acc_consonant'] = round((preds2 == y[:, 2]).sum() / len(y), 6)\n",
    "    metrics['acc_word'] = round((preds3 == y[:, 3]).sum() / len(y), 6)    \n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(outputs, y_true):\n",
    "    # outputs: (N, 182)\n",
    "    # y_true: (N, 3)\n",
    "    \n",
    "    outputs = torch.split(outputs, [168, 11, 7, 1295], dim=1)\n",
    "    loss0 = F.cross_entropy(outputs[0], y_true[:, 0], reduction='mean')\n",
    "    loss1 = F.cross_entropy(outputs[1], y_true[:, 1], reduction='mean')\n",
    "    loss2 = F.cross_entropy(outputs[2], y_true[:, 2], reduction='mean')\n",
    "    loss3 = F.cross_entropy(outputs[3], y_true[:, 3], reduction='mean')\n",
    "    \n",
    "    return loss0 + loss1 + loss2 + loss3 #, loss0.item(), loss1.item(), loss2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    loss0, loss1, loss2, loss3 = 0., 0., 0., 0.\n",
    "    preds0, preds1, preds2, preds3 = [], [], [], []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            y_true.append(y)\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            outputs, _, _ = model(x)\n",
    "            outputs = torch.split(outputs, [168, 11, 7, 1295], dim=1)\n",
    "            \n",
    "            preds0.append(torch.max(outputs[0], dim=1)[1])\n",
    "            preds1.append(torch.max(outputs[1], dim=1)[1])\n",
    "            preds2.append(torch.max(outputs[2], dim=1)[1])\n",
    "            preds3.append(torch.max(outputs[3], dim=1)[1])\n",
    "            loss0 += F.cross_entropy(outputs[0], y[:, 0], reduction='sum').item()\n",
    "            loss1 += F.cross_entropy(outputs[1], y[:, 1], reduction='sum').item()\n",
    "            loss2 += F.cross_entropy(outputs[2], y[:, 2], reduction='sum').item()\n",
    "            loss3 += F.cross_entropy(outputs[3], y[:, 3], reduction='sum').item()\n",
    "            \n",
    "            # for debug\n",
    "            #metrics = {}\n",
    "            #metrics['loss_grapheme'] =  F.cross_entropy(outputs[0], y[:, 0], reduction='mean').item()\n",
    "            #metrics['loss_vowel'] =  F.cross_entropy(outputs[1], y[:, 1], reduction='mean').item()\n",
    "            #metrics['loss_consonant'] =  F.cross_entropy(outputs[2], y[:, 2], reduction='mean').item()\n",
    "            #return metrics\n",
    "    \n",
    "    preds0 = torch.cat(preds0, 0).cpu().numpy()\n",
    "    preds1 = torch.cat(preds1, 0).cpu().numpy()\n",
    "    preds2 = torch.cat(preds2, 0).cpu().numpy()\n",
    "    preds3 = torch.cat(preds3, 0).cpu().numpy()\n",
    "    \n",
    "    y_true = torch.cat(y_true, 0).numpy()\n",
    "    \n",
    "    #print('y_true:', y_true.shape)\n",
    "    #print('preds0:', preds0.shape)\n",
    "    \n",
    "    metrics = calc_metrics(preds0, preds1, preds2, preds3, y_true)\n",
    "    metrics['loss_grapheme'] = round(loss0 / val_loader.num, 6)\n",
    "    metrics['loss_vowel'] = round(loss1 / val_loader.num, 6)\n",
    "    metrics['loss_consonant'] = round(loss2 / val_loader.num, 6)\n",
    "    metrics['loss_word'] = round(loss3 / val_loader.num, 6)\n",
    "    \n",
    "    return metrics\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lrs(optimizer):\n",
    "    lrs = []\n",
    "    for pgs in optimizer.state_dict()['param_groups']:\n",
    "        lrs.append(pgs['lr'])\n",
    "    lrs = ['{:.6f}'.format(x) for x in lrs]\n",
    "    return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_file):\n",
    "    parent_dir = os.path.dirname(model_file)\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        torch.save(model.module.state_dict(), model_file)\n",
    "    else:\n",
    "        torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(data, targets, alpha=1):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets = targets[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    data = data * lam + shuffled_data * (1 - lam)\n",
    "    targets = (targets, shuffled_targets, lam)\n",
    "\n",
    "    return data, targets\n",
    "\n",
    "\n",
    "def mixup_criterion(outputs, targets):\n",
    "    targets1, targets2, lam = targets\n",
    "    #criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    return lam * criterion(outputs, targets1) + (1 - lam) * criterion(outputs, targets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox_new(size, lam):\n",
    "    H = size[2]\n",
    "    W = size[3]\n",
    "\n",
    "    x_margin_rate = 0.2\n",
    "\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * (1-x_margin_rate*2) * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "    \n",
    "    min_x_center = np.int(W * x_margin_rate + cut_w / 2)\n",
    "    max_x_center = np.int(W * (1-x_margin_rate) - cut_w / 2)\n",
    "    #print(min_x_center, max_x_center, lam, cut_w)\n",
    "    min_y_center = cut_h // 2\n",
    "    max_y_center = H - cut_h // 2\n",
    "    if max_y_center == min_y_center:\n",
    "        max_y_center += 1\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(min_x_center, max_x_center)\n",
    "    cy = np.random.randint(min_y_center, max_y_center)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    #print(bbx1, bbx2, bby1, bby2)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2938356071852881"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from over9000.over9000 import Over9000\n",
    "from over9000.radam import RAdam\n",
    "from gridmask import GridMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvcore.solver import WarmupCyclicalLR\n",
    "def make_optimizer(model, base_lr=4e-4, weight_decay=0., weight_decay_bias=0., epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    Create optimizer with per-layer learning rate and weight decay.\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    for key, value in model.named_parameters():\n",
    "        if not value.requires_grad:\n",
    "            continue\n",
    "        lr = base_lr\n",
    "        params += [{\"params\": [value], \"lr\": lr, \"weight_decay\": weight_decay_bias if 'bias' in key else weight_decay}]\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(params, lr, eps=epsilon)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(args, model, train_loader, epoch, optimizer, lr_scheduler, grid):\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (img, orig_img, targets) in enumerate(train_loader):\n",
    "        img, orig_img, targets  = img.cuda(), orig_img.cuda(), targets.cuda()\n",
    "        batch_size = img.size(0)\n",
    "        r = np.random.rand()\n",
    "\n",
    "        if r < 0.3:\n",
    "            # generate mixed sample\n",
    "            lam = np.random.beta(args.beta, args.beta)\n",
    "            rand_index = torch.randperm(img.size()[0]).cuda()\n",
    "            target_a = targets\n",
    "            target_b = targets[rand_index]\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(img.size(), lam)\n",
    "            #img[:, :, bby1:bby2, bbx1:bbx2] = img[rand_index, :, bby1:bby2, bbx1:bbx2] #for new cutmix\n",
    "            img[:, :, bbx1:bbx2, bby1:bby2] = img[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            \n",
    "            # adjust lambda to exactly match pixel ratio\n",
    "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (img.size()[-1] * img.size()[-2]))\n",
    "            # compute output\n",
    "            outputs, outputs_aux1, outputs_aux2 = model(img)\n",
    "            loss_primary = criterion(outputs, target_a) * lam + criterion(outputs, target_b) * (1. - lam)\n",
    "            loss_aux1 = criterion(outputs_aux1, target_a) * lam + criterion(outputs_aux1, target_b) * (1. - lam)\n",
    "            loss_aux2 = criterion(outputs_aux2, target_a) * lam + criterion(outputs_aux2, target_b) * (1. - lam)\n",
    "            loss = loss_primary + (loss_aux1 + loss_aux2)*0.8\n",
    "        elif r > 0.7:\n",
    "            img = grid(img)\n",
    "            outputs, outputs_aux1, outputs_aux2 = model(img)\n",
    "            loss_primary = criterion(outputs, targets)\n",
    "            loss_aux1 = criterion(outputs_aux1, targets)\n",
    "            loss_aux2 = criterion(outputs_aux2, targets)\n",
    "            loss = loss_primary + (loss_aux1 + loss_aux2)*0.8\n",
    "        else:\n",
    "            orig_img, targets = mixup(orig_img, targets)\n",
    "            outputs, outputs_aux1, outputs_aux2 = model(orig_img)\n",
    "            loss_primary = mixup_criterion(outputs, targets)\n",
    "            loss_aux1 = mixup_criterion(outputs_aux1, targets)\n",
    "            loss_aux2 = mixup_criterion(outputs_aux2, targets)\n",
    "            loss = loss_primary + (loss_aux1 + loss_aux2)*0.8\n",
    "            #loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        #loss.backward()\n",
    "        lr_scheduler(optimizer, batch_idx, epoch)\n",
    "        optimizer.step()            \n",
    "        \n",
    "        current_lr = get_lrs(optimizer)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        print('\\r {:4d} | {:.6f} | {:06d}/{} | {:.4f} | {:.4f} |'.format(\n",
    "            epoch, float(current_lr[0]), batch_size*(batch_idx+1), train_loader.num, \n",
    "            loss.item(), train_loss/(batch_idx+1)), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metrics = 0.\n",
    "\n",
    "def validate_and_save(model, model_file, val_loader, save=False):\n",
    "    global best_metrics\n",
    "    best_key = 'recall'\n",
    "    val_metrics = validate(model, val_loader)\n",
    "    print('\\nval:', val_metrics)\n",
    "    \n",
    "    if val_metrics[best_key] > best_metrics:\n",
    "        best_metrics = val_metrics[best_key]\n",
    "        if save:\n",
    "            save_model(model, model_file)\n",
    "            print('###>>>>> saved', model_file)\n",
    "    model.train()\n",
    "    \n",
    "\n",
    "def train(args):\n",
    "    model, model_file = create_model(cfg)\n",
    "    model = model.cuda()\n",
    "\n",
    "    swa_model, _ = create_model(cfg)\n",
    "    swa_model = swa_model.cuda()\n",
    "    swa_model_file = model_file + '_swa'\n",
    "\n",
    "    optimizer = make_optimizer(model)\n",
    "    lr_scheduler = WarmupCyclicalLR(\n",
    "        \"cos\", args.base_lr, args.num_epochs, iters_per_epoch=len(train_loader), warmup_epochs=args.warmup_epochs)\n",
    "    \n",
    "    [model, swa_model], optimizer = amp.initialize([model, swa_model], optimizer, opt_level=\"O1\",verbosity=0)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "        swa_model = nn.DataParallel(swa_model)\n",
    "    \n",
    "    validate_and_save(model, model_file, val_loader, save=False)\n",
    "    \n",
    "    for cycle in range(1, args.num_cycles+1):\n",
    "        print('CYCLE:', cycle)\n",
    "        grid = GridMask(64, 128, rotate=15, ratio=0.6, mode=1, prob=1.)\n",
    "\n",
    "        for epoch in range(args.num_epochs):\n",
    "            grid.set_prob(epoch, args.st_epochs)\n",
    "            train_epoch(args, model, train_loader, epoch, optimizer, lr_scheduler, grid)\n",
    "            validate_and_save(model, model_file, val_loader, save=True)\n",
    "            \n",
    "            if (epoch+1) == args.swa_start:\n",
    "                copy_model(swa_model, model)\n",
    "                swa_n = 0\n",
    "            if (epoch+1) >= args.swa_start and (epoch+1) % args.swa_freq == 0:\n",
    "                print('SWA>>>:')\n",
    "                moving_average(swa_model, model, 1.0 / (swa_n + 1))\n",
    "                swa_n += 1\n",
    "                bn_update(train_loader, swa_model)\n",
    "                validate_and_save(swa_model, swa_model_file + '_' + str(cycle), val_loader, save=True)\n",
    "\n",
    "        # reset scheduler at each cycle\n",
    "        lr_scheduler = WarmupCyclicalLR(\n",
    "            \"cos\", args.base_lr, args.num_epochs, iters_per_epoch=len(train_loader), warmup_epochs=args.warmup_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Namespace()\n",
    "cfg.MODEL_NAME = 'tf_efficientnet_b4'\n",
    "cfg.PRETRAINED = True\n",
    "cfg.IN_CHANNELS = 1\n",
    "cfg.POOL_TYPE = 'avg'\n",
    "cfg.CLS_HEAD = 'linear'\n",
    "cfg.MODEL_ACTIVATION = 'swish'\n",
    "cfg.DROP_CONNECT = 0.2\n",
    "cfg.DROPOUT= 0.\n",
    "cfg.NUM_WORD_CLASSES = 1295\n",
    "cfg.NUM_GRAPHEME_CLASSES = 168\n",
    "cfg.NUM_VOWEL_CLASSES = 11\n",
    "cfg.NUM_CONSONANT_CLASSES = 7\n",
    "cfg.CKP_NAME = 'model4_eb4_fold1.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model, model_file = create_model(cfg)\n",
    "#model(torch.randn(2,1,137,236))[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "\n",
    "args.base_lr = 3e-4\n",
    "args.num_epochs = 100\n",
    "args.warmup_epochs = 5\n",
    "args.num_cycles = 100\n",
    "args.batch_size = 640\n",
    "args.val_batch_size = 1024\n",
    "args.st_epochs = 5\n",
    "\n",
    "args.swa_start = 20\n",
    "args.swa_freq = 4\n",
    "\n",
    "args.beta = 1.0\n",
    "args.cutmix_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200840, 6)\n",
      "(200840, 32332)\n",
      "(160635, 6) (40205, 6)\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_train_val_loaders(batch_size=args.batch_size, val_batch_size=args.val_batch_size, ifold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model file: ./model4-ckps/tf_efficientnet_b4/model4_eb4_fold1.pth, exist: True\n",
      "loading ./model4-ckps/tf_efficientnet_b4/model4_eb4_fold1.pth...\n",
      "model file: ./model4-ckps/tf_efficientnet_b4/model4_eb4_fold1.pth, exist: True\n",
      "loading ./model4-ckps/tf_efficientnet_b4/model4_eb4_fold1.pth...\n",
      "\n",
      "val: {'recall': 0.998144, 'recall_grapheme': 0.997615, 'recall_vowel': 0.998435, 'recall_consonant': 0.99891, 'recall_word': 0.996674, 'acc_grapheme': 0.997289, 'acc_vowel': 0.998632, 'acc_consonant': 0.999204, 'acc_word': 0.996642, 'loss_grapheme': 0.012393, 'loss_vowel': 0.006921, 'loss_consonant': 0.004288, 'loss_word': 0.015194}\n",
      "CYCLE: 1\n",
      "    0 | 0.000060 | 160000/160635 | 0.2001 | 5.8343 ||\n",
      "val: {'recall': 0.997523, 'recall_grapheme': 0.996929, 'recall_vowel': 0.998144, 'recall_consonant': 0.998089, 'recall_word': 0.996077, 'acc_grapheme': 0.996468, 'acc_vowel': 0.998358, 'acc_consonant': 0.999055, 'acc_word': 0.996045, 'loss_grapheme': 0.028587, 'loss_vowel': 0.021784, 'loss_consonant': 0.016461, 'loss_word': 0.025845}\n",
      "    1 | 0.000078 | 049920/160635 | 0.2074 | 5.4063 ||"
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CYCLE: 1\n",
      "{'recall': 0.99719, 'recall_grapheme': 0.996382, 'recall_vowel': 0.997862, 'recall_consonant': 0.998134, 'recall_word': 0.99567, 'acc_grapheme': 0.995871, 'acc_vowel': 0.998085, 'acc_consonant': 0.99898, 'acc_word': 0.995672, 'loss_grapheme': 0.022536, 'loss_vowel': 0.014581, 'loss_consonant': 0.010935, 'loss_word': 0.021575}\n",
      "    0 | 0.000060 | 160000/160635 | 4.1378 | 5.8142 ||\n",
      "val: {'recall': 0.996434, 'recall_grapheme': 0.995432, 'recall_vowel': 0.997529, 'recall_consonant': 0.997342, 'recall_word': 0.995104, 'acc_grapheme': 0.995001, 'acc_vowel': 0.997513, 'acc_consonant': 0.998483, 'acc_word': 0.99515, 'loss_grapheme': 0.065838, 'loss_vowel': 0.050984, 'loss_consonant': 0.03582, 'loss_word': 0.04643}\n",
      "    1 | 0.000120 | 160000/160635 | 0.3627 | 5.8495 ||\n",
      "val: {'recall': 0.996762, 'recall_grapheme': 0.995615, 'recall_vowel': 0.99794, 'recall_consonant': 0.997876, 'recall_word': 0.995165, 'acc_grapheme': 0.995075, 'acc_vowel': 0.997662, 'acc_consonant': 0.998632, 'acc_word': 0.99515, 'loss_grapheme': 0.068615, 'loss_vowel': 0.052479, 'loss_consonant': 0.037917, 'loss_word': 0.051871}\n",
      "    2 | 0.000179 | 160000/160635 | 12.6780 | 6.4328 ||\n",
      "val: {'recall': 0.996291, 'recall_grapheme': 0.995428, 'recall_vowel': 0.997126, 'recall_consonant': 0.99718, 'recall_word': 0.994181, 'acc_grapheme': 0.994528, 'acc_vowel': 0.997264, 'acc_consonant': 0.998309, 'acc_word': 0.99423, 'loss_grapheme': 0.078953, 'loss_vowel': 0.061625, 'loss_consonant': 0.044495, 'loss_word': 0.054236}\n",
      "    3 | 0.000238 | 160000/160635 | 5.4893 | 6.1681 |||\n",
      "val: {'recall': 0.996727, 'recall_grapheme': 0.995794, 'recall_vowel': 0.997522, 'recall_consonant': 0.997799, 'recall_word': 0.99486, 'acc_grapheme': 0.995324, 'acc_vowel': 0.997488, 'acc_consonant': 0.998582, 'acc_word': 0.994876, 'loss_grapheme': 0.03436, 'loss_vowel': 0.023702, 'loss_consonant': 0.01771, 'loss_word': 0.030244}\n",
      "    4 | 0.000297 | 160000/160635 | 4.1726 | 6.1468 ||\n",
      "val: {'recall': 0.995683, 'recall_grapheme': 0.994222, 'recall_vowel': 0.997477, 'recall_consonant': 0.996812, 'recall_word': 0.993625, 'acc_grapheme': 0.993757, 'acc_vowel': 0.997388, 'acc_consonant': 0.998035, 'acc_word': 0.993483, 'loss_grapheme': 0.101916, 'loss_vowel': 0.081382, 'loss_consonant': 0.055207, 'loss_word': 0.071448}\n",
      "    5 | 0.000296 | 160000/160635 | 5.1500 | 5.9120 ||\n",
      "val: {'recall': 0.996127, 'recall_grapheme': 0.995187, 'recall_vowel': 0.996454, 'recall_consonant': 0.99768, 'recall_word': 0.994049, 'acc_grapheme': 0.994354, 'acc_vowel': 0.997214, 'acc_consonant': 0.998458, 'acc_word': 0.994006, 'loss_grapheme': 0.071413, 'loss_vowel': 0.057741, 'loss_consonant': 0.040991, 'loss_word': 0.05305}\n",
      "    6 | 0.000294 | 160000/160635 | 7.3798 | 5.9245 |||\n",
      "val: {'recall': 0.995594, 'recall_grapheme': 0.995189, 'recall_vowel': 0.996952, 'recall_consonant': 0.995046, 'recall_word': 0.9935, 'acc_grapheme': 0.994006, 'acc_vowel': 0.996791, 'acc_consonant': 0.997861, 'acc_word': 0.993409, 'loss_grapheme': 0.079213, 'loss_vowel': 0.054197, 'loss_consonant': 0.039914, 'loss_word': 0.059615}\n",
      "    7 | 0.000293 | 160000/160635 | 5.9793 | 6.8202 ||\n",
      "val: {'recall': 0.996111, 'recall_grapheme': 0.995002, 'recall_vowel': 0.996966, 'recall_consonant': 0.997474, 'recall_word': 0.993917, 'acc_grapheme': 0.993981, 'acc_vowel': 0.997214, 'acc_consonant': 0.998135, 'acc_word': 0.993906, 'loss_grapheme': 0.073066, 'loss_vowel': 0.060257, 'loss_consonant': 0.04261, 'loss_word': 0.049414}\n",
      "    8 | 0.000291 | 160000/160635 | 16.1451 | 6.6164 |\n",
      "val: {'recall': 0.995926, 'recall_grapheme': 0.995163, 'recall_vowel': 0.996679, 'recall_consonant': 0.996698, 'recall_word': 0.993966, 'acc_grapheme': 0.994404, 'acc_vowel': 0.99704, 'acc_consonant': 0.99796, 'acc_word': 0.993906, 'loss_grapheme': 0.067712, 'loss_vowel': 0.052508, 'loss_consonant': 0.039555, 'loss_word': 0.047533}\n",
      "    9 | 0.000289 | 160000/160635 | 0.6245 | 6.3163 ||\n",
      "val: {'recall': 0.99686, 'recall_grapheme': 0.996189, 'recall_vowel': 0.997595, 'recall_consonant': 0.997467, 'recall_word': 0.994491, 'acc_grapheme': 0.995498, 'acc_vowel': 0.997637, 'acc_consonant': 0.998756, 'acc_word': 0.994528, 'loss_grapheme': 0.023019, 'loss_vowel': 0.014783, 'loss_consonant': 0.010512, 'loss_word': 0.022926}\n",
      "   10 | 0.000286 | 160000/160635 | 12.0595 | 6.7021 |\n",
      "val: {'recall': 0.996349, 'recall_grapheme': 0.995122, 'recall_vowel': 0.997463, 'recall_consonant': 0.997688, 'recall_word': 0.99421, 'acc_grapheme': 0.994702, 'acc_vowel': 0.997587, 'acc_consonant': 0.998433, 'acc_word': 0.994155, 'loss_grapheme': 0.051687, 'loss_vowel': 0.040126, 'loss_consonant': 0.029531, 'loss_word': 0.039832}\n",
      "   11 | 0.000284 | 160000/160635 | 12.4178 | 6.1406 ||\n",
      "val: {'recall': 0.996381, 'recall_grapheme': 0.995066, 'recall_vowel': 0.997248, 'recall_consonant': 0.998142, 'recall_word': 0.99375, 'acc_grapheme': 0.994031, 'acc_vowel': 0.99704, 'acc_consonant': 0.998334, 'acc_word': 0.993682, 'loss_grapheme': 0.071971, 'loss_vowel': 0.058545, 'loss_consonant': 0.042429, 'loss_word': 0.050622}\n",
      "   12 | 0.000281 | 160000/160635 | 13.7050 | 5.8457 ||\n",
      "val: {'recall': 0.996087, 'recall_grapheme': 0.9949, 'recall_vowel': 0.997179, 'recall_consonant': 0.997369, 'recall_word': 0.993778, 'acc_grapheme': 0.99413, 'acc_vowel': 0.997314, 'acc_consonant': 0.998159, 'acc_word': 0.993832, 'loss_grapheme': 0.060864, 'loss_vowel': 0.050535, 'loss_consonant': 0.036164, 'loss_word': 0.045137}\n",
      "   13 | 0.000278 | 160000/160635 | 10.1145 | 5.9332 |\n",
      "val: {'recall': 0.995843, 'recall_grapheme': 0.995185, 'recall_vowel': 0.997074, 'recall_consonant': 0.995929, 'recall_word': 0.993765, 'acc_grapheme': 0.994354, 'acc_vowel': 0.997214, 'acc_consonant': 0.99811, 'acc_word': 0.993707, 'loss_grapheme': 0.06957, 'loss_vowel': 0.052289, 'loss_consonant': 0.038597, 'loss_word': 0.055137}\n",
      "   14 | 0.000275 | 160000/160635 | 7.7094 | 6.8233 ||\n",
      "val: {'recall': 0.996184, 'recall_grapheme': 0.995029, 'recall_vowel': 0.997137, 'recall_consonant': 0.997542, 'recall_word': 0.993759, 'acc_grapheme': 0.994404, 'acc_vowel': 0.997214, 'acc_consonant': 0.998259, 'acc_word': 0.993732, 'loss_grapheme': 0.076966, 'loss_vowel': 0.059087, 'loss_consonant': 0.043112, 'loss_word': 0.056045}\n",
      "   15 | 0.000271 | 160000/160635 | 0.8464 | 5.8420 |||\n",
      "val: {'recall': 0.996154, 'recall_grapheme': 0.995277, 'recall_vowel': 0.997801, 'recall_consonant': 0.996262, 'recall_word': 0.99458, 'acc_grapheme': 0.994851, 'acc_vowel': 0.997761, 'acc_consonant': 0.998483, 'acc_word': 0.994503, 'loss_grapheme': 0.042369, 'loss_vowel': 0.032321, 'loss_consonant': 0.02369, 'loss_word': 0.033891}\n",
      "   16 | 0.000268 | 160000/160635 | 2.2505 | 6.7477 |||\n",
      "val: {'recall': 0.996396, 'recall_grapheme': 0.995297, 'recall_vowel': 0.997758, 'recall_consonant': 0.99723, 'recall_word': 0.99435, 'acc_grapheme': 0.994901, 'acc_vowel': 0.997488, 'acc_consonant': 0.998458, 'acc_word': 0.994329, 'loss_grapheme': 0.059657, 'loss_vowel': 0.050021, 'loss_consonant': 0.035254, 'loss_word': 0.048409}\n",
      "   17 | 0.000264 | 160000/160635 | 4.5768 | 6.0560 ||\n",
      "val: {'recall': 0.996249, 'recall_grapheme': 0.995143, 'recall_vowel': 0.997385, 'recall_consonant': 0.997326, 'recall_word': 0.994579, 'acc_grapheme': 0.994503, 'acc_vowel': 0.997488, 'acc_consonant': 0.998458, 'acc_word': 0.994528, 'loss_grapheme': 0.060702, 'loss_vowel': 0.04848, 'loss_consonant': 0.034092, 'loss_word': 0.048591}\n",
      "   18 | 0.000260 | 160000/160635 | 11.9578 | 6.1821 |\n",
      "val: {'recall': 0.996524, 'recall_grapheme': 0.995095, 'recall_vowel': 0.997911, 'recall_consonant': 0.997995, 'recall_word': 0.994593, 'acc_grapheme': 0.994802, 'acc_vowel': 0.997687, 'acc_consonant': 0.998682, 'acc_word': 0.994603, 'loss_grapheme': 0.053249, 'loss_vowel': 0.043941, 'loss_consonant': 0.033143, 'loss_word': 0.043079}\n",
      "   19 | 0.000256 | 160000/160635 | 6.5628 | 5.7668 |||\n",
      "val: {'recall': 0.996662, 'recall_grapheme': 0.995538, 'recall_vowel': 0.998026, 'recall_consonant': 0.997548, 'recall_word': 0.99482, 'acc_grapheme': 0.995224, 'acc_vowel': 0.997836, 'acc_consonant': 0.998756, 'acc_word': 0.994777, 'loss_grapheme': 0.040758, 'loss_vowel': 0.029388, 'loss_consonant': 0.021714, 'loss_word': 0.03611}\n",
      "   20 | 0.000252 | 160000/160635 | 6.0847 | 6.0127 |||\n",
      "val: {'recall': 0.99599, 'recall_grapheme': 0.995052, 'recall_vowel': 0.997777, 'recall_consonant': 0.996078, 'recall_word': 0.993383, 'acc_grapheme': 0.993881, 'acc_vowel': 0.997314, 'acc_consonant': 0.998135, 'acc_word': 0.99326, 'loss_grapheme': 0.077485, 'loss_vowel': 0.059587, 'loss_consonant': 0.04078, 'loss_word': 0.060438}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   21 | 0.000247 | 160000/160635 | 12.1958 | 6.2732 ||\n",
      "val: {'recall': 0.996551, 'recall_grapheme': 0.995729, 'recall_vowel': 0.997595, 'recall_consonant': 0.997152, 'recall_word': 0.994702, 'acc_grapheme': 0.994876, 'acc_vowel': 0.997562, 'acc_consonant': 0.998433, 'acc_word': 0.994702, 'loss_grapheme': 0.059388, 'loss_vowel': 0.043675, 'loss_consonant': 0.03106, 'loss_word': 0.04741}\n",
      "   22 | 0.000243 | 160000/160635 | 0.6093 | 6.0554 ||\n",
      "val: {'recall': 0.996553, 'recall_grapheme': 0.995726, 'recall_vowel': 0.998072, 'recall_consonant': 0.996688, 'recall_word': 0.994773, 'acc_grapheme': 0.9951, 'acc_vowel': 0.997886, 'acc_consonant': 0.998582, 'acc_word': 0.994727, 'loss_grapheme': 0.052127, 'loss_vowel': 0.039893, 'loss_consonant': 0.030727, 'loss_word': 0.039822}\n",
      "   23 | 0.000238 | 160000/160635 | 0.5189 | 5.5368 ||\n",
      "val: {'recall': 0.996704, 'recall_grapheme': 0.995564, 'recall_vowel': 0.99786, 'recall_consonant': 0.997829, 'recall_word': 0.995001, 'acc_grapheme': 0.995274, 'acc_vowel': 0.997662, 'acc_consonant': 0.998806, 'acc_word': 0.995001, 'loss_grapheme': 0.033016, 'loss_vowel': 0.024017, 'loss_consonant': 0.018244, 'loss_word': 0.0299}\n",
      "   24 | 0.000233 | 160000/160635 | 6.3384 | 6.7732 ||\n",
      "val: {'recall': 0.996365, 'recall_grapheme': 0.995573, 'recall_vowel': 0.997179, 'recall_consonant': 0.997135, 'recall_word': 0.994345, 'acc_grapheme': 0.994851, 'acc_vowel': 0.997364, 'acc_consonant': 0.998408, 'acc_word': 0.994304, 'loss_grapheme': 0.08583, 'loss_vowel': 0.062847, 'loss_consonant': 0.045013, 'loss_word': 0.067252}\n",
      "   25 | 0.000228 | 160000/160635 | 0.6244 | 6.2984 |||\n",
      "val: {'recall': 0.996687, 'recall_grapheme': 0.995337, 'recall_vowel': 0.997655, 'recall_consonant': 0.998421, 'recall_word': 0.99448, 'acc_grapheme': 0.994727, 'acc_vowel': 0.997562, 'acc_consonant': 0.998682, 'acc_word': 0.994453, 'loss_grapheme': 0.052604, 'loss_vowel': 0.043618, 'loss_consonant': 0.031815, 'loss_word': 0.040514}\n",
      "   26 | 0.000223 | 160000/160635 | 4.9471 | 6.5445 ||\n",
      "val: {'recall': 0.996934, 'recall_grapheme': 0.996397, 'recall_vowel': 0.997767, 'recall_consonant': 0.997176, 'recall_word': 0.995164, 'acc_grapheme': 0.995672, 'acc_vowel': 0.997836, 'acc_consonant': 0.998607, 'acc_word': 0.9952, 'loss_grapheme': 0.058771, 'loss_vowel': 0.045854, 'loss_consonant': 0.032388, 'loss_word': 0.048655}\n",
      "   27 | 0.000218 | 160000/160635 | 0.4998 | 6.0781 |||\n",
      "val: {'recall': 0.996554, 'recall_grapheme': 0.995897, 'recall_vowel': 0.99777, 'recall_consonant': 0.996652, 'recall_word': 0.994767, 'acc_grapheme': 0.994976, 'acc_vowel': 0.997836, 'acc_consonant': 0.998533, 'acc_word': 0.994752, 'loss_grapheme': 0.048422, 'loss_vowel': 0.035865, 'loss_consonant': 0.026668, 'loss_word': 0.041837}\n",
      "   28 | 0.000213 | 160000/160635 | 1.5324 | 5.8803 ||\n",
      "val: {'recall': 0.997142, 'recall_grapheme': 0.996572, 'recall_vowel': 0.997759, 'recall_consonant': 0.997666, 'recall_word': 0.995493, 'acc_grapheme': 0.995797, 'acc_vowel': 0.998035, 'acc_consonant': 0.998881, 'acc_word': 0.995448, 'loss_grapheme': 0.029405, 'loss_vowel': 0.022777, 'loss_consonant': 0.016411, 'loss_word': 0.025405}\n",
      "   29 | 0.000207 | 160000/160635 | 1.4747 | 6.0287 ||\n",
      "val: {'recall': 0.997038, 'recall_grapheme': 0.996225, 'recall_vowel': 0.997707, 'recall_consonant': 0.997996, 'recall_word': 0.99498, 'acc_grapheme': 0.995399, 'acc_vowel': 0.997911, 'acc_consonant': 0.998881, 'acc_word': 0.994951, 'loss_grapheme': 0.029012, 'loss_vowel': 0.021252, 'loss_consonant': 0.015991, 'loss_word': 0.026041}\n",
      "   30 | 0.000202 | 160000/160635 | 9.1464 | 5.7275 ||\n",
      "val: {'recall': 0.996685, 'recall_grapheme': 0.99591, 'recall_vowel': 0.997629, 'recall_consonant': 0.99729, 'recall_word': 0.994779, 'acc_grapheme': 0.995025, 'acc_vowel': 0.997612, 'acc_consonant': 0.998483, 'acc_word': 0.994727, 'loss_grapheme': 0.063179, 'loss_vowel': 0.050879, 'loss_consonant': 0.035991, 'loss_word': 0.048536}\n",
      "   31 | 0.000196 | 160000/160635 | 6.2421 | 6.1791 ||\n",
      "val: {'recall': 0.996277, 'recall_grapheme': 0.995815, 'recall_vowel': 0.997395, 'recall_consonant': 0.996083, 'recall_word': 0.994477, 'acc_grapheme': 0.994951, 'acc_vowel': 0.997637, 'acc_consonant': 0.998408, 'acc_word': 0.994429, 'loss_grapheme': 0.070842, 'loss_vowel': 0.056011, 'loss_consonant': 0.040842, 'loss_word': 0.053962}\n",
      "   32 | 0.000191 | 160000/160635 | 0.4890 | 6.0289 ||\n",
      "val: {'recall': 0.99686, 'recall_grapheme': 0.99607, 'recall_vowel': 0.997588, 'recall_consonant': 0.997713, 'recall_word': 0.995136, 'acc_grapheme': 0.995399, 'acc_vowel': 0.997737, 'acc_consonant': 0.998955, 'acc_word': 0.995125, 'loss_grapheme': 0.029328, 'loss_vowel': 0.02185, 'loss_consonant': 0.016426, 'loss_word': 0.025872}\n",
      "   33 | 0.000185 | 160000/160635 | 13.4921 | 6.1073 ||\n",
      "val: {'recall': 0.996775, 'recall_grapheme': 0.995668, 'recall_vowel': 0.997647, 'recall_consonant': 0.998117, 'recall_word': 0.994984, 'acc_grapheme': 0.994951, 'acc_vowel': 0.997786, 'acc_consonant': 0.998906, 'acc_word': 0.995025, 'loss_grapheme': 0.046339, 'loss_vowel': 0.038612, 'loss_consonant': 0.028202, 'loss_word': 0.036573}\n",
      "   34 | 0.000179 | 160000/160635 | 11.8400 | 5.6341 |\n",
      "val: {'recall': 0.996726, 'recall_grapheme': 0.99582, 'recall_vowel': 0.997931, 'recall_consonant': 0.997334, 'recall_word': 0.995164, 'acc_grapheme': 0.995324, 'acc_vowel': 0.99811, 'acc_consonant': 0.998856, 'acc_word': 0.995075, 'loss_grapheme': 0.038505, 'loss_vowel': 0.028559, 'loss_consonant': 0.021915, 'loss_word': 0.031804}\n",
      "   35 | 0.000173 | 160000/160635 | 7.5402 | 6.5280 ||\n",
      "val: {'recall': 0.996108, 'recall_grapheme': 0.995432, 'recall_vowel': 0.997202, 'recall_consonant': 0.996369, 'recall_word': 0.994329, 'acc_grapheme': 0.994752, 'acc_vowel': 0.997289, 'acc_consonant': 0.998408, 'acc_word': 0.994329, 'loss_grapheme': 0.087614, 'loss_vowel': 0.07291, 'loss_consonant': 0.050614, 'loss_word': 0.057547}\n",
      "   36 | 0.000168 | 160000/160635 | 4.8006 | 6.4140 |||\n",
      "val: {'recall': 0.996444, 'recall_grapheme': 0.995543, 'recall_vowel': 0.997425, 'recall_consonant': 0.997263, 'recall_word': 0.993974, 'acc_grapheme': 0.994379, 'acc_vowel': 0.997364, 'acc_consonant': 0.998309, 'acc_word': 0.993956, 'loss_grapheme': 0.129591, 'loss_vowel': 0.087723, 'loss_consonant': 0.06151, 'loss_word': 0.093513}\n",
      "   37 | 0.000162 | 160000/160635 | 5.3839 | 6.2392 ||\n",
      "val: {'recall': 0.996825, 'recall_grapheme': 0.996141, 'recall_vowel': 0.997611, 'recall_consonant': 0.997408, 'recall_word': 0.99481, 'acc_grapheme': 0.995224, 'acc_vowel': 0.997662, 'acc_consonant': 0.998632, 'acc_word': 0.994777, 'loss_grapheme': 0.059446, 'loss_vowel': 0.04774, 'loss_consonant': 0.032931, 'loss_word': 0.044362}\n",
      "   38 | 0.000156 | 160000/160635 | 7.1480 | 5.4922 ||\n",
      "val: {'recall': 0.997407, 'recall_grapheme': 0.996398, 'recall_vowel': 0.997952, 'recall_consonant': 0.998878, 'recall_word': 0.995616, 'acc_grapheme': 0.996045, 'acc_vowel': 0.998085, 'acc_consonant': 0.99908, 'acc_word': 0.995573, 'loss_grapheme': 0.021295, 'loss_vowel': 0.013863, 'loss_consonant': 0.009569, 'loss_word': 0.021172}\n",
      "###>>>>> saved\n",
      "   39 | 0.000150 | 160000/160635 | 9.1858 | 6.4696 ||\n",
      "val: {'recall': 0.996533, 'recall_grapheme': 0.995614, 'recall_vowel': 0.997498, 'recall_consonant': 0.997405, 'recall_word': 0.994794, 'acc_grapheme': 0.995025, 'acc_vowel': 0.997587, 'acc_consonant': 0.998756, 'acc_word': 0.994827, 'loss_grapheme': 0.066195, 'loss_vowel': 0.05919, 'loss_consonant': 0.042078, 'loss_word': 0.044415}\n",
      "   40 | 0.000144 | 160000/160635 | 5.3839 | 5.7097 ||\n",
      "val: {'recall': 0.9969, 'recall_grapheme': 0.996204, 'recall_vowel': 0.997632, 'recall_consonant': 0.997559, 'recall_word': 0.995227, 'acc_grapheme': 0.995573, 'acc_vowel': 0.997861, 'acc_consonant': 0.998881, 'acc_word': 0.995175, 'loss_grapheme': 0.031326, 'loss_vowel': 0.023541, 'loss_consonant': 0.018097, 'loss_word': 0.027935}\n",
      "   41 | 0.000138 | 160000/160635 | 0.6824 | 6.1819 |||\n",
      "val: {'recall': 0.997189, 'recall_grapheme': 0.996307, 'recall_vowel': 0.99777, 'recall_consonant': 0.998372, 'recall_word': 0.995459, 'acc_grapheme': 0.995896, 'acc_vowel': 0.99806, 'acc_consonant': 0.999154, 'acc_word': 0.995473, 'loss_grapheme': 0.030395, 'loss_vowel': 0.021946, 'loss_consonant': 0.015558, 'loss_word': 0.027988}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   42 | 0.000132 | 160000/160635 | 0.5902 | 5.4162 |||\n",
      "val: {'recall': 0.996919, 'recall_grapheme': 0.996034, 'recall_vowel': 0.997751, 'recall_consonant': 0.997856, 'recall_word': 0.99521, 'acc_grapheme': 0.995622, 'acc_vowel': 0.99801, 'acc_consonant': 0.998756, 'acc_word': 0.9952, 'loss_grapheme': 0.04251, 'loss_vowel': 0.03518, 'loss_consonant': 0.025374, 'loss_word': 0.035236}\n",
      "   43 | 0.000127 | 160000/160635 | 9.3936 | 6.0009 ||\n",
      "val: {'recall': 0.99703, 'recall_grapheme': 0.996459, 'recall_vowel': 0.99764, 'recall_consonant': 0.997563, 'recall_word': 0.995395, 'acc_grapheme': 0.995672, 'acc_vowel': 0.997712, 'acc_consonant': 0.998881, 'acc_word': 0.995324, 'loss_grapheme': 0.058771, 'loss_vowel': 0.04855, 'loss_consonant': 0.03469, 'loss_word': 0.046771}\n",
      "   44 | 0.000121 | 160000/160635 | 0.4705 | 5.8303 ||\n",
      "val: {'recall': 0.997126, 'recall_grapheme': 0.996633, 'recall_vowel': 0.997846, 'recall_consonant': 0.997394, 'recall_word': 0.995704, 'acc_grapheme': 0.99607, 'acc_vowel': 0.998085, 'acc_consonant': 0.998906, 'acc_word': 0.995672, 'loss_grapheme': 0.022656, 'loss_vowel': 0.017307, 'loss_consonant': 0.013301, 'loss_word': 0.021349}\n",
      "   45 | 0.000115 | 160000/160635 | 6.4059 | 5.5987 ||\n",
      "val: {'recall': 0.997048, 'recall_grapheme': 0.996439, 'recall_vowel': 0.997807, 'recall_consonant': 0.997506, 'recall_word': 0.995271, 'acc_grapheme': 0.995821, 'acc_vowel': 0.997985, 'acc_consonant': 0.998881, 'acc_word': 0.995274, 'loss_grapheme': 0.049339, 'loss_vowel': 0.038929, 'loss_consonant': 0.02884, 'loss_word': 0.042607}\n",
      "   46 | 0.000109 | 160000/160635 | 15.7340 | 5.6312 |\n",
      "val: {'recall': 0.996978, 'recall_grapheme': 0.996347, 'recall_vowel': 0.997695, 'recall_consonant': 0.997525, 'recall_word': 0.995089, 'acc_grapheme': 0.995797, 'acc_vowel': 0.997861, 'acc_consonant': 0.998856, 'acc_word': 0.995075, 'loss_grapheme': 0.031924, 'loss_vowel': 0.02481, 'loss_consonant': 0.018912, 'loss_word': 0.028122}\n",
      "   47 | 0.000104 | 160000/160635 | 8.0891 | 6.0352 ||\n",
      "val: {'recall': 0.996708, 'recall_grapheme': 0.995925, 'recall_vowel': 0.997395, 'recall_consonant': 0.997589, 'recall_word': 0.994757, 'acc_grapheme': 0.995125, 'acc_vowel': 0.997562, 'acc_consonant': 0.998906, 'acc_word': 0.994727, 'loss_grapheme': 0.046522, 'loss_vowel': 0.040195, 'loss_consonant': 0.029355, 'loss_word': 0.036308}\n",
      "   48 | 0.000098 | 160000/160635 | 0.1570 | 5.4808 ||\n",
      "val: {'recall': 0.996976, 'recall_grapheme': 0.996368, 'recall_vowel': 0.997694, 'recall_consonant': 0.997475, 'recall_word': 0.995355, 'acc_grapheme': 0.995797, 'acc_vowel': 0.997861, 'acc_consonant': 0.99893, 'acc_word': 0.995324, 'loss_grapheme': 0.034966, 'loss_vowel': 0.027717, 'loss_consonant': 0.019306, 'loss_word': 0.030398}\n",
      "   49 | 0.000093 | 160000/160635 | 4.5237 | 5.9589 ||\n",
      "val: {'recall': 0.996799, 'recall_grapheme': 0.996105, 'recall_vowel': 0.997641, 'recall_consonant': 0.997344, 'recall_word': 0.994769, 'acc_grapheme': 0.995324, 'acc_vowel': 0.997712, 'acc_consonant': 0.998756, 'acc_word': 0.994752, 'loss_grapheme': 0.042907, 'loss_vowel': 0.034576, 'loss_consonant': 0.026686, 'loss_word': 0.034774}\n",
      "   50 | 0.000087 | 160000/160635 | 0.4501 | 5.6400 |||\n",
      "val: {'recall': 0.99689, 'recall_grapheme': 0.996236, 'recall_vowel': 0.997582, 'recall_consonant': 0.997506, 'recall_word': 0.995278, 'acc_grapheme': 0.995797, 'acc_vowel': 0.997811, 'acc_consonant': 0.998881, 'acc_word': 0.9952, 'loss_grapheme': 0.025618, 'loss_vowel': 0.018604, 'loss_consonant': 0.013902, 'loss_word': 0.024148}\n",
      "   51 | 0.000082 | 160000/160635 | 0.4393 | 5.8429 ||\n",
      "val: {'recall': 0.997303, 'recall_grapheme': 0.996636, 'recall_vowel': 0.997689, 'recall_consonant': 0.99825, 'recall_word': 0.995778, 'acc_grapheme': 0.996244, 'acc_vowel': 0.99796, 'acc_consonant': 0.999105, 'acc_word': 0.995722, 'loss_grapheme': 0.018647, 'loss_vowel': 0.011874, 'loss_consonant': 0.008344, 'loss_word': 0.019335}\n",
      "   52 | 0.000077 | 160000/160635 | 12.2037 | 5.9288 ||\n",
      "val: {'recall': 0.997071, 'recall_grapheme': 0.996455, 'recall_vowel': 0.997773, 'recall_consonant': 0.997601, 'recall_word': 0.99552, 'acc_grapheme': 0.995871, 'acc_vowel': 0.997936, 'acc_consonant': 0.99893, 'acc_word': 0.995498, 'loss_grapheme': 0.029401, 'loss_vowel': 0.023964, 'loss_consonant': 0.018151, 'loss_word': 0.026115}\n",
      "   53 | 0.000072 | 160000/160635 | 13.8661 | 5.6169 |\n",
      "val: {'recall': 0.996814, 'recall_grapheme': 0.996076, 'recall_vowel': 0.997624, 'recall_consonant': 0.99748, 'recall_word': 0.994866, 'acc_grapheme': 0.995573, 'acc_vowel': 0.997836, 'acc_consonant': 0.998806, 'acc_word': 0.994827, 'loss_grapheme': 0.042367, 'loss_vowel': 0.036027, 'loss_consonant': 0.026739, 'loss_word': 0.035433}\n",
      "   54 | 0.000067 | 160000/160635 | 0.3589 | 5.6760 |||\n",
      "val: {'recall': 0.997227, 'recall_grapheme': 0.996748, 'recall_vowel': 0.997757, 'recall_consonant': 0.997656, 'recall_word': 0.995246, 'acc_grapheme': 0.995871, 'acc_vowel': 0.998085, 'acc_consonant': 0.998955, 'acc_word': 0.995249, 'loss_grapheme': 0.036652, 'loss_vowel': 0.029024, 'loss_consonant': 0.021468, 'loss_word': 0.030673}\n",
      "   55 | 0.000062 | 160000/160635 | 6.6554 | 6.1315 |||\n",
      "val: {'recall': 0.996822, 'recall_grapheme': 0.996171, 'recall_vowel': 0.997575, 'recall_consonant': 0.997373, 'recall_word': 0.995202, 'acc_grapheme': 0.995573, 'acc_vowel': 0.997662, 'acc_consonant': 0.998781, 'acc_word': 0.9952, 'loss_grapheme': 0.050154, 'loss_vowel': 0.040085, 'loss_consonant': 0.028835, 'loss_word': 0.038959}\n",
      "   56 | 0.000057 | 160000/160635 | 0.4540 | 5.5451 |||\n",
      "val: {'recall': 0.997415, 'recall_grapheme': 0.996906, 'recall_vowel': 0.997677, 'recall_consonant': 0.99817, 'recall_word': 0.995568, 'acc_grapheme': 0.99617, 'acc_vowel': 0.99806, 'acc_consonant': 0.99903, 'acc_word': 0.995548, 'loss_grapheme': 0.020373, 'loss_vowel': 0.013469, 'loss_consonant': 0.010093, 'loss_word': 0.020369}\n",
      "###>>>>> saved\n",
      "   57 | 0.000053 | 160000/160635 | 14.5990 | 5.5895 |\n",
      "val: {'recall': 0.997, 'recall_grapheme': 0.99641, 'recall_vowel': 0.997664, 'recall_consonant': 0.997517, 'recall_word': 0.995116, 'acc_grapheme': 0.995871, 'acc_vowel': 0.997886, 'acc_consonant': 0.998831, 'acc_word': 0.9951, 'loss_grapheme': 0.040524, 'loss_vowel': 0.033831, 'loss_consonant': 0.025122, 'loss_word': 0.034107}\n",
      "   58 | 0.000048 | 160000/160635 | 4.0816 | 5.7815 ||\n",
      "val: {'recall': 0.997001, 'recall_grapheme': 0.996409, 'recall_vowel': 0.997567, 'recall_consonant': 0.997616, 'recall_word': 0.995502, 'acc_grapheme': 0.99602, 'acc_vowel': 0.997911, 'acc_consonant': 0.998906, 'acc_word': 0.995523, 'loss_grapheme': 0.022013, 'loss_vowel': 0.014784, 'loss_consonant': 0.011312, 'loss_word': 0.021789}\n",
      "   59 | 0.000044 | 160000/160635 | 0.3281 | 6.2512 |||\n",
      "val: {'recall': 0.996914, 'recall_grapheme': 0.996391, 'recall_vowel': 0.99739, 'recall_consonant': 0.997485, 'recall_word': 0.995662, 'acc_grapheme': 0.996045, 'acc_vowel': 0.997886, 'acc_consonant': 0.998806, 'acc_word': 0.995672, 'loss_grapheme': 0.029276, 'loss_vowel': 0.022617, 'loss_consonant': 0.017287, 'loss_word': 0.025301}\n",
      "   60 | 0.000040 | 160000/160635 | 0.9163 | 5.9662 |||\n",
      "val: {'recall': 0.997356, 'recall_grapheme': 0.996684, 'recall_vowel': 0.997871, 'recall_consonant': 0.998185, 'recall_word': 0.996009, 'acc_grapheme': 0.996369, 'acc_vowel': 0.998159, 'acc_consonant': 0.99908, 'acc_word': 0.995996, 'loss_grapheme': 0.021521, 'loss_vowel': 0.01503, 'loss_consonant': 0.011235, 'loss_word': 0.020392}\n",
      "   61 | 0.000036 | 160000/160635 | 8.8201 | 5.8813 |||\n",
      "val: {'recall': 0.996986, 'recall_grapheme': 0.996451, 'recall_vowel': 0.997488, 'recall_consonant': 0.997554, 'recall_word': 0.995263, 'acc_grapheme': 0.995821, 'acc_vowel': 0.997761, 'acc_consonant': 0.998781, 'acc_word': 0.995249, 'loss_grapheme': 0.045153, 'loss_vowel': 0.037039, 'loss_consonant': 0.027491, 'loss_word': 0.036833}\n",
      "   62 | 0.000032 | 160000/160635 | 4.0275 | 6.0927 ||\n",
      "val: {'recall': 0.997451, 'recall_grapheme': 0.9969, 'recall_vowel': 0.998296, 'recall_consonant': 0.99771, 'recall_word': 0.995944, 'acc_grapheme': 0.996443, 'acc_vowel': 0.998284, 'acc_consonant': 0.999055, 'acc_word': 0.995921, 'loss_grapheme': 0.024556, 'loss_vowel': 0.017193, 'loss_consonant': 0.013143, 'loss_word': 0.023336}\n",
      "###>>>>> saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   63 | 0.000029 | 160000/160635 | 13.4482 | 5.8638 ||\n",
      "val: {'recall': 0.996825, 'recall_grapheme': 0.996039, 'recall_vowel': 0.997694, 'recall_consonant': 0.997528, 'recall_word': 0.995306, 'acc_grapheme': 0.995598, 'acc_vowel': 0.99796, 'acc_consonant': 0.998856, 'acc_word': 0.995274, 'loss_grapheme': 0.04512, 'loss_vowel': 0.038703, 'loss_consonant': 0.028448, 'loss_word': 0.036169}\n",
      "   64 | 0.000025 | 160000/160635 | 7.0196 | 6.2755 ||\n",
      "val: {'recall': 0.996891, 'recall_grapheme': 0.996094, 'recall_vowel': 0.997755, 'recall_consonant': 0.99762, 'recall_word': 0.995298, 'acc_grapheme': 0.995722, 'acc_vowel': 0.997911, 'acc_consonant': 0.99893, 'acc_word': 0.995324, 'loss_grapheme': 0.053122, 'loss_vowel': 0.046575, 'loss_consonant': 0.033558, 'loss_word': 0.039225}\n",
      "   65 | 0.000022 | 160000/160635 | 14.3695 | 5.9077 |\n",
      "val: {'recall': 0.996719, 'recall_grapheme': 0.996104, 'recall_vowel': 0.997337, 'recall_consonant': 0.997331, 'recall_word': 0.994826, 'acc_grapheme': 0.995374, 'acc_vowel': 0.997687, 'acc_consonant': 0.998707, 'acc_word': 0.994827, 'loss_grapheme': 0.065303, 'loss_vowel': 0.056111, 'loss_consonant': 0.040497, 'loss_word': 0.047466}\n",
      "   66 | 0.000019 | 160000/160635 | 1.1128 | 5.2262 |||\n",
      "val: {'recall': 0.996828, 'recall_grapheme': 0.996102, 'recall_vowel': 0.997544, 'recall_consonant': 0.997564, 'recall_word': 0.995209, 'acc_grapheme': 0.995622, 'acc_vowel': 0.997811, 'acc_consonant': 0.998856, 'acc_word': 0.9952, 'loss_grapheme': 0.037808, 'loss_vowel': 0.030073, 'loss_consonant': 0.022282, 'loss_word': 0.032862}\n",
      "   67 | 0.000016 | 160000/160635 | 14.9547 | 6.5760 |\n",
      "val: {'recall': 0.996738, 'recall_grapheme': 0.996027, 'recall_vowel': 0.9973, 'recall_consonant': 0.997598, 'recall_word': 0.995209, 'acc_grapheme': 0.995598, 'acc_vowel': 0.997737, 'acc_consonant': 0.998881, 'acc_word': 0.995224, 'loss_grapheme': 0.051467, 'loss_vowel': 0.045172, 'loss_consonant': 0.031702, 'loss_word': 0.038082}\n",
      "   68 | 0.000014 | 160000/160635 | 8.5511 | 5.5480 ||\n",
      "val: {'recall': 0.997117, 'recall_grapheme': 0.996473, 'recall_vowel': 0.997859, 'recall_consonant': 0.997661, 'recall_word': 0.99569, 'acc_grapheme': 0.996095, 'acc_vowel': 0.99801, 'acc_consonant': 0.99898, 'acc_word': 0.995697, 'loss_grapheme': 0.024054, 'loss_vowel': 0.017143, 'loss_consonant': 0.013229, 'loss_word': 0.022775}\n",
      "   69 | 0.000011 | 160000/160635 | 0.3808 | 5.8944 ||\n",
      "val: {'recall': 0.996976, 'recall_grapheme': 0.996303, 'recall_vowel': 0.997718, 'recall_consonant': 0.99758, 'recall_word': 0.995573, 'acc_grapheme': 0.995996, 'acc_vowel': 0.998035, 'acc_consonant': 0.998955, 'acc_word': 0.995598, 'loss_grapheme': 0.037069, 'loss_vowel': 0.030844, 'loss_consonant': 0.023074, 'loss_word': 0.02984}\n",
      "   70 | 0.000009 | 160000/160635 | 0.3768 | 5.7025 ||\n",
      "val: {'recall': 0.996705, 'recall_grapheme': 0.99605, 'recall_vowel': 0.997258, 'recall_consonant': 0.997463, 'recall_word': 0.995129, 'acc_grapheme': 0.995647, 'acc_vowel': 0.997761, 'acc_consonant': 0.998831, 'acc_word': 0.9952, 'loss_grapheme': 0.041041, 'loss_vowel': 0.035933, 'loss_consonant': 0.027915, 'loss_word': 0.031063}\n",
      "   71 | 0.000007 | 160000/160635 | 14.8193 | 5.0082 |\n",
      "val: {'recall': 0.997383, 'recall_grapheme': 0.996623, 'recall_vowel': 0.997652, 'recall_consonant': 0.998632, 'recall_word': 0.995815, 'acc_grapheme': 0.996344, 'acc_vowel': 0.998184, 'acc_consonant': 0.999055, 'acc_word': 0.995797, 'loss_grapheme': 0.021461, 'loss_vowel': 0.015127, 'loss_consonant': 0.011306, 'loss_word': 0.020628}\n",
      "   72 | 0.000006 | 160000/160635 | 7.5040 | 5.4278 |||\n",
      "val: {'recall': 0.997034, 'recall_grapheme': 0.996428, 'recall_vowel': 0.997736, 'recall_consonant': 0.997542, 'recall_word': 0.995557, 'acc_grapheme': 0.995946, 'acc_vowel': 0.99801, 'acc_consonant': 0.99893, 'acc_word': 0.995548, 'loss_grapheme': 0.037395, 'loss_vowel': 0.030625, 'loss_consonant': 0.022688, 'loss_word': 0.031902}\n",
      "   73 | 0.000004 | 160000/160635 | 11.6027 | 5.5176 |\n",
      "val: {'recall': 0.996711, 'recall_grapheme': 0.995937, 'recall_vowel': 0.997463, 'recall_consonant': 0.997508, 'recall_word': 0.995003, 'acc_grapheme': 0.995498, 'acc_vowel': 0.997836, 'acc_consonant': 0.998806, 'acc_word': 0.99505, 'loss_grapheme': 0.076456, 'loss_vowel': 0.065644, 'loss_consonant': 0.045039, 'loss_word': 0.050823}\n",
      "   74 | 0.000003 | 160000/160635 | 4.8666 | 5.7416 ||\n",
      "val: {'recall': 0.996883, 'recall_grapheme': 0.996148, 'recall_vowel': 0.997751, 'recall_consonant': 0.997485, 'recall_word': 0.995408, 'acc_grapheme': 0.995846, 'acc_vowel': 0.997911, 'acc_consonant': 0.998806, 'acc_word': 0.995423, 'loss_grapheme': 0.036049, 'loss_vowel': 0.029577, 'loss_consonant': 0.021699, 'loss_word': 0.029106}\n",
      "   75 | 0.000002 | 160000/160635 | 0.3640 | 6.0013 |||\n",
      "val: {'recall': 0.997106, 'recall_grapheme': 0.99648, 'recall_vowel': 0.997905, 'recall_consonant': 0.997557, 'recall_word': 0.995869, 'acc_grapheme': 0.996219, 'acc_vowel': 0.998309, 'acc_consonant': 0.99898, 'acc_word': 0.995871, 'loss_grapheme': 0.023805, 'loss_vowel': 0.017434, 'loss_consonant': 0.013832, 'loss_word': 0.022068}\n",
      "   76 | 0.000001 | 160000/160635 | 7.2529 | 5.7162 ||\n",
      "val: {'recall': 0.996905, 'recall_grapheme': 0.996246, 'recall_vowel': 0.997584, 'recall_consonant': 0.997546, 'recall_word': 0.995225, 'acc_grapheme': 0.995747, 'acc_vowel': 0.997737, 'acc_consonant': 0.998856, 'acc_word': 0.995224, 'loss_grapheme': 0.040346, 'loss_vowel': 0.033296, 'loss_consonant': 0.025291, 'loss_word': 0.033138}\n",
      "   77 | 0.000000 | 160000/160635 | 7.5001 | 5.4208 ||\n",
      "val: {'recall': 0.99699, 'recall_grapheme': 0.996336, 'recall_vowel': 0.997737, 'recall_consonant': 0.997553, 'recall_word': 0.995201, 'acc_grapheme': 0.995772, 'acc_vowel': 0.997911, 'acc_consonant': 0.998856, 'acc_word': 0.995175, 'loss_grapheme': 0.045617, 'loss_vowel': 0.035112, 'loss_consonant': 0.025502, 'loss_word': 0.037701}\n",
      "   78 | 0.000000 | 160000/160635 | 1.2597 | 6.3020 ||\n",
      "val: {'recall': 0.996837, 'recall_grapheme': 0.996164, 'recall_vowel': 0.997641, 'recall_consonant': 0.997379, 'recall_word': 0.995146, 'acc_grapheme': 0.995647, 'acc_vowel': 0.997811, 'acc_consonant': 0.998781, 'acc_word': 0.99515, 'loss_grapheme': 0.050319, 'loss_vowel': 0.040709, 'loss_consonant': 0.029828, 'loss_word': 0.041562}\n",
      "   79 | 0.000000 | 160000/160635 | 0.2950 | 5.9399 |||\n",
      "val: {'recall': 0.997251, 'recall_grapheme': 0.996808, 'recall_vowel': 0.99781, 'recall_consonant': 0.997579, 'recall_word': 0.995959, 'acc_grapheme': 0.996344, 'acc_vowel': 0.998184, 'acc_consonant': 0.998955, 'acc_word': 0.995946, 'loss_grapheme': 0.023546, 'loss_vowel': 0.017564, 'loss_consonant': 0.013636, 'loss_word': 0.021432}\n",
      "CYCLE: 2\n",
      "{'recall': 0.997251, 'recall_grapheme': 0.996808, 'recall_vowel': 0.99781, 'recall_consonant': 0.997579, 'recall_word': 0.995959, 'acc_grapheme': 0.996344, 'acc_vowel': 0.998184, 'acc_consonant': 0.998955, 'acc_word': 0.995946, 'loss_grapheme': 0.023546, 'loss_vowel': 0.017564, 'loss_consonant': 0.013636, 'loss_word': 0.021432}\n",
      "    0 | 0.000060 | 160000/160635 | 0.1241 | 5.8911 ||\n",
      "val: {'recall': 0.997645, 'recall_grapheme': 0.996815, 'recall_vowel': 0.99822, 'recall_consonant': 0.998731, 'recall_word': 0.995896, 'acc_grapheme': 0.996418, 'acc_vowel': 0.998358, 'acc_consonant': 0.999154, 'acc_word': 0.995896, 'loss_grapheme': 0.021259, 'loss_vowel': 0.013588, 'loss_consonant': 0.00947, 'loss_word': 0.020751}\n",
      "###>>>>> saved\n",
      "    1 | 0.000120 | 160000/160635 | 1.5541 | 6.2342 ||\n",
      "val: {'recall': 0.996975, 'recall_grapheme': 0.996387, 'recall_vowel': 0.997524, 'recall_consonant': 0.9976, 'recall_word': 0.995396, 'acc_grapheme': 0.995921, 'acc_vowel': 0.99801, 'acc_consonant': 0.998955, 'acc_word': 0.995374, 'loss_grapheme': 0.032291, 'loss_vowel': 0.025405, 'loss_consonant': 0.019053, 'loss_word': 0.028376}\n",
      "    2 | 0.000179 | 160000/160635 | 0.3231 | 6.2847 ||\n",
      "val: {'recall': 0.996772, 'recall_grapheme': 0.995984, 'recall_vowel': 0.997633, 'recall_consonant': 0.997489, 'recall_word': 0.995144, 'acc_grapheme': 0.995498, 'acc_vowel': 0.997936, 'acc_consonant': 0.998831, 'acc_word': 0.9951, 'loss_grapheme': 0.033766, 'loss_vowel': 0.027896, 'loss_consonant': 0.020449, 'loss_word': 0.029186}\n",
      "    3 | 0.000238 | 160000/160635 | 0.3935 | 5.5175 |||\n",
      "val: {'recall': 0.996556, 'recall_grapheme': 0.995782, 'recall_vowel': 0.997207, 'recall_consonant': 0.997453, 'recall_word': 0.994599, 'acc_grapheme': 0.9952, 'acc_vowel': 0.997587, 'acc_consonant': 0.998756, 'acc_word': 0.994652, 'loss_grapheme': 0.036044, 'loss_vowel': 0.027853, 'loss_consonant': 0.02175, 'loss_word': 0.031897}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    4 | 0.000297 | 160000/160635 | 0.4369 | 5.6495 ||\n",
      "val: {'recall': 0.99713, 'recall_grapheme': 0.996358, 'recall_vowel': 0.998175, 'recall_consonant': 0.99763, 'recall_word': 0.995337, 'acc_grapheme': 0.995996, 'acc_vowel': 0.99801, 'acc_consonant': 0.998607, 'acc_word': 0.995299, 'loss_grapheme': 0.021639, 'loss_vowel': 0.014439, 'loss_consonant': 0.010829, 'loss_word': 0.02167}\n",
      "    5 | 0.000296 | 160000/160635 | 12.6550 | 6.2098 |\n",
      "val: {'recall': 0.996045, 'recall_grapheme': 0.995383, 'recall_vowel': 0.997299, 'recall_consonant': 0.996116, 'recall_word': 0.99476, 'acc_grapheme': 0.995001, 'acc_vowel': 0.997687, 'acc_consonant': 0.998632, 'acc_word': 0.994752, 'loss_grapheme': 0.041029, 'loss_vowel': 0.031883, 'loss_consonant': 0.023205, 'loss_word': 0.032587}\n",
      "    6 | 0.000294 | 160000/160635 | 8.3512 | 6.3980 |||\n",
      "val: {'recall': 0.996431, 'recall_grapheme': 0.995546, 'recall_vowel': 0.997308, 'recall_consonant': 0.997324, 'recall_word': 0.99438, 'acc_grapheme': 0.994926, 'acc_vowel': 0.997637, 'acc_consonant': 0.998607, 'acc_word': 0.994329, 'loss_grapheme': 0.039075, 'loss_vowel': 0.032662, 'loss_consonant': 0.024862, 'loss_word': 0.035351}\n",
      "    7 | 0.000293 | 160000/160635 | 5.7580 | 6.1984 ||\n",
      "val: {'recall': 0.996025, 'recall_grapheme': 0.99479, 'recall_vowel': 0.997624, 'recall_consonant': 0.996895, 'recall_word': 0.99398, 'acc_grapheme': 0.99413, 'acc_vowel': 0.997438, 'acc_consonant': 0.998159, 'acc_word': 0.993956, 'loss_grapheme': 0.106603, 'loss_vowel': 0.081071, 'loss_consonant': 0.058024, 'loss_word': 0.07723}\n",
      "    8 | 0.000291 | 160000/160635 | 7.3886 | 6.1003 ||\n",
      "val: {'recall': 0.996688, 'recall_grapheme': 0.995914, 'recall_vowel': 0.997974, 'recall_consonant': 0.996952, 'recall_word': 0.994596, 'acc_grapheme': 0.995274, 'acc_vowel': 0.997737, 'acc_consonant': 0.998458, 'acc_word': 0.994553, 'loss_grapheme': 0.064005, 'loss_vowel': 0.053301, 'loss_consonant': 0.03941, 'loss_word': 0.053284}\n",
      "    9 | 0.000289 | 160000/160635 | 7.7821 | 6.5154 ||\n",
      "val: {'recall': 0.996207, 'recall_grapheme': 0.995288, 'recall_vowel': 0.997034, 'recall_consonant': 0.997219, 'recall_word': 0.994316, 'acc_grapheme': 0.994429, 'acc_vowel': 0.997463, 'acc_consonant': 0.998433, 'acc_word': 0.994205, 'loss_grapheme': 0.0983, 'loss_vowel': 0.068034, 'loss_consonant': 0.050934, 'loss_word': 0.072956}\n",
      "   10 | 0.000286 | 160000/160635 | 15.8133 | 6.5126 ||\n",
      "val: {'recall': 0.997037, 'recall_grapheme': 0.996503, 'recall_vowel': 0.997586, 'recall_consonant': 0.997554, 'recall_word': 0.995487, 'acc_grapheme': 0.995697, 'acc_vowel': 0.997886, 'acc_consonant': 0.998533, 'acc_word': 0.995473, 'loss_grapheme': 0.033229, 'loss_vowel': 0.027201, 'loss_consonant': 0.02124, 'loss_word': 0.02762}\n",
      "   11 | 0.000284 | 160000/160635 | 0.5219 | 5.6495 ||\n",
      "val: {'recall': 0.996685, 'recall_grapheme': 0.995949, 'recall_vowel': 0.997767, 'recall_consonant': 0.997075, 'recall_word': 0.99524, 'acc_grapheme': 0.995399, 'acc_vowel': 0.997911, 'acc_consonant': 0.998557, 'acc_word': 0.995175, 'loss_grapheme': 0.03145, 'loss_vowel': 0.023475, 'loss_consonant': 0.018979, 'loss_word': 0.027991}\n",
      "   12 | 0.000281 | 160000/160635 | 13.1293 | 6.4314 |\n",
      "val: {'recall': 0.996716, 'recall_grapheme': 0.995747, 'recall_vowel': 0.997832, 'recall_consonant': 0.997537, 'recall_word': 0.994809, 'acc_grapheme': 0.9951, 'acc_vowel': 0.997861, 'acc_consonant': 0.998358, 'acc_word': 0.994727, 'loss_grapheme': 0.074183, 'loss_vowel': 0.068254, 'loss_consonant': 0.051021, 'loss_word': 0.053047}\n",
      "   13 | 0.000278 | 160000/160635 | 4.3997 | 6.3932 ||\n",
      "val: {'recall': 0.997088, 'recall_grapheme': 0.996214, 'recall_vowel': 0.998269, 'recall_consonant': 0.997655, 'recall_word': 0.995529, 'acc_grapheme': 0.995821, 'acc_vowel': 0.998259, 'acc_consonant': 0.998906, 'acc_word': 0.995498, 'loss_grapheme': 0.022406, 'loss_vowel': 0.013749, 'loss_consonant': 0.010567, 'loss_word': 0.02215}\n",
      "   14 | 0.000275 | 160000/160635 | 1.8751 | 6.1169 ||\n",
      "val: {'recall': 0.996636, 'recall_grapheme': 0.995945, 'recall_vowel': 0.997337, 'recall_consonant': 0.997318, 'recall_word': 0.994418, 'acc_grapheme': 0.9951, 'acc_vowel': 0.997513, 'acc_consonant': 0.998582, 'acc_word': 0.994329, 'loss_grapheme': 0.053023, 'loss_vowel': 0.043018, 'loss_consonant': 0.029047, 'loss_word': 0.044946}\n",
      "   15 | 0.000271 | 160000/160635 | 0.6690 | 5.7181 ||\n",
      "val: {'recall': 0.996477, 'recall_grapheme': 0.995476, 'recall_vowel': 0.997646, 'recall_consonant': 0.997312, 'recall_word': 0.995008, 'acc_grapheme': 0.995399, 'acc_vowel': 0.997886, 'acc_consonant': 0.998657, 'acc_word': 0.994976, 'loss_grapheme': 0.034512, 'loss_vowel': 0.02655, 'loss_consonant': 0.01916, 'loss_word': 0.028585}\n",
      "   16 | 0.000268 | 160000/160635 | 5.3875 | 5.8078 ||\n",
      "val: {'recall': 0.996754, 'recall_grapheme': 0.99594, 'recall_vowel': 0.997383, 'recall_consonant': 0.997755, 'recall_word': 0.994925, 'acc_grapheme': 0.995274, 'acc_vowel': 0.997662, 'acc_consonant': 0.998632, 'acc_word': 0.994777, 'loss_grapheme': 0.065902, 'loss_vowel': 0.054041, 'loss_consonant': 0.037757, 'loss_word': 0.055641}\n",
      "   17 | 0.000264 | 160000/160635 | 7.6482 | 6.1874 ||\n",
      "val: {'recall': 0.996691, 'recall_grapheme': 0.995779, 'recall_vowel': 0.997918, 'recall_consonant': 0.997289, 'recall_word': 0.994947, 'acc_grapheme': 0.995374, 'acc_vowel': 0.99796, 'acc_consonant': 0.998682, 'acc_word': 0.994901, 'loss_grapheme': 0.043769, 'loss_vowel': 0.036006, 'loss_consonant': 0.027206, 'loss_word': 0.035774}\n",
      "   18 | 0.000260 | 160000/160635 | 3.1407 | 5.6480 ||\n",
      "val: {'recall': 0.996894, 'recall_grapheme': 0.996033, 'recall_vowel': 0.998066, 'recall_consonant': 0.997442, 'recall_word': 0.995187, 'acc_grapheme': 0.995996, 'acc_vowel': 0.99796, 'acc_consonant': 0.998756, 'acc_word': 0.995249, 'loss_grapheme': 0.022217, 'loss_vowel': 0.0146, 'loss_consonant': 0.011337, 'loss_word': 0.023026}\n",
      "   19 | 0.000256 | 160000/160635 | 8.9533 | 6.4498 ||\n",
      "val: {'recall': 0.996352, 'recall_grapheme': 0.995277, 'recall_vowel': 0.997205, 'recall_consonant': 0.997648, 'recall_word': 0.994661, 'acc_grapheme': 0.994503, 'acc_vowel': 0.997836, 'acc_consonant': 0.998433, 'acc_word': 0.994553, 'loss_grapheme': 0.048325, 'loss_vowel': 0.04023, 'loss_consonant': 0.029307, 'loss_word': 0.03861}\n",
      "   20 | 0.000252 | 160000/160635 | 14.1310 | 6.0127 |\n",
      "val: {'recall': 0.9964, 'recall_grapheme': 0.995342, 'recall_vowel': 0.99746, 'recall_consonant': 0.997458, 'recall_word': 0.994371, 'acc_grapheme': 0.994702, 'acc_vowel': 0.997637, 'acc_consonant': 0.998408, 'acc_word': 0.994354, 'loss_grapheme': 0.050486, 'loss_vowel': 0.040009, 'loss_consonant': 0.029117, 'loss_word': 0.041225}\n",
      "   21 | 0.000247 | 160000/160635 | 12.2301 | 6.1305 |\n",
      "val: {'recall': 0.997043, 'recall_grapheme': 0.996295, 'recall_vowel': 0.997941, 'recall_consonant': 0.99764, 'recall_word': 0.995478, 'acc_grapheme': 0.995672, 'acc_vowel': 0.998035, 'acc_consonant': 0.998906, 'acc_word': 0.995399, 'loss_grapheme': 0.026717, 'loss_vowel': 0.018505, 'loss_consonant': 0.013592, 'loss_word': 0.025116}\n",
      "   22 | 0.000243 | 160000/160635 | 0.2907 | 6.3551 ||\n",
      "val: {'recall': 0.996806, 'recall_grapheme': 0.995816, 'recall_vowel': 0.998107, 'recall_consonant': 0.997486, 'recall_word': 0.995254, 'acc_grapheme': 0.995598, 'acc_vowel': 0.998035, 'acc_consonant': 0.998756, 'acc_word': 0.995175, 'loss_grapheme': 0.041558, 'loss_vowel': 0.032393, 'loss_consonant': 0.026157, 'loss_word': 0.0354}\n",
      "   23 | 0.000238 | 160000/160635 | 0.5147 | 6.0594 ||\n",
      "val: {'recall': 0.996905, 'recall_grapheme': 0.996238, 'recall_vowel': 0.997783, 'recall_consonant': 0.997362, 'recall_word': 0.995114, 'acc_grapheme': 0.995647, 'acc_vowel': 0.997985, 'acc_consonant': 0.998732, 'acc_word': 0.995075, 'loss_grapheme': 0.034428, 'loss_vowel': 0.025075, 'loss_consonant': 0.019437, 'loss_word': 0.029918}\n",
      "   24 | 0.000233 | 160000/160635 | 0.4011 | 5.9387 ||\n",
      "val: {'recall': 0.996913, 'recall_grapheme': 0.996034, 'recall_vowel': 0.997542, 'recall_consonant': 0.998042, 'recall_word': 0.995418, 'acc_grapheme': 0.995722, 'acc_vowel': 0.997911, 'acc_consonant': 0.998881, 'acc_word': 0.995399, 'loss_grapheme': 0.029206, 'loss_vowel': 0.021448, 'loss_consonant': 0.016084, 'loss_word': 0.026736}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   25 | 0.000228 | 160000/160635 | 11.9801 | 6.0955 |\n",
      "val: {'recall': 0.99624, 'recall_grapheme': 0.995238, 'recall_vowel': 0.99756, 'recall_consonant': 0.996924, 'recall_word': 0.994447, 'acc_grapheme': 0.995001, 'acc_vowel': 0.997761, 'acc_consonant': 0.998458, 'acc_word': 0.994429, 'loss_grapheme': 0.039731, 'loss_vowel': 0.031945, 'loss_consonant': 0.026598, 'loss_word': 0.032693}\n",
      "   26 | 0.000223 | 160000/160635 | 5.4470 | 5.7559 ||\n",
      "val: {'recall': 0.996501, 'recall_grapheme': 0.995579, 'recall_vowel': 0.997807, 'recall_consonant': 0.997036, 'recall_word': 0.994917, 'acc_grapheme': 0.995349, 'acc_vowel': 0.99801, 'acc_consonant': 0.998458, 'acc_word': 0.994827, 'loss_grapheme': 0.051517, 'loss_vowel': 0.041801, 'loss_consonant': 0.032477, 'loss_word': 0.045697}\n",
      "   27 | 0.000218 | 160000/160635 | 5.6375 | 5.4967 ||\n",
      "val: {'recall': 0.996877, 'recall_grapheme': 0.996121, 'recall_vowel': 0.997963, 'recall_consonant': 0.997303, 'recall_word': 0.995049, 'acc_grapheme': 0.995274, 'acc_vowel': 0.99806, 'acc_consonant': 0.998682, 'acc_word': 0.995025, 'loss_grapheme': 0.034523, 'loss_vowel': 0.02759, 'loss_consonant': 0.021286, 'loss_word': 0.031946}\n",
      "   28 | 0.000213 | 160000/160635 | 16.5959 | 6.1713 |\n",
      "val: {'recall': 0.996604, 'recall_grapheme': 0.995775, 'recall_vowel': 0.997556, 'recall_consonant': 0.997311, 'recall_word': 0.994769, 'acc_grapheme': 0.995399, 'acc_vowel': 0.997737, 'acc_consonant': 0.998657, 'acc_word': 0.994727, 'loss_grapheme': 0.034663, 'loss_vowel': 0.028713, 'loss_consonant': 0.020736, 'loss_word': 0.031547}\n",
      "   29 | 0.000207 | 160000/160635 | 7.7054 | 5.9775 ||\n",
      "val: {'recall': 0.996923, 'recall_grapheme': 0.995917, 'recall_vowel': 0.998067, 'recall_consonant': 0.997791, 'recall_word': 0.994881, 'acc_grapheme': 0.995224, 'acc_vowel': 0.997936, 'acc_consonant': 0.998582, 'acc_word': 0.994827, 'loss_grapheme': 0.034522, 'loss_vowel': 0.027553, 'loss_consonant': 0.021427, 'loss_word': 0.031321}\n",
      "   30 | 0.000202 | 160000/160635 | 4.2419 | 5.3622 ||\n",
      "val: {'recall': 0.997179, 'recall_grapheme': 0.99631, 'recall_vowel': 0.997992, 'recall_consonant': 0.998105, 'recall_word': 0.99543, 'acc_grapheme': 0.995846, 'acc_vowel': 0.99811, 'acc_consonant': 0.998856, 'acc_word': 0.995399, 'loss_grapheme': 0.028688, 'loss_vowel': 0.019958, 'loss_consonant': 0.015477, 'loss_word': 0.026597}\n",
      "   31 | 0.000196 | 160000/160635 | 6.0092 | 5.4594 ||\n",
      "val: {'recall': 0.997248, 'recall_grapheme': 0.996465, 'recall_vowel': 0.997976, 'recall_consonant': 0.998087, 'recall_word': 0.995446, 'acc_grapheme': 0.995971, 'acc_vowel': 0.99801, 'acc_consonant': 0.998856, 'acc_word': 0.995423, 'loss_grapheme': 0.026541, 'loss_vowel': 0.018321, 'loss_consonant': 0.013744, 'loss_word': 0.025008}\n",
      "   32 | 0.000191 | 160000/160635 | 8.9058 | 5.6285 ||\n",
      "val: {'recall': 0.997111, 'recall_grapheme': 0.996051, 'recall_vowel': 0.998154, 'recall_consonant': 0.99819, 'recall_word': 0.995308, 'acc_grapheme': 0.995722, 'acc_vowel': 0.998085, 'acc_consonant': 0.998906, 'acc_word': 0.995299, 'loss_grapheme': 0.032573, 'loss_vowel': 0.02405, 'loss_consonant': 0.017727, 'loss_word': 0.028663}\n",
      "   33 | 0.000185 | 160000/160635 | 6.7200 | 6.1434 |||\n",
      "val: {'recall': 0.996847, 'recall_grapheme': 0.995636, 'recall_vowel': 0.99754, 'recall_consonant': 0.998576, 'recall_word': 0.994588, 'acc_grapheme': 0.99515, 'acc_vowel': 0.997761, 'acc_consonant': 0.998732, 'acc_word': 0.994553, 'loss_grapheme': 0.042593, 'loss_vowel': 0.035982, 'loss_consonant': 0.026547, 'loss_word': 0.034565}\n",
      "   34 | 0.000179 | 160000/160635 | 0.5505 | 5.7421 ||\n",
      "val: {'recall': 0.997091, 'recall_grapheme': 0.996353, 'recall_vowel': 0.99744, 'recall_consonant': 0.99822, 'recall_word': 0.995162, 'acc_grapheme': 0.995821, 'acc_vowel': 0.997786, 'acc_consonant': 0.999005, 'acc_word': 0.99515, 'loss_grapheme': 0.032621, 'loss_vowel': 0.02483, 'loss_consonant': 0.018747, 'loss_word': 0.0295}\n",
      "   35 | 0.000173 | 160000/160635 | 0.4662 | 6.2473 ||\n",
      "val: {'recall': 0.997462, 'recall_grapheme': 0.996806, 'recall_vowel': 0.99803, 'recall_consonant': 0.998207, 'recall_word': 0.995996, 'acc_grapheme': 0.996617, 'acc_vowel': 0.998159, 'acc_consonant': 0.999055, 'acc_word': 0.996045, 'loss_grapheme': 0.018975, 'loss_vowel': 0.011978, 'loss_consonant': 0.00921, 'loss_word': 0.019445}\n",
      "   36 | 0.000168 | 160000/160635 | 4.2802 | 5.9422 ||\n",
      "val: {'recall': 0.996836, 'recall_grapheme': 0.995655, 'recall_vowel': 0.997912, 'recall_consonant': 0.99812, 'recall_word': 0.994955, 'acc_grapheme': 0.995374, 'acc_vowel': 0.998035, 'acc_consonant': 0.998831, 'acc_word': 0.995001, 'loss_grapheme': 0.062986, 'loss_vowel': 0.052678, 'loss_consonant': 0.036738, 'loss_word': 0.048616}\n",
      "   37 | 0.000162 | 160000/160635 | 5.7511 | 5.8293 ||\n",
      "val: {'recall': 0.997112, 'recall_grapheme': 0.996205, 'recall_vowel': 0.997957, 'recall_consonant': 0.99808, 'recall_word': 0.995142, 'acc_grapheme': 0.995871, 'acc_vowel': 0.99796, 'acc_consonant': 0.998881, 'acc_word': 0.995125, 'loss_grapheme': 0.033913, 'loss_vowel': 0.025703, 'loss_consonant': 0.019015, 'loss_word': 0.031166}\n",
      "   38 | 0.000156 | 160000/160635 | 13.2651 | 6.4189 ||\n",
      "val: {'recall': 0.996387, 'recall_grapheme': 0.995335, 'recall_vowel': 0.997662, 'recall_consonant': 0.997214, 'recall_word': 0.994748, 'acc_grapheme': 0.994926, 'acc_vowel': 0.997761, 'acc_consonant': 0.998657, 'acc_word': 0.994727, 'loss_grapheme': 0.053269, 'loss_vowel': 0.049591, 'loss_consonant': 0.037806, 'loss_word': 0.042114}\n",
      "   39 | 0.000150 | 160000/160635 | 7.5397 | 5.5335 |||\n",
      "val: {'recall': 0.996488, 'recall_grapheme': 0.995389, 'recall_vowel': 0.99739, 'recall_consonant': 0.997786, 'recall_word': 0.994665, 'acc_grapheme': 0.994802, 'acc_vowel': 0.997637, 'acc_consonant': 0.998557, 'acc_word': 0.994652, 'loss_grapheme': 0.093359, 'loss_vowel': 0.077331, 'loss_consonant': 0.053522, 'loss_word': 0.064031}\n",
      "   40 | 0.000144 | 160000/160635 | 8.9664 | 6.3936 |||\n",
      "val: {'recall': 0.996778, 'recall_grapheme': 0.995821, 'recall_vowel': 0.998112, 'recall_consonant': 0.997357, 'recall_word': 0.994947, 'acc_grapheme': 0.995448, 'acc_vowel': 0.99801, 'acc_consonant': 0.998632, 'acc_word': 0.994926, 'loss_grapheme': 0.047229, 'loss_vowel': 0.037934, 'loss_consonant': 0.028301, 'loss_word': 0.039957}\n",
      "   41 | 0.000138 | 160000/160635 | 15.6644 | 5.3668 |\n",
      "val: {'recall': 0.99637, 'recall_grapheme': 0.995294, 'recall_vowel': 0.997462, 'recall_consonant': 0.997431, 'recall_word': 0.994711, 'acc_grapheme': 0.995001, 'acc_vowel': 0.997712, 'acc_consonant': 0.998657, 'acc_word': 0.994702, 'loss_grapheme': 0.043016, 'loss_vowel': 0.039033, 'loss_consonant': 0.029413, 'loss_word': 0.036579}\n",
      "   42 | 0.000132 | 160000/160635 | 0.1791 | 6.0944 |||\n",
      "val: {'recall': 0.99747, 'recall_grapheme': 0.996626, 'recall_vowel': 0.998008, 'recall_consonant': 0.99862, 'recall_word': 0.995776, 'acc_grapheme': 0.996219, 'acc_vowel': 0.998159, 'acc_consonant': 0.99893, 'acc_word': 0.995772, 'loss_grapheme': 0.029025, 'loss_vowel': 0.022776, 'loss_consonant': 0.017213, 'loss_word': 0.026847}\n",
      "   43 | 0.000127 | 160000/160635 | 1.0549 | 5.8943 ||\n",
      "val: {'recall': 0.997363, 'recall_grapheme': 0.996382, 'recall_vowel': 0.997957, 'recall_consonant': 0.998729, 'recall_word': 0.995479, 'acc_grapheme': 0.996095, 'acc_vowel': 0.99811, 'acc_consonant': 0.99893, 'acc_word': 0.995473, 'loss_grapheme': 0.031381, 'loss_vowel': 0.024665, 'loss_consonant': 0.018584, 'loss_word': 0.028878}\n",
      "   44 | 0.000121 | 160000/160635 | 6.6618 | 5.9087 ||\n",
      "val: {'recall': 0.996802, 'recall_grapheme': 0.995699, 'recall_vowel': 0.997443, 'recall_consonant': 0.998365, 'recall_word': 0.994876, 'acc_grapheme': 0.995075, 'acc_vowel': 0.997637, 'acc_consonant': 0.998632, 'acc_word': 0.994876, 'loss_grapheme': 0.06078, 'loss_vowel': 0.053262, 'loss_consonant': 0.037494, 'loss_word': 0.048111}\n",
      "   45 | 0.000115 | 160000/160635 | 14.2714 | 5.5944 ||\n",
      "val: {'recall': 0.99696, 'recall_grapheme': 0.995729, 'recall_vowel': 0.997927, 'recall_consonant': 0.998457, 'recall_word': 0.995123, 'acc_grapheme': 0.995249, 'acc_vowel': 0.997985, 'acc_consonant': 0.998781, 'acc_word': 0.995075, 'loss_grapheme': 0.062063, 'loss_vowel': 0.050809, 'loss_consonant': 0.037596, 'loss_word': 0.047524}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   46 | 0.000109 | 160000/160635 | 9.8065 | 5.4907 ||\n",
      "val: {'recall': 0.997194, 'recall_grapheme': 0.996399, 'recall_vowel': 0.997983, 'recall_consonant': 0.997995, 'recall_word': 0.995271, 'acc_grapheme': 0.995747, 'acc_vowel': 0.99806, 'acc_consonant': 0.998707, 'acc_word': 0.995274, 'loss_grapheme': 0.044276, 'loss_vowel': 0.037074, 'loss_consonant': 0.027793, 'loss_word': 0.033986}\n",
      "   47 | 0.000104 | 160000/160635 | 0.6937 | 5.9416 ||\n",
      "val: {'recall': 0.997561, 'recall_grapheme': 0.996697, 'recall_vowel': 0.998234, 'recall_consonant': 0.998614, 'recall_word': 0.995896, 'acc_grapheme': 0.996319, 'acc_vowel': 0.998309, 'acc_consonant': 0.99898, 'acc_word': 0.995896, 'loss_grapheme': 0.02396, 'loss_vowel': 0.017194, 'loss_consonant': 0.013379, 'loss_word': 0.023348}\n",
      "   48 | 0.000098 | 160000/160635 | 10.4296 | 5.7264 |\n",
      "val: {'recall': 0.997061, 'recall_grapheme': 0.996126, 'recall_vowel': 0.997913, 'recall_consonant': 0.998079, 'recall_word': 0.995004, 'acc_grapheme': 0.995399, 'acc_vowel': 0.99796, 'acc_consonant': 0.998831, 'acc_word': 0.995001, 'loss_grapheme': 0.054803, 'loss_vowel': 0.048512, 'loss_consonant': 0.035946, 'loss_word': 0.041659}\n",
      "   49 | 0.000093 | 160000/160635 | 4.0864 | 6.1653 |||\n",
      "val: {'recall': 0.997431, 'recall_grapheme': 0.996409, 'recall_vowel': 0.998267, 'recall_consonant': 0.998637, 'recall_word': 0.995428, 'acc_grapheme': 0.995846, 'acc_vowel': 0.998209, 'acc_consonant': 0.99898, 'acc_word': 0.995399, 'loss_grapheme': 0.032083, 'loss_vowel': 0.026573, 'loss_consonant': 0.020288, 'loss_word': 0.027561}\n",
      "   50 | 0.000087 | 160000/160635 | 0.2957 | 6.2908 ||\n",
      "val: {'recall': 0.997302, 'recall_grapheme': 0.996428, 'recall_vowel': 0.997979, 'recall_consonant': 0.998373, 'recall_word': 0.995534, 'acc_grapheme': 0.995871, 'acc_vowel': 0.99801, 'acc_consonant': 0.998756, 'acc_word': 0.995523, 'loss_grapheme': 0.035435, 'loss_vowel': 0.029641, 'loss_consonant': 0.021473, 'loss_word': 0.02938}\n",
      "   51 | 0.000082 | 160000/160635 | 6.3033 | 6.7676 ||\n",
      "val: {'recall': 0.997129, 'recall_grapheme': 0.996442, 'recall_vowel': 0.997647, 'recall_consonant': 0.997982, 'recall_word': 0.995193, 'acc_grapheme': 0.99607, 'acc_vowel': 0.997786, 'acc_consonant': 0.998732, 'acc_word': 0.995125, 'loss_grapheme': 0.047269, 'loss_vowel': 0.035829, 'loss_consonant': 0.026297, 'loss_word': 0.040725}\n",
      "   52 | 0.000077 | 160000/160635 | 15.9764 | 5.6877 |\n",
      "val: {'recall': 0.996954, 'recall_grapheme': 0.996354, 'recall_vowel': 0.99755, 'recall_consonant': 0.997556, 'recall_word': 0.994863, 'acc_grapheme': 0.995349, 'acc_vowel': 0.997786, 'acc_consonant': 0.998806, 'acc_word': 0.994827, 'loss_grapheme': 0.059826, 'loss_vowel': 0.052325, 'loss_consonant': 0.0377, 'loss_word': 0.047629}\n",
      "   53 | 0.000072 | 160000/160635 | 0.5539 | 5.5349 |||\n",
      "val: {'recall': 0.99764, 'recall_grapheme': 0.996855, 'recall_vowel': 0.998136, 'recall_consonant': 0.998712, 'recall_word': 0.995805, 'acc_grapheme': 0.996369, 'acc_vowel': 0.998334, 'acc_consonant': 0.99908, 'acc_word': 0.995772, 'loss_grapheme': 0.024896, 'loss_vowel': 0.018087, 'loss_consonant': 0.012817, 'loss_word': 0.024095}\n",
      "   54 | 0.000067 | 160000/160635 | 1.0763 | 5.1667 |||\n",
      "val: {'recall': 0.997701, 'recall_grapheme': 0.996864, 'recall_vowel': 0.998308, 'recall_consonant': 0.998768, 'recall_word': 0.996022, 'acc_grapheme': 0.996518, 'acc_vowel': 0.998433, 'acc_consonant': 0.999179, 'acc_word': 0.99602, 'loss_grapheme': 0.021395, 'loss_vowel': 0.013972, 'loss_consonant': 0.009804, 'loss_word': 0.021706}\n",
      "###>>>>> saved\n",
      "   55 | 0.000062 | 160000/160635 | 0.5594 | 5.6450 ||\n",
      "val: {'recall': 0.997468, 'recall_grapheme': 0.996752, 'recall_vowel': 0.998321, 'recall_consonant': 0.998047, 'recall_word': 0.995861, 'acc_grapheme': 0.996244, 'acc_vowel': 0.998433, 'acc_consonant': 0.99898, 'acc_word': 0.995846, 'loss_grapheme': 0.022859, 'loss_vowel': 0.016235, 'loss_consonant': 0.012502, 'loss_word': 0.021999}\n",
      "   56 | 0.000057 | 160000/160635 | 6.7338 | 6.0111 ||\n",
      "val: {'recall': 0.996944, 'recall_grapheme': 0.996211, 'recall_vowel': 0.997843, 'recall_consonant': 0.99751, 'recall_word': 0.995284, 'acc_grapheme': 0.995622, 'acc_vowel': 0.99806, 'acc_consonant': 0.998831, 'acc_word': 0.995249, 'loss_grapheme': 0.055318, 'loss_vowel': 0.049306, 'loss_consonant': 0.034572, 'loss_word': 0.04224}\n",
      "   57 | 0.000053 | 160000/160635 | 5.4625 | 6.0170 ||\n",
      "val: {'recall': 0.997539, 'recall_grapheme': 0.996913, 'recall_vowel': 0.998152, 'recall_consonant': 0.998179, 'recall_word': 0.995945, 'acc_grapheme': 0.996269, 'acc_vowel': 0.998358, 'acc_consonant': 0.999055, 'acc_word': 0.995896, 'loss_grapheme': 0.025591, 'loss_vowel': 0.018707, 'loss_consonant': 0.013809, 'loss_word': 0.024377}\n",
      "   58 | 0.000048 | 160000/160635 | 9.6932 | 5.7177 ||\n",
      "val: {'recall': 0.997572, 'recall_grapheme': 0.996806, 'recall_vowel': 0.997987, 'recall_consonant': 0.998689, 'recall_word': 0.995793, 'acc_grapheme': 0.996344, 'acc_vowel': 0.998259, 'acc_consonant': 0.99903, 'acc_word': 0.995772, 'loss_grapheme': 0.027998, 'loss_vowel': 0.021414, 'loss_consonant': 0.015671, 'loss_word': 0.024785}\n",
      "   59 | 0.000044 | 160000/160635 | 6.8959 | 5.7419 ||\n",
      "val: {'recall': 0.997247, 'recall_grapheme': 0.996561, 'recall_vowel': 0.997694, 'recall_consonant': 0.998174, 'recall_word': 0.995491, 'acc_grapheme': 0.99612, 'acc_vowel': 0.998035, 'acc_consonant': 0.998955, 'acc_word': 0.995448, 'loss_grapheme': 0.033106, 'loss_vowel': 0.026119, 'loss_consonant': 0.018996, 'loss_word': 0.028706}\n",
      "   60 | 0.000040 | 160000/160635 | 8.7344 | 6.2292 ||\n",
      "val: {'recall': 0.997279, 'recall_grapheme': 0.996653, 'recall_vowel': 0.998192, 'recall_consonant': 0.997617, 'recall_word': 0.995561, 'acc_grapheme': 0.99602, 'acc_vowel': 0.998234, 'acc_consonant': 0.99898, 'acc_word': 0.995498, 'loss_grapheme': 0.033216, 'loss_vowel': 0.02695, 'loss_consonant': 0.020515, 'loss_word': 0.030163}\n",
      "   61 | 0.000036 | 160000/160635 | 6.8850 | 5.0591 ||\n",
      "val: {'recall': 0.99718, 'recall_grapheme': 0.996544, 'recall_vowel': 0.998, 'recall_consonant': 0.997631, 'recall_word': 0.995568, 'acc_grapheme': 0.995871, 'acc_vowel': 0.998209, 'acc_consonant': 0.998906, 'acc_word': 0.995498, 'loss_grapheme': 0.045961, 'loss_vowel': 0.037156, 'loss_consonant': 0.027084, 'loss_word': 0.039403}\n",
      "   62 | 0.000032 | 160000/160635 | 12.4223 | 6.2778 |\n",
      "val: {'recall': 0.997691, 'recall_grapheme': 0.996803, 'recall_vowel': 0.998344, 'recall_consonant': 0.998813, 'recall_word': 0.995929, 'acc_grapheme': 0.996393, 'acc_vowel': 0.998433, 'acc_consonant': 0.999129, 'acc_word': 0.995871, 'loss_grapheme': 0.022587, 'loss_vowel': 0.015801, 'loss_consonant': 0.011577, 'loss_word': 0.022819}\n",
      "   63 | 0.000029 | 160000/160635 | 0.3817 | 5.7107 ||\n",
      "val: {'recall': 0.997418, 'recall_grapheme': 0.996732, 'recall_vowel': 0.998068, 'recall_consonant': 0.998138, 'recall_word': 0.995773, 'acc_grapheme': 0.996045, 'acc_vowel': 0.998209, 'acc_consonant': 0.99898, 'acc_word': 0.995747, 'loss_grapheme': 0.027659, 'loss_vowel': 0.02226, 'loss_consonant': 0.017247, 'loss_word': 0.024708}\n",
      "   64 | 0.000025 | 160000/160635 | 3.7329 | 5.8977 |||\n",
      "val: {'recall': 0.997354, 'recall_grapheme': 0.996808, 'recall_vowel': 0.998122, 'recall_consonant': 0.997678, 'recall_word': 0.995718, 'acc_grapheme': 0.99607, 'acc_vowel': 0.998284, 'acc_consonant': 0.99898, 'acc_word': 0.995697, 'loss_grapheme': 0.035892, 'loss_vowel': 0.02853, 'loss_consonant': 0.021652, 'loss_word': 0.031781}\n",
      "   65 | 0.000022 | 160000/160635 | 0.4691 | 5.5458 ||\n",
      "val: {'recall': 0.997675, 'recall_grapheme': 0.996918, 'recall_vowel': 0.998109, 'recall_consonant': 0.998757, 'recall_word': 0.996049, 'acc_grapheme': 0.996418, 'acc_vowel': 0.998433, 'acc_consonant': 0.999129, 'acc_word': 0.99602, 'loss_grapheme': 0.020428, 'loss_vowel': 0.013684, 'loss_consonant': 0.010224, 'loss_word': 0.020634}\n",
      "   66 | 0.000019 | 160000/160635 | 2.7132 | 5.4412 ||\n",
      "val: {'recall': 0.997705, 'recall_grapheme': 0.997171, 'recall_vowel': 0.998176, 'recall_consonant': 0.998303, 'recall_word': 0.996095, 'acc_grapheme': 0.996667, 'acc_vowel': 0.998358, 'acc_consonant': 0.999105, 'acc_word': 0.99607, 'loss_grapheme': 0.019683, 'loss_vowel': 0.013333, 'loss_consonant': 0.009645, 'loss_word': 0.020185}\n",
      "###>>>>> saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   67 | 0.000016 | 160000/160635 | 10.4374 | 5.9359 |\n",
      "val: {'recall': 0.99747, 'recall_grapheme': 0.996933, 'recall_vowel': 0.998257, 'recall_consonant': 0.997755, 'recall_word': 0.99564, 'acc_grapheme': 0.99617, 'acc_vowel': 0.998284, 'acc_consonant': 0.999055, 'acc_word': 0.995647, 'loss_grapheme': 0.032994, 'loss_vowel': 0.026946, 'loss_consonant': 0.021132, 'loss_word': 0.029545}\n",
      "   68 | 0.000014 | 160000/160635 | 12.8787 | 5.1190 |\n",
      "val: {'recall': 0.997087, 'recall_grapheme': 0.996493, 'recall_vowel': 0.997766, 'recall_consonant': 0.997596, 'recall_word': 0.995087, 'acc_grapheme': 0.995598, 'acc_vowel': 0.99801, 'acc_consonant': 0.998856, 'acc_word': 0.995075, 'loss_grapheme': 0.063344, 'loss_vowel': 0.056675, 'loss_consonant': 0.040807, 'loss_word': 0.046101}\n",
      "   69 | 0.000011 | 160000/160635 | 0.4144 | 6.2776 |||\n",
      "val: {'recall': 0.997692, 'recall_grapheme': 0.997121, 'recall_vowel': 0.998292, 'recall_consonant': 0.998235, 'recall_word': 0.995898, 'acc_grapheme': 0.996393, 'acc_vowel': 0.998433, 'acc_consonant': 0.999055, 'acc_word': 0.995871, 'loss_grapheme': 0.029208, 'loss_vowel': 0.022359, 'loss_consonant': 0.016969, 'loss_word': 0.02667}\n",
      "   70 | 0.000009 | 160000/160635 | 13.4493 | 5.6363 ||\n",
      "val: {'recall': 0.997442, 'recall_grapheme': 0.99685, 'recall_vowel': 0.997998, 'recall_consonant': 0.998069, 'recall_word': 0.995853, 'acc_grapheme': 0.996269, 'acc_vowel': 0.998259, 'acc_consonant': 0.998955, 'acc_word': 0.995846, 'loss_grapheme': 0.0301, 'loss_vowel': 0.024346, 'loss_consonant': 0.018227, 'loss_word': 0.027457}\n",
      "   71 | 0.000007 | 160000/160635 | 4.5431 | 5.9101 |||\n",
      "val: {'recall': 0.996995, 'recall_grapheme': 0.996332, 'recall_vowel': 0.99772, 'recall_consonant': 0.997595, 'recall_word': 0.995148, 'acc_grapheme': 0.995598, 'acc_vowel': 0.99796, 'acc_consonant': 0.998856, 'acc_word': 0.99515, 'loss_grapheme': 0.054564, 'loss_vowel': 0.048693, 'loss_consonant': 0.036351, 'loss_word': 0.043684}\n",
      "   72 | 0.000006 | 160000/160635 | 7.8151 | 5.4799 ||\n",
      "val: {'recall': 0.997356, 'recall_grapheme': 0.996879, 'recall_vowel': 0.998041, 'recall_consonant': 0.997626, 'recall_word': 0.995716, 'acc_grapheme': 0.996095, 'acc_vowel': 0.998358, 'acc_consonant': 0.99893, 'acc_word': 0.995697, 'loss_grapheme': 0.040636, 'loss_vowel': 0.035013, 'loss_consonant': 0.027516, 'loss_word': 0.033472}\n",
      "   73 | 0.000004 | 160000/160635 | 7.9792 | 6.2982 |||\n",
      "val: {'recall': 0.997287, 'recall_grapheme': 0.996666, 'recall_vowel': 0.998073, 'recall_consonant': 0.997744, 'recall_word': 0.99565, 'acc_grapheme': 0.995896, 'acc_vowel': 0.998234, 'acc_consonant': 0.999005, 'acc_word': 0.995622, 'loss_grapheme': 0.035955, 'loss_vowel': 0.02989, 'loss_consonant': 0.022642, 'loss_word': 0.03041}\n",
      "   74 | 0.000003 | 160000/160635 | 4.0162 | 5.8761 |||\n",
      "val: {'recall': 0.997351, 'recall_grapheme': 0.996725, 'recall_vowel': 0.998264, 'recall_consonant': 0.997689, 'recall_word': 0.995875, 'acc_grapheme': 0.996219, 'acc_vowel': 0.998383, 'acc_consonant': 0.99898, 'acc_word': 0.995846, 'loss_grapheme': 0.02856, 'loss_vowel': 0.022798, 'loss_consonant': 0.017275, 'loss_word': 0.026607}\n",
      "   75 | 0.000002 | 160000/160635 | 10.7098 | 5.9064 ||\n",
      "val: {'recall': 0.99715, 'recall_grapheme': 0.996582, 'recall_vowel': 0.997921, 'recall_consonant': 0.997518, 'recall_word': 0.99558, 'acc_grapheme': 0.995896, 'acc_vowel': 0.998135, 'acc_consonant': 0.998881, 'acc_word': 0.995523, 'loss_grapheme': 0.040506, 'loss_vowel': 0.036333, 'loss_consonant': 0.02717, 'loss_word': 0.032671}\n",
      "   76 | 0.000001 | 160000/160635 | 11.9464 | 6.1652 |\n",
      "val: {'recall': 0.997438, 'recall_grapheme': 0.996712, 'recall_vowel': 0.998124, 'recall_consonant': 0.998205, 'recall_word': 0.995916, 'acc_grapheme': 0.996095, 'acc_vowel': 0.998309, 'acc_consonant': 0.99903, 'acc_word': 0.995896, 'loss_grapheme': 0.028104, 'loss_vowel': 0.022286, 'loss_consonant': 0.01658, 'loss_word': 0.025122}\n",
      "   77 | 0.000000 | 160000/160635 | 7.0448 | 5.6930 ||\n",
      "val: {'recall': 0.997505, 'recall_grapheme': 0.996792, 'recall_vowel': 0.998267, 'recall_consonant': 0.998172, 'recall_word': 0.995931, 'acc_grapheme': 0.99612, 'acc_vowel': 0.998408, 'acc_consonant': 0.999005, 'acc_word': 0.995921, 'loss_grapheme': 0.033168, 'loss_vowel': 0.026502, 'loss_consonant': 0.020315, 'loss_word': 0.029015}\n",
      "   78 | 0.000000 | 160000/160635 | 0.5524 | 5.8045 |||\n",
      "val: {'recall': 0.997437, 'recall_grapheme': 0.996945, 'recall_vowel': 0.998149, 'recall_consonant': 0.99771, 'recall_word': 0.995824, 'acc_grapheme': 0.996195, 'acc_vowel': 0.998309, 'acc_consonant': 0.99898, 'acc_word': 0.995797, 'loss_grapheme': 0.029473, 'loss_vowel': 0.024024, 'loss_consonant': 0.01811, 'loss_word': 0.025859}\n",
      "   79 | 0.000000 | 160000/160635 | 7.1064 | 5.6340 ||\n",
      "val: {'recall': 0.997724, 'recall_grapheme': 0.997161, 'recall_vowel': 0.998307, 'recall_consonant': 0.998265, 'recall_word': 0.995965, 'acc_grapheme': 0.996493, 'acc_vowel': 0.998433, 'acc_consonant': 0.99908, 'acc_word': 0.995946, 'loss_grapheme': 0.025744, 'loss_vowel': 0.019319, 'loss_consonant': 0.014667, 'loss_word': 0.02431}\n",
      "###>>>>> saved\n",
      "CYCLE: 3\n",
      "{'recall': 0.997724, 'recall_grapheme': 0.997161, 'recall_vowel': 0.998307, 'recall_consonant': 0.998265, 'recall_word': 0.995965, 'acc_grapheme': 0.996493, 'acc_vowel': 0.998433, 'acc_consonant': 0.99908, 'acc_word': 0.995946, 'loss_grapheme': 0.025744, 'loss_vowel': 0.019319, 'loss_consonant': 0.014667, 'loss_word': 0.02431}\n",
      "    0 | 0.000060 | 160000/160635 | 6.7204 | 5.1137 ||\n",
      "val: {'recall': 0.997111, 'recall_grapheme': 0.996595, 'recall_vowel': 0.997699, 'recall_consonant': 0.997556, 'recall_word': 0.99552, 'acc_grapheme': 0.995772, 'acc_vowel': 0.998135, 'acc_consonant': 0.998806, 'acc_word': 0.995523, 'loss_grapheme': 0.046552, 'loss_vowel': 0.040913, 'loss_consonant': 0.029691, 'loss_word': 0.03618}\n",
      "    1 | 0.000120 | 160000/160635 | 4.0575 | 4.8764 ||\n",
      "val: {'recall': 0.997373, 'recall_grapheme': 0.997067, 'recall_vowel': 0.997891, 'recall_consonant': 0.997468, 'recall_word': 0.995679, 'acc_grapheme': 0.996344, 'acc_vowel': 0.99811, 'acc_consonant': 0.99893, 'acc_word': 0.995647, 'loss_grapheme': 0.026633, 'loss_vowel': 0.018829, 'loss_consonant': 0.013788, 'loss_word': 0.025493}\n",
      "    2 | 0.000179 | 160000/160635 | 14.6830 | 5.1970 |\n",
      "val: {'recall': 0.997, 'recall_grapheme': 0.996226, 'recall_vowel': 0.997519, 'recall_consonant': 0.998031, 'recall_word': 0.995178, 'acc_grapheme': 0.995697, 'acc_vowel': 0.997911, 'acc_consonant': 0.998806, 'acc_word': 0.99515, 'loss_grapheme': 0.035635, 'loss_vowel': 0.028643, 'loss_consonant': 0.021597, 'loss_word': 0.030691}\n",
      "    3 | 0.000238 | 160000/160635 | 4.2568 | 6.2782 ||\n",
      "val: {'recall': 0.996547, 'recall_grapheme': 0.995478, 'recall_vowel': 0.997721, 'recall_consonant': 0.997512, 'recall_word': 0.99488, 'acc_grapheme': 0.995374, 'acc_vowel': 0.997761, 'acc_consonant': 0.998806, 'acc_word': 0.994851, 'loss_grapheme': 0.051568, 'loss_vowel': 0.042597, 'loss_consonant': 0.031044, 'loss_word': 0.039812}\n",
      "    4 | 0.000297 | 160000/160635 | 3.8766 | 5.9394 ||\n",
      "val: {'recall': 0.99659, 'recall_grapheme': 0.996152, 'recall_vowel': 0.997198, 'recall_consonant': 0.99686, 'recall_word': 0.995184, 'acc_grapheme': 0.995747, 'acc_vowel': 0.997836, 'acc_consonant': 0.998732, 'acc_word': 0.995175, 'loss_grapheme': 0.038766, 'loss_vowel': 0.032359, 'loss_consonant': 0.025052, 'loss_word': 0.035132}\n",
      "    5 | 0.000296 | 160000/160635 | 0.3826 | 5.7068 ||\n",
      "val: {'recall': 0.997342, 'recall_grapheme': 0.996374, 'recall_vowel': 0.998108, 'recall_consonant': 0.998512, 'recall_word': 0.995341, 'acc_grapheme': 0.995797, 'acc_vowel': 0.998135, 'acc_consonant': 0.99893, 'acc_word': 0.995299, 'loss_grapheme': 0.026239, 'loss_vowel': 0.019453, 'loss_consonant': 0.014559, 'loss_word': 0.024332}\n",
      "    6 | 0.000294 | 160000/160635 | 5.7513 | 6.1231 |||\n",
      "val: {'recall': 0.997106, 'recall_grapheme': 0.99624, 'recall_vowel': 0.998017, 'recall_consonant': 0.997927, 'recall_word': 0.995397, 'acc_grapheme': 0.995797, 'acc_vowel': 0.998209, 'acc_consonant': 0.998806, 'acc_word': 0.995423, 'loss_grapheme': 0.034593, 'loss_vowel': 0.025862, 'loss_consonant': 0.019853, 'loss_word': 0.02901}\n",
      "    7 | 0.000293 | 160000/160635 | 16.4091 | 5.7095 |\n",
      "val: {'recall': 0.996537, 'recall_grapheme': 0.99558, 'recall_vowel': 0.997343, 'recall_consonant': 0.997644, 'recall_word': 0.994699, 'acc_grapheme': 0.995224, 'acc_vowel': 0.997861, 'acc_consonant': 0.998533, 'acc_word': 0.994652, 'loss_grapheme': 0.062006, 'loss_vowel': 0.053027, 'loss_consonant': 0.040154, 'loss_word': 0.046438}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    8 | 0.000291 | 160000/160635 | 16.9044 | 5.7784 |\n",
      "val: {'recall': 0.996995, 'recall_grapheme': 0.996165, 'recall_vowel': 0.997812, 'recall_consonant': 0.997836, 'recall_word': 0.995302, 'acc_grapheme': 0.995946, 'acc_vowel': 0.99806, 'acc_consonant': 0.998881, 'acc_word': 0.995299, 'loss_grapheme': 0.028182, 'loss_vowel': 0.020619, 'loss_consonant': 0.015741, 'loss_word': 0.026441}\n",
      "    9 | 0.000289 | 160000/160635 | 0.4810 | 5.7326 |||\n",
      "val: {'recall': 0.997221, 'recall_grapheme': 0.996417, 'recall_vowel': 0.998064, 'recall_consonant': 0.997987, 'recall_word': 0.995315, 'acc_grapheme': 0.995797, 'acc_vowel': 0.99801, 'acc_consonant': 0.998906, 'acc_word': 0.995324, 'loss_grapheme': 0.023892, 'loss_vowel': 0.016276, 'loss_consonant': 0.01127, 'loss_word': 0.02406}\n",
      "   10 | 0.000286 | 160000/160635 | 7.0158 | 5.9027 |||\n",
      "val: {'recall': 0.996588, 'recall_grapheme': 0.995294, 'recall_vowel': 0.997828, 'recall_consonant': 0.997935, 'recall_word': 0.994137, 'acc_grapheme': 0.994926, 'acc_vowel': 0.997886, 'acc_consonant': 0.998657, 'acc_word': 0.994105, 'loss_grapheme': 0.044788, 'loss_vowel': 0.032244, 'loss_consonant': 0.02486, 'loss_word': 0.04167}\n",
      "   11 | 0.000284 | 160000/160635 | 12.8475 | 5.8768 |\n",
      "val: {'recall': 0.996645, 'recall_grapheme': 0.995621, 'recall_vowel': 0.997948, 'recall_consonant': 0.997391, 'recall_word': 0.994831, 'acc_grapheme': 0.99505, 'acc_vowel': 0.997861, 'acc_consonant': 0.998831, 'acc_word': 0.994752, 'loss_grapheme': 0.047471, 'loss_vowel': 0.039604, 'loss_consonant': 0.028269, 'loss_word': 0.038938}\n",
      "   12 | 0.000281 | 160000/160635 | 0.5714 | 5.9422 ||\n",
      "val: {'recall': 0.996839, 'recall_grapheme': 0.995703, 'recall_vowel': 0.997853, 'recall_consonant': 0.998095, 'recall_word': 0.994849, 'acc_grapheme': 0.995249, 'acc_vowel': 0.997985, 'acc_consonant': 0.998856, 'acc_word': 0.994802, 'loss_grapheme': 0.04175, 'loss_vowel': 0.031264, 'loss_consonant': 0.023705, 'loss_word': 0.037172}\n",
      "   13 | 0.000278 | 160000/160635 | 0.3847 | 5.6221 ||\n",
      "val: {'recall': 0.997194, 'recall_grapheme': 0.996514, 'recall_vowel': 0.997826, 'recall_consonant': 0.997921, 'recall_word': 0.995157, 'acc_grapheme': 0.995871, 'acc_vowel': 0.997985, 'acc_consonant': 0.998831, 'acc_word': 0.995125, 'loss_grapheme': 0.022035, 'loss_vowel': 0.013914, 'loss_consonant': 0.010283, 'loss_word': 0.022785}\n",
      "   14 | 0.000275 | 160000/160635 | 4.8685 | 6.3193 |||\n",
      "val: {'recall': 0.997085, 'recall_grapheme': 0.996094, 'recall_vowel': 0.998293, 'recall_consonant': 0.997857, 'recall_word': 0.995097, 'acc_grapheme': 0.995598, 'acc_vowel': 0.998209, 'acc_consonant': 0.998732, 'acc_word': 0.99505, 'loss_grapheme': 0.033916, 'loss_vowel': 0.025649, 'loss_consonant': 0.020145, 'loss_word': 0.031642}\n",
      "   15 | 0.000271 | 160000/160635 | 0.4482 | 5.4245 ||\n",
      "val: {'recall': 0.996965, 'recall_grapheme': 0.996336, 'recall_vowel': 0.997651, 'recall_consonant': 0.997538, 'recall_word': 0.995477, 'acc_grapheme': 0.99602, 'acc_vowel': 0.99801, 'acc_consonant': 0.99893, 'acc_word': 0.995399, 'loss_grapheme': 0.020708, 'loss_vowel': 0.013391, 'loss_consonant': 0.009895, 'loss_word': 0.020592}\n",
      "   16 | 0.000268 | 160000/160635 | 0.4447 | 5.8013 ||\n",
      "val: {'recall': 0.996716, 'recall_grapheme': 0.996173, 'recall_vowel': 0.997256, 'recall_consonant': 0.997262, 'recall_word': 0.995412, 'acc_grapheme': 0.995747, 'acc_vowel': 0.99811, 'acc_consonant': 0.998732, 'acc_word': 0.995399, 'loss_grapheme': 0.026676, 'loss_vowel': 0.018783, 'loss_consonant': 0.015411, 'loss_word': 0.025613}\n",
      "   17 | 0.000264 | 160000/160635 | 7.5683 | 5.7437 ||\n",
      "val: {'recall': 0.996262, 'recall_grapheme': 0.995309, 'recall_vowel': 0.997173, 'recall_consonant': 0.997257, 'recall_word': 0.994362, 'acc_grapheme': 0.994652, 'acc_vowel': 0.997513, 'acc_consonant': 0.998806, 'acc_word': 0.994304, 'loss_grapheme': 0.057607, 'loss_vowel': 0.047523, 'loss_consonant': 0.036355, 'loss_word': 0.044791}\n",
      "   18 | 0.000260 | 160000/160635 | 0.4853 | 5.6104 ||\n",
      "val: {'recall': 0.996715, 'recall_grapheme': 0.996139, 'recall_vowel': 0.997414, 'recall_consonant': 0.997168, 'recall_word': 0.994988, 'acc_grapheme': 0.995473, 'acc_vowel': 0.997836, 'acc_consonant': 0.998657, 'acc_word': 0.994976, 'loss_grapheme': 0.037016, 'loss_vowel': 0.026994, 'loss_consonant': 0.020437, 'loss_word': 0.031205}\n",
      "   19 | 0.000256 | 160000/160635 | 7.0230 | 5.8102 ||\n",
      "val: {'recall': 0.996836, 'recall_grapheme': 0.995764, 'recall_vowel': 0.997854, 'recall_consonant': 0.997964, 'recall_word': 0.994869, 'acc_grapheme': 0.995423, 'acc_vowel': 0.997811, 'acc_consonant': 0.998756, 'acc_word': 0.994876, 'loss_grapheme': 0.040584, 'loss_vowel': 0.032176, 'loss_consonant': 0.02612, 'loss_word': 0.035749}\n",
      "   20 | 0.000252 | 160000/160635 | 16.5275 | 5.9101 ||\n",
      "val: {'recall': 0.99667, 'recall_grapheme': 0.996059, 'recall_vowel': 0.99736, 'recall_consonant': 0.9972, 'recall_word': 0.994626, 'acc_grapheme': 0.995498, 'acc_vowel': 0.997811, 'acc_consonant': 0.998607, 'acc_word': 0.994603, 'loss_grapheme': 0.041765, 'loss_vowel': 0.032628, 'loss_consonant': 0.02644, 'loss_word': 0.036442}\n",
      "   21 | 0.000247 | 160000/160635 | 0.5256 | 6.0645 |||\n",
      "val: {'recall': 0.996897, 'recall_grapheme': 0.996181, 'recall_vowel': 0.997857, 'recall_consonant': 0.997369, 'recall_word': 0.995364, 'acc_grapheme': 0.995821, 'acc_vowel': 0.99801, 'acc_consonant': 0.998781, 'acc_word': 0.995299, 'loss_grapheme': 0.027007, 'loss_vowel': 0.018645, 'loss_consonant': 0.015122, 'loss_word': 0.026242}\n",
      "   22 | 0.000243 | 160000/160635 | 10.9176 | 5.6616 |\n",
      "val: {'recall': 0.996754, 'recall_grapheme': 0.99614, 'recall_vowel': 0.997551, 'recall_consonant': 0.997184, 'recall_word': 0.995026, 'acc_grapheme': 0.995797, 'acc_vowel': 0.997637, 'acc_consonant': 0.998632, 'acc_word': 0.994976, 'loss_grapheme': 0.028188, 'loss_vowel': 0.021087, 'loss_consonant': 0.015921, 'loss_word': 0.02607}\n",
      "   23 | 0.000238 | 160000/160635 | 6.6515 | 5.5376 ||\n",
      "val: {'recall': 0.996721, 'recall_grapheme': 0.995592, 'recall_vowel': 0.998152, 'recall_consonant': 0.997549, 'recall_word': 0.994955, 'acc_grapheme': 0.995324, 'acc_vowel': 0.997911, 'acc_consonant': 0.998806, 'acc_word': 0.994901, 'loss_grapheme': 0.032798, 'loss_vowel': 0.027183, 'loss_consonant': 0.019916, 'loss_word': 0.030606}\n",
      "   24 | 0.000233 | 160000/160635 | 3.6417 | 5.9828 ||\n",
      "val: {'recall': 0.997006, 'recall_grapheme': 0.99639, 'recall_vowel': 0.99779, 'recall_consonant': 0.997453, 'recall_word': 0.995332, 'acc_grapheme': 0.995722, 'acc_vowel': 0.997886, 'acc_consonant': 0.998806, 'acc_word': 0.995274, 'loss_grapheme': 0.030919, 'loss_vowel': 0.022761, 'loss_consonant': 0.016948, 'loss_word': 0.029556}\n",
      "   25 | 0.000228 | 160000/160635 | 15.2275 | 5.5440 ||\n",
      "val: {'recall': 0.996158, 'recall_grapheme': 0.994963, 'recall_vowel': 0.997359, 'recall_consonant': 0.997346, 'recall_word': 0.994706, 'acc_grapheme': 0.994851, 'acc_vowel': 0.997587, 'acc_consonant': 0.998682, 'acc_word': 0.994603, 'loss_grapheme': 0.041571, 'loss_vowel': 0.035029, 'loss_consonant': 0.024826, 'loss_word': 0.032877}\n",
      "   26 | 0.000223 | 160000/160635 | 6.9858 | 5.7775 ||\n",
      "val: {'recall': 0.99715, 'recall_grapheme': 0.99633, 'recall_vowel': 0.997898, 'recall_consonant': 0.998041, 'recall_word': 0.995186, 'acc_grapheme': 0.995797, 'acc_vowel': 0.997936, 'acc_consonant': 0.998881, 'acc_word': 0.995175, 'loss_grapheme': 0.036035, 'loss_vowel': 0.029409, 'loss_consonant': 0.021569, 'loss_word': 0.033018}\n",
      "   27 | 0.000218 | 160000/160635 | 13.9062 | 5.7837 |\n",
      "val: {'recall': 0.996606, 'recall_grapheme': 0.996155, 'recall_vowel': 0.997961, 'recall_consonant': 0.996154, 'recall_word': 0.994911, 'acc_grapheme': 0.995473, 'acc_vowel': 0.99811, 'acc_consonant': 0.998732, 'acc_word': 0.994926, 'loss_grapheme': 0.049045, 'loss_vowel': 0.04351, 'loss_consonant': 0.034312, 'loss_word': 0.040477}\n",
      "   28 | 0.000213 | 160000/160635 | 6.5203 | 5.8805 ||\n",
      "val: {'recall': 0.996312, 'recall_grapheme': 0.995193, 'recall_vowel': 0.997763, 'recall_consonant': 0.9971, 'recall_word': 0.994917, 'acc_grapheme': 0.99505, 'acc_vowel': 0.997936, 'acc_consonant': 0.998657, 'acc_word': 0.994901, 'loss_grapheme': 0.043192, 'loss_vowel': 0.036448, 'loss_consonant': 0.028122, 'loss_word': 0.035724}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   29 | 0.000207 | 160000/160635 | 13.4666 | 5.8615 |\n",
      "val: {'recall': 0.996277, 'recall_grapheme': 0.995514, 'recall_vowel': 0.997339, 'recall_consonant': 0.996742, 'recall_word': 0.994625, 'acc_grapheme': 0.994876, 'acc_vowel': 0.997488, 'acc_consonant': 0.998632, 'acc_word': 0.994553, 'loss_grapheme': 0.062222, 'loss_vowel': 0.057674, 'loss_consonant': 0.0427, 'loss_word': 0.048799}\n",
      "   30 | 0.000202 | 160000/160635 | 12.9628 | 5.6233 |\n",
      "val: {'recall': 0.997201, 'recall_grapheme': 0.996381, 'recall_vowel': 0.998176, 'recall_consonant': 0.997866, 'recall_word': 0.995365, 'acc_grapheme': 0.995672, 'acc_vowel': 0.99811, 'acc_consonant': 0.99898, 'acc_word': 0.995299, 'loss_grapheme': 0.031323, 'loss_vowel': 0.025269, 'loss_consonant': 0.018557, 'loss_word': 0.028789}\n",
      "   31 | 0.000196 | 160000/160635 | 0.2912 | 5.7281 |||\n",
      "val: {'recall': 0.997484, 'recall_grapheme': 0.996745, 'recall_vowel': 0.997983, 'recall_consonant': 0.998464, 'recall_word': 0.995805, 'acc_grapheme': 0.996369, 'acc_vowel': 0.998085, 'acc_consonant': 0.998955, 'acc_word': 0.995772, 'loss_grapheme': 0.022594, 'loss_vowel': 0.014776, 'loss_consonant': 0.010392, 'loss_word': 0.022334}\n",
      "   32 | 0.000191 | 160000/160635 | 5.2031 | 5.7052 ||\n",
      "val: {'recall': 0.996918, 'recall_grapheme': 0.996363, 'recall_vowel': 0.99747, 'recall_consonant': 0.997475, 'recall_word': 0.99516, 'acc_grapheme': 0.995846, 'acc_vowel': 0.998085, 'acc_consonant': 0.99898, 'acc_word': 0.995075, 'loss_grapheme': 0.031864, 'loss_vowel': 0.023848, 'loss_consonant': 0.018021, 'loss_word': 0.029504}\n",
      "   33 | 0.000185 | 160000/160635 | 5.9910 | 5.6749 ||\n",
      "val: {'recall': 0.996737, 'recall_grapheme': 0.995888, 'recall_vowel': 0.997783, 'recall_consonant': 0.997388, 'recall_word': 0.995074, 'acc_grapheme': 0.995473, 'acc_vowel': 0.997861, 'acc_consonant': 0.998955, 'acc_word': 0.995025, 'loss_grapheme': 0.028393, 'loss_vowel': 0.022207, 'loss_consonant': 0.016287, 'loss_word': 0.026845}\n",
      "   34 | 0.000179 | 160000/160635 | 9.9817 | 5.9716 |||\n",
      "val: {'recall': 0.997376, 'recall_grapheme': 0.997044, 'recall_vowel': 0.998099, 'recall_consonant': 0.997317, 'recall_word': 0.995944, 'acc_grapheme': 0.996443, 'acc_vowel': 0.998184, 'acc_consonant': 0.998955, 'acc_word': 0.995896, 'loss_grapheme': 0.022171, 'loss_vowel': 0.016542, 'loss_consonant': 0.011662, 'loss_word': 0.021926}\n",
      "   35 | 0.000173 | 160000/160635 | 6.9381 | 5.6959 ||\n",
      "val: {'recall': 0.99712, 'recall_grapheme': 0.996251, 'recall_vowel': 0.998237, 'recall_consonant': 0.997741, 'recall_word': 0.995007, 'acc_grapheme': 0.995573, 'acc_vowel': 0.99806, 'acc_consonant': 0.998682, 'acc_word': 0.994926, 'loss_grapheme': 0.035066, 'loss_vowel': 0.028147, 'loss_consonant': 0.022065, 'loss_word': 0.032646}\n",
      "   36 | 0.000168 | 160000/160635 | 10.6860 | 5.5490 |\n",
      "val: {'recall': 0.997127, 'recall_grapheme': 0.996824, 'recall_vowel': 0.997547, 'recall_consonant': 0.997315, 'recall_word': 0.995571, 'acc_grapheme': 0.996244, 'acc_vowel': 0.998135, 'acc_consonant': 0.998831, 'acc_word': 0.995548, 'loss_grapheme': 0.022022, 'loss_vowel': 0.014829, 'loss_consonant': 0.011547, 'loss_word': 0.021791}\n",
      "   37 | 0.000162 | 160000/160635 | 13.6028 | 5.3534 |\n",
      "val: {'recall': 0.996704, 'recall_grapheme': 0.996477, 'recall_vowel': 0.997528, 'recall_consonant': 0.996333, 'recall_word': 0.995341, 'acc_grapheme': 0.995722, 'acc_vowel': 0.99801, 'acc_consonant': 0.998856, 'acc_word': 0.995274, 'loss_grapheme': 0.037606, 'loss_vowel': 0.031497, 'loss_consonant': 0.024278, 'loss_word': 0.032187}\n",
      "   38 | 0.000156 | 160000/160635 | 4.5851 | 5.8833 ||\n",
      "val: {'recall': 0.997578, 'recall_grapheme': 0.996752, 'recall_vowel': 0.998108, 'recall_consonant': 0.9987, 'recall_word': 0.995923, 'acc_grapheme': 0.996294, 'acc_vowel': 0.998358, 'acc_consonant': 0.999055, 'acc_word': 0.995971, 'loss_grapheme': 0.023182, 'loss_vowel': 0.015682, 'loss_consonant': 0.011956, 'loss_word': 0.023012}\n",
      "   39 | 0.000150 | 160000/160635 | 0.5286 | 5.9349 ||\n",
      "val: {'recall': 0.996976, 'recall_grapheme': 0.995991, 'recall_vowel': 0.99781, 'recall_consonant': 0.998113, 'recall_word': 0.995229, 'acc_grapheme': 0.995622, 'acc_vowel': 0.99811, 'acc_consonant': 0.998781, 'acc_word': 0.9952, 'loss_grapheme': 0.033398, 'loss_vowel': 0.027086, 'loss_consonant': 0.02031, 'loss_word': 0.029427}\n",
      "   40 | 0.000144 | 160000/160635 | 7.9378 | 5.4357 ||\n",
      "val: {'recall': 0.997216, 'recall_grapheme': 0.996417, 'recall_vowel': 0.997901, 'recall_consonant': 0.99813, 'recall_word': 0.995647, 'acc_grapheme': 0.996045, 'acc_vowel': 0.998259, 'acc_consonant': 0.998856, 'acc_word': 0.995622, 'loss_grapheme': 0.024161, 'loss_vowel': 0.017294, 'loss_consonant': 0.013124, 'loss_word': 0.024277}\n",
      "   41 | 0.000138 | 160000/160635 | 5.4726 | 5.4006 ||\n",
      "val: {'recall': 0.996768, 'recall_grapheme': 0.996151, 'recall_vowel': 0.99731, 'recall_consonant': 0.997459, 'recall_word': 0.995194, 'acc_grapheme': 0.995598, 'acc_vowel': 0.997612, 'acc_consonant': 0.998682, 'acc_word': 0.99515, 'loss_grapheme': 0.039463, 'loss_vowel': 0.033494, 'loss_consonant': 0.025744, 'loss_word': 0.035521}\n",
      "   42 | 0.000132 | 160000/160635 | 11.7568 | 5.9704 |\n",
      "val: {'recall': 0.997003, 'recall_grapheme': 0.996394, 'recall_vowel': 0.997914, 'recall_consonant': 0.99731, 'recall_word': 0.99556, 'acc_grapheme': 0.995821, 'acc_vowel': 0.99811, 'acc_consonant': 0.998781, 'acc_word': 0.995548, 'loss_grapheme': 0.035453, 'loss_vowel': 0.028777, 'loss_consonant': 0.022109, 'loss_word': 0.030878}\n",
      "   43 | 0.000127 | 160000/160635 | 0.2679 | 5.7853 ||\n",
      "val: {'recall': 0.997061, 'recall_grapheme': 0.996531, 'recall_vowel': 0.997833, 'recall_consonant': 0.99735, 'recall_word': 0.995612, 'acc_grapheme': 0.995921, 'acc_vowel': 0.998159, 'acc_consonant': 0.998806, 'acc_word': 0.995498, 'loss_grapheme': 0.035512, 'loss_vowel': 0.028602, 'loss_consonant': 0.020918, 'loss_word': 0.031309}\n",
      "   44 | 0.000121 | 160000/160635 | 0.4781 | 6.0355 ||\n",
      "val: {'recall': 0.997698, 'recall_grapheme': 0.99718, 'recall_vowel': 0.997878, 'recall_consonant': 0.998552, 'recall_word': 0.996375, 'acc_grapheme': 0.996791, 'acc_vowel': 0.998259, 'acc_consonant': 0.99898, 'acc_word': 0.996344, 'loss_grapheme': 0.018027, 'loss_vowel': 0.011755, 'loss_consonant': 0.008458, 'loss_word': 0.019245}\n",
      "   45 | 0.000115 | 160000/160635 | 8.2240 | 5.8148 ||\n",
      "val: {'recall': 0.997177, 'recall_grapheme': 0.99667, 'recall_vowel': 0.997864, 'recall_consonant': 0.997501, 'recall_word': 0.995843, 'acc_grapheme': 0.99612, 'acc_vowel': 0.998209, 'acc_consonant': 0.99893, 'acc_word': 0.995821, 'loss_grapheme': 0.02489, 'loss_vowel': 0.018627, 'loss_consonant': 0.014799, 'loss_word': 0.023454}\n",
      "   46 | 0.000109 | 160000/160635 | 0.2145 | 5.9471 ||\n",
      "val: {'recall': 0.997349, 'recall_grapheme': 0.996795, 'recall_vowel': 0.998108, 'recall_consonant': 0.997697, 'recall_word': 0.995721, 'acc_grapheme': 0.996244, 'acc_vowel': 0.998209, 'acc_consonant': 0.99903, 'acc_word': 0.995722, 'loss_grapheme': 0.028294, 'loss_vowel': 0.021544, 'loss_consonant': 0.016508, 'loss_word': 0.026156}\n",
      "   47 | 0.000104 | 160000/160635 | 0.3298 | 5.7033 ||\n",
      "val: {'recall': 0.997551, 'recall_grapheme': 0.996973, 'recall_vowel': 0.998212, 'recall_consonant': 0.998044, 'recall_word': 0.995915, 'acc_grapheme': 0.996493, 'acc_vowel': 0.998383, 'acc_consonant': 0.99903, 'acc_word': 0.995896, 'loss_grapheme': 0.028245, 'loss_vowel': 0.021091, 'loss_consonant': 0.015807, 'loss_word': 0.025991}\n",
      "   48 | 0.000098 | 160000/160635 | 0.5194 | 5.6023 ||\n",
      "val: {'recall': 0.997368, 'recall_grapheme': 0.996807, 'recall_vowel': 0.99845, 'recall_consonant': 0.997407, 'recall_word': 0.996197, 'acc_grapheme': 0.996418, 'acc_vowel': 0.998433, 'acc_consonant': 0.998906, 'acc_word': 0.99617, 'loss_grapheme': 0.022211, 'loss_vowel': 0.015203, 'loss_consonant': 0.011262, 'loss_word': 0.021395}\n",
      "   49 | 0.000093 | 160000/160635 | 12.6136 | 5.4295 |\n",
      "val: {'recall': 0.99741, 'recall_grapheme': 0.99689, 'recall_vowel': 0.99831, 'recall_consonant': 0.997552, 'recall_word': 0.996037, 'acc_grapheme': 0.996468, 'acc_vowel': 0.998408, 'acc_consonant': 0.999005, 'acc_word': 0.99602, 'loss_grapheme': 0.023965, 'loss_vowel': 0.016848, 'loss_consonant': 0.012972, 'loss_word': 0.022982}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   50 | 0.000087 | 160000/160635 | 0.4565 | 5.7895 ||\n",
      "val: {'recall': 0.997172, 'recall_grapheme': 0.996568, 'recall_vowel': 0.99805, 'recall_consonant': 0.997501, 'recall_word': 0.995769, 'acc_grapheme': 0.996344, 'acc_vowel': 0.998408, 'acc_consonant': 0.99893, 'acc_word': 0.995772, 'loss_grapheme': 0.030396, 'loss_vowel': 0.024626, 'loss_consonant': 0.018384, 'loss_word': 0.026732}\n",
      "   51 | 0.000082 | 160000/160635 | 15.2841 | 5.5126 |\n",
      "val: {'recall': 0.996983, 'recall_grapheme': 0.996295, 'recall_vowel': 0.99788, 'recall_consonant': 0.997463, 'recall_word': 0.99554, 'acc_grapheme': 0.995896, 'acc_vowel': 0.998184, 'acc_consonant': 0.99893, 'acc_word': 0.995548, 'loss_grapheme': 0.029333, 'loss_vowel': 0.023453, 'loss_consonant': 0.018837, 'loss_word': 0.026721}\n",
      "   52 | 0.000077 | 160000/160635 | 0.4116 | 5.5332 ||\n",
      "val: {'recall': 0.997146, 'recall_grapheme': 0.996466, 'recall_vowel': 0.998143, 'recall_consonant': 0.997509, 'recall_word': 0.995642, 'acc_grapheme': 0.995921, 'acc_vowel': 0.998184, 'acc_consonant': 0.999055, 'acc_word': 0.995598, 'loss_grapheme': 0.02726, 'loss_vowel': 0.021245, 'loss_consonant': 0.015842, 'loss_word': 0.025354}\n",
      "   53 | 0.000072 | 160000/160635 | 6.2619 | 5.5988 |||\n",
      "val: {'recall': 0.997176, 'recall_grapheme': 0.996726, 'recall_vowel': 0.997729, 'recall_consonant': 0.997522, 'recall_word': 0.995793, 'acc_grapheme': 0.996195, 'acc_vowel': 0.998209, 'acc_consonant': 0.999005, 'acc_word': 0.995772, 'loss_grapheme': 0.028473, 'loss_vowel': 0.0221, 'loss_consonant': 0.017441, 'loss_word': 0.026925}\n",
      "   54 | 0.000067 | 160000/160635 | 5.8325 | 5.6697 ||\n",
      "val: {'recall': 0.997112, 'recall_grapheme': 0.996368, 'recall_vowel': 0.998073, 'recall_consonant': 0.99764, 'recall_word': 0.995757, 'acc_grapheme': 0.995921, 'acc_vowel': 0.998259, 'acc_consonant': 0.999005, 'acc_word': 0.995722, 'loss_grapheme': 0.028602, 'loss_vowel': 0.021737, 'loss_consonant': 0.016211, 'loss_word': 0.027266}\n",
      "   55 | 0.000062 | 160000/160635 | 0.3281 | 5.8822 ||\n",
      "val: {'recall': 0.997573, 'recall_grapheme': 0.996762, 'recall_vowel': 0.998204, 'recall_consonant': 0.998562, 'recall_word': 0.995963, 'acc_grapheme': 0.996468, 'acc_vowel': 0.998433, 'acc_consonant': 0.99908, 'acc_word': 0.995946, 'loss_grapheme': 0.025643, 'loss_vowel': 0.017566, 'loss_consonant': 0.012842, 'loss_word': 0.025836}\n",
      "   56 | 0.000057 | 160000/160635 | 0.4450 | 5.1761 ||\n",
      "val: {'recall': 0.997231, 'recall_grapheme': 0.996765, 'recall_vowel': 0.997961, 'recall_consonant': 0.997435, 'recall_word': 0.995946, 'acc_grapheme': 0.996219, 'acc_vowel': 0.998259, 'acc_consonant': 0.99893, 'acc_word': 0.995921, 'loss_grapheme': 0.024376, 'loss_vowel': 0.018422, 'loss_consonant': 0.013347, 'loss_word': 0.023703}\n",
      "   57 | 0.000053 | 160000/160635 | 6.7950 | 5.4158 ||\n",
      "val: {'recall': 0.996931, 'recall_grapheme': 0.996364, 'recall_vowel': 0.997736, 'recall_consonant': 0.997262, 'recall_word': 0.995342, 'acc_grapheme': 0.995996, 'acc_vowel': 0.997936, 'acc_consonant': 0.998831, 'acc_word': 0.995299, 'loss_grapheme': 0.03067, 'loss_vowel': 0.024706, 'loss_consonant': 0.019836, 'loss_word': 0.028604}\n",
      "   58 | 0.000048 | 160000/160635 | 5.6224 | 5.8068 ||\n",
      "val: {'recall': 0.997237, 'recall_grapheme': 0.996752, 'recall_vowel': 0.997954, 'recall_consonant': 0.99749, 'recall_word': 0.995841, 'acc_grapheme': 0.996294, 'acc_vowel': 0.998159, 'acc_consonant': 0.999005, 'acc_word': 0.995821, 'loss_grapheme': 0.02887, 'loss_vowel': 0.0224, 'loss_consonant': 0.016686, 'loss_word': 0.026639}\n",
      "   59 | 0.000044 | 160000/160635 | 6.8961 | 5.5522 ||\n",
      "val: {'recall': 0.997256, 'recall_grapheme': 0.99657, 'recall_vowel': 0.997796, 'recall_consonant': 0.998089, 'recall_word': 0.995629, 'acc_grapheme': 0.99612, 'acc_vowel': 0.998184, 'acc_consonant': 0.99898, 'acc_word': 0.995548, 'loss_grapheme': 0.031629, 'loss_vowel': 0.024684, 'loss_consonant': 0.018004, 'loss_word': 0.027627}\n",
      "   60 | 0.000040 | 160000/160635 | 0.3540 | 6.1468 ||\n",
      "val: {'recall': 0.997337, 'recall_grapheme': 0.996866, 'recall_vowel': 0.998161, 'recall_consonant': 0.997454, 'recall_word': 0.995892, 'acc_grapheme': 0.996319, 'acc_vowel': 0.998284, 'acc_consonant': 0.998906, 'acc_word': 0.995871, 'loss_grapheme': 0.028428, 'loss_vowel': 0.022712, 'loss_consonant': 0.017418, 'loss_word': 0.026515}\n",
      "   61 | 0.000036 | 160000/160635 | 0.3789 | 5.0120 ||\n",
      "val: {'recall': 0.997522, 'recall_grapheme': 0.99681, 'recall_vowel': 0.998368, 'recall_consonant': 0.998098, 'recall_word': 0.996337, 'acc_grapheme': 0.996692, 'acc_vowel': 0.998458, 'acc_consonant': 0.999005, 'acc_word': 0.996319, 'loss_grapheme': 0.016139, 'loss_vowel': 0.009432, 'loss_consonant': 0.006571, 'loss_word': 0.017516}\n",
      "   62 | 0.000032 | 160000/160635 | 5.9163 | 5.5751 ||\n",
      "val: {'recall': 0.997173, 'recall_grapheme': 0.996578, 'recall_vowel': 0.997883, 'recall_consonant': 0.997652, 'recall_word': 0.995524, 'acc_grapheme': 0.99607, 'acc_vowel': 0.998284, 'acc_consonant': 0.999055, 'acc_word': 0.995498, 'loss_grapheme': 0.024905, 'loss_vowel': 0.018952, 'loss_consonant': 0.014091, 'loss_word': 0.024125}\n",
      "   63 | 0.000029 | 160000/160635 | 8.4425 | 5.6151 |||\n",
      "val: {'recall': 0.996801, 'recall_grapheme': 0.996069, 'recall_vowel': 0.997842, 'recall_consonant': 0.997225, 'recall_word': 0.9951, 'acc_grapheme': 0.995523, 'acc_vowel': 0.99801, 'acc_consonant': 0.998732, 'acc_word': 0.995075, 'loss_grapheme': 0.047712, 'loss_vowel': 0.04185, 'loss_consonant': 0.032936, 'loss_word': 0.038236}\n",
      "   64 | 0.000025 | 160000/160635 | 3.5089 | 5.6965 ||\n",
      "val: {'recall': 0.997069, 'recall_grapheme': 0.996385, 'recall_vowel': 0.998072, 'recall_consonant': 0.997435, 'recall_word': 0.995789, 'acc_grapheme': 0.99617, 'acc_vowel': 0.998259, 'acc_consonant': 0.99893, 'acc_word': 0.995772, 'loss_grapheme': 0.027761, 'loss_vowel': 0.020913, 'loss_consonant': 0.016238, 'loss_word': 0.025937}\n",
      "   65 | 0.000022 | 160000/160635 | 16.5836 | 5.3094 ||\n",
      "val: {'recall': 0.997166, 'recall_grapheme': 0.996516, 'recall_vowel': 0.998197, 'recall_consonant': 0.997437, 'recall_word': 0.995697, 'acc_grapheme': 0.996095, 'acc_vowel': 0.998334, 'acc_consonant': 0.998955, 'acc_word': 0.995672, 'loss_grapheme': 0.028817, 'loss_vowel': 0.022503, 'loss_consonant': 0.017761, 'loss_word': 0.026858}\n",
      "   66 | 0.000019 | 160000/160635 | 2.7146 | 5.6709 ||\n",
      "val: {'recall': 0.997299, 'recall_grapheme': 0.996685, 'recall_vowel': 0.99824, 'recall_consonant': 0.997585, 'recall_word': 0.995846, 'acc_grapheme': 0.996145, 'acc_vowel': 0.998383, 'acc_consonant': 0.999005, 'acc_word': 0.995846, 'loss_grapheme': 0.028788, 'loss_vowel': 0.023376, 'loss_consonant': 0.018451, 'loss_word': 0.026665}\n",
      "   67 | 0.000016 | 160000/160635 | 15.3837 | 5.6284 |\n",
      "val: {'recall': 0.99664, 'recall_grapheme': 0.995806, 'recall_vowel': 0.997646, 'recall_consonant': 0.997303, 'recall_word': 0.994843, 'acc_grapheme': 0.995448, 'acc_vowel': 0.997886, 'acc_consonant': 0.998707, 'acc_word': 0.994802, 'loss_grapheme': 0.041963, 'loss_vowel': 0.037231, 'loss_consonant': 0.0291, 'loss_word': 0.033051}\n",
      "   68 | 0.000014 | 160000/160635 | 12.7205 | 5.3895 |\n",
      "val: {'recall': 0.997447, 'recall_grapheme': 0.996984, 'recall_vowel': 0.998312, 'recall_consonant': 0.997507, 'recall_word': 0.996059, 'acc_grapheme': 0.996642, 'acc_vowel': 0.998483, 'acc_consonant': 0.99903, 'acc_word': 0.99607, 'loss_grapheme': 0.019321, 'loss_vowel': 0.013185, 'loss_consonant': 0.010176, 'loss_word': 0.019594}\n",
      "   69 | 0.000011 | 160000/160635 | 5.0655 | 5.7712 ||\n",
      "val: {'recall': 0.997053, 'recall_grapheme': 0.996438, 'recall_vowel': 0.997939, 'recall_consonant': 0.997398, 'recall_word': 0.995599, 'acc_grapheme': 0.995996, 'acc_vowel': 0.998209, 'acc_consonant': 0.998906, 'acc_word': 0.995622, 'loss_grapheme': 0.027727, 'loss_vowel': 0.02158, 'loss_consonant': 0.017443, 'loss_word': 0.02519}\n",
      "   70 | 0.000009 | 160000/160635 | 0.1798 | 5.3549 ||\n",
      "val: {'recall': 0.997536, 'recall_grapheme': 0.997053, 'recall_vowel': 0.998464, 'recall_consonant': 0.997572, 'recall_word': 0.996345, 'acc_grapheme': 0.996667, 'acc_vowel': 0.998557, 'acc_consonant': 0.999055, 'acc_word': 0.996344, 'loss_grapheme': 0.018505, 'loss_vowel': 0.012135, 'loss_consonant': 0.008993, 'loss_word': 0.019071}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   71 | 0.000007 | 160000/160635 | 3.8256 | 5.1787 ||\n",
      "val: {'recall': 0.997136, 'recall_grapheme': 0.996515, 'recall_vowel': 0.997964, 'recall_consonant': 0.99755, 'recall_word': 0.995205, 'acc_grapheme': 0.995797, 'acc_vowel': 0.997886, 'acc_consonant': 0.99893, 'acc_word': 0.995125, 'loss_grapheme': 0.039368, 'loss_vowel': 0.03063, 'loss_consonant': 0.022482, 'loss_word': 0.035292}\n",
      "   72 | 0.000006 | 160000/160635 | 13.0108 | 5.6229 ||\n",
      "val: {'recall': 0.997141, 'recall_grapheme': 0.996498, 'recall_vowel': 0.998005, 'recall_consonant': 0.997565, 'recall_word': 0.995204, 'acc_grapheme': 0.995846, 'acc_vowel': 0.998184, 'acc_consonant': 0.99898, 'acc_word': 0.9952, 'loss_grapheme': 0.056504, 'loss_vowel': 0.044744, 'loss_consonant': 0.032248, 'loss_word': 0.04706}\n",
      "   73 | 0.000004 | 160000/160635 | 12.0829 | 5.5248 |\n",
      "val: {'recall': 0.997391, 'recall_grapheme': 0.996965, 'recall_vowel': 0.998167, 'recall_consonant': 0.997467, 'recall_word': 0.995994, 'acc_grapheme': 0.996617, 'acc_vowel': 0.998408, 'acc_consonant': 0.99898, 'acc_word': 0.995996, 'loss_grapheme': 0.020874, 'loss_vowel': 0.015212, 'loss_consonant': 0.012079, 'loss_word': 0.02041}\n",
      "   74 | 0.000003 | 160000/160635 | 4.0738 | 5.3518 ||\n",
      "val: {'recall': 0.997229, 'recall_grapheme': 0.996699, 'recall_vowel': 0.998089, 'recall_consonant': 0.997429, 'recall_word': 0.995774, 'acc_grapheme': 0.99617, 'acc_vowel': 0.998334, 'acc_consonant': 0.998906, 'acc_word': 0.995747, 'loss_grapheme': 0.028984, 'loss_vowel': 0.021666, 'loss_consonant': 0.016977, 'loss_word': 0.02771}\n",
      "   75 | 0.000002 | 160000/160635 | 12.6947 | 6.0960 |\n",
      "val: {'recall': 0.997016, 'recall_grapheme': 0.996523, 'recall_vowel': 0.997643, 'recall_consonant': 0.997377, 'recall_word': 0.995159, 'acc_grapheme': 0.995821, 'acc_vowel': 0.998035, 'acc_consonant': 0.998831, 'acc_word': 0.995125, 'loss_grapheme': 0.045986, 'loss_vowel': 0.040927, 'loss_consonant': 0.031226, 'loss_word': 0.038593}\n",
      "   76 | 0.000001 | 160000/160635 | 5.4843 | 4.8665 ||\n",
      "val: {'recall': 0.997668, 'recall_grapheme': 0.997226, 'recall_vowel': 0.998568, 'recall_consonant': 0.997651, 'recall_word': 0.996584, 'acc_grapheme': 0.996866, 'acc_vowel': 0.998582, 'acc_consonant': 0.999154, 'acc_word': 0.996568, 'loss_grapheme': 0.016429, 'loss_vowel': 0.009833, 'loss_consonant': 0.007265, 'loss_word': 0.017705}\n",
      "   77 | 0.000000 | 160000/160635 | 11.1567 | 5.9281 ||\n",
      "val: {'recall': 0.997342, 'recall_grapheme': 0.996983, 'recall_vowel': 0.997945, 'recall_consonant': 0.997456, 'recall_word': 0.995929, 'acc_grapheme': 0.996568, 'acc_vowel': 0.998309, 'acc_consonant': 0.99893, 'acc_word': 0.995921, 'loss_grapheme': 0.024227, 'loss_vowel': 0.018773, 'loss_consonant': 0.015078, 'loss_word': 0.02294}\n",
      "   78 | 0.000000 | 160000/160635 | 0.2930 | 5.3798 |||\n",
      "val: {'recall': 0.99768, 'recall_grapheme': 0.99721, 'recall_vowel': 0.998727, 'recall_consonant': 0.997572, 'recall_word': 0.996525, 'acc_grapheme': 0.996941, 'acc_vowel': 0.998707, 'acc_consonant': 0.999055, 'acc_word': 0.996518, 'loss_grapheme': 0.01651, 'loss_vowel': 0.009918, 'loss_consonant': 0.007281, 'loss_word': 0.017657}\n",
      "   79 | 0.000000 | 160000/160635 | 0.4889 | 5.4123 ||\n",
      "val: {'recall': 0.997725, 'recall_grapheme': 0.997246, 'recall_vowel': 0.998418, 'recall_consonant': 0.997989, 'recall_word': 0.996291, 'acc_grapheme': 0.996841, 'acc_vowel': 0.998533, 'acc_consonant': 0.99903, 'acc_word': 0.996294, 'loss_grapheme': 0.020505, 'loss_vowel': 0.013832, 'loss_consonant': 0.010612, 'loss_word': 0.020501}\n",
      "###>>>>> saved\n",
      "CYCLE: 4\n",
      "{'recall': 0.997725, 'recall_grapheme': 0.997246, 'recall_vowel': 0.998418, 'recall_consonant': 0.997989, 'recall_word': 0.996291, 'acc_grapheme': 0.996841, 'acc_vowel': 0.998533, 'acc_consonant': 0.99903, 'acc_word': 0.996294, 'loss_grapheme': 0.020505, 'loss_vowel': 0.013832, 'loss_consonant': 0.010612, 'loss_word': 0.020501}\n",
      "    0 | 0.000060 | 160000/160635 | 13.8242 | 6.2216 |\n",
      "val: {'recall': 0.997337, 'recall_grapheme': 0.996899, 'recall_vowel': 0.998092, 'recall_consonant': 0.997459, 'recall_word': 0.995824, 'acc_grapheme': 0.996393, 'acc_vowel': 0.998383, 'acc_consonant': 0.99893, 'acc_word': 0.995821, 'loss_grapheme': 0.031726, 'loss_vowel': 0.025828, 'loss_consonant': 0.019337, 'loss_word': 0.026687}\n",
      "    1 | 0.000120 | 160000/160635 | 16.4907 | 5.4993 |\n",
      "val: {'recall': 0.996883, 'recall_grapheme': 0.996314, 'recall_vowel': 0.997764, 'recall_consonant': 0.997137, 'recall_word': 0.995564, 'acc_grapheme': 0.995971, 'acc_vowel': 0.998309, 'acc_consonant': 0.998756, 'acc_word': 0.995548, 'loss_grapheme': 0.046065, 'loss_vowel': 0.037981, 'loss_consonant': 0.027648, 'loss_word': 0.037327}\n",
      "    2 | 0.000179 | 160000/160635 | 4.7186 | 5.6518 ||\n",
      "val: {'recall': 0.997143, 'recall_grapheme': 0.996535, 'recall_vowel': 0.998177, 'recall_consonant': 0.997324, 'recall_word': 0.995557, 'acc_grapheme': 0.995946, 'acc_vowel': 0.998358, 'acc_consonant': 0.99898, 'acc_word': 0.995573, 'loss_grapheme': 0.032999, 'loss_vowel': 0.02421, 'loss_consonant': 0.019214, 'loss_word': 0.030882}\n",
      "    3 | 0.000238 | 160000/160635 | 8.4699 | 5.8355 ||\n",
      "val: {'recall': 0.996866, 'recall_grapheme': 0.996199, 'recall_vowel': 0.997744, 'recall_consonant': 0.997321, 'recall_word': 0.995335, 'acc_grapheme': 0.995697, 'acc_vowel': 0.998035, 'acc_consonant': 0.998756, 'acc_word': 0.995324, 'loss_grapheme': 0.036725, 'loss_vowel': 0.030719, 'loss_consonant': 0.024791, 'loss_word': 0.032484}\n",
      "    4 | 0.000297 | 160000/160635 | 9.1557 | 5.6999 |||\n",
      "val: {'recall': 0.996875, 'recall_grapheme': 0.996018, 'recall_vowel': 0.998059, 'recall_consonant': 0.997407, 'recall_word': 0.995154, 'acc_grapheme': 0.995772, 'acc_vowel': 0.998209, 'acc_consonant': 0.998906, 'acc_word': 0.9951, 'loss_grapheme': 0.029317, 'loss_vowel': 0.022329, 'loss_consonant': 0.017113, 'loss_word': 0.029152}\n",
      "    5 | 0.000296 | 160000/160635 | 6.5516 | 5.7694 ||\n",
      "val: {'recall': 0.996962, 'recall_grapheme': 0.996443, 'recall_vowel': 0.997687, 'recall_consonant': 0.997273, 'recall_word': 0.995014, 'acc_grapheme': 0.995647, 'acc_vowel': 0.997936, 'acc_consonant': 0.998682, 'acc_word': 0.994976, 'loss_grapheme': 0.033013, 'loss_vowel': 0.025432, 'loss_consonant': 0.018716, 'loss_word': 0.029589}\n",
      "    6 | 0.000294 | 160000/160635 | 4.3208 | 5.6494 ||\n",
      "val: {'recall': 0.996224, 'recall_grapheme': 0.995268, 'recall_vowel': 0.997603, 'recall_consonant': 0.996756, 'recall_word': 0.994415, 'acc_grapheme': 0.995001, 'acc_vowel': 0.997662, 'acc_consonant': 0.998284, 'acc_word': 0.994379, 'loss_grapheme': 0.061626, 'loss_vowel': 0.049425, 'loss_consonant': 0.036967, 'loss_word': 0.053112}\n",
      "    7 | 0.000293 | 160000/160635 | 8.6952 | 6.6705 ||\n",
      "val: {'recall': 0.996985, 'recall_grapheme': 0.996287, 'recall_vowel': 0.997599, 'recall_consonant': 0.997767, 'recall_word': 0.995162, 'acc_grapheme': 0.995946, 'acc_vowel': 0.997886, 'acc_consonant': 0.998756, 'acc_word': 0.995125, 'loss_grapheme': 0.025248, 'loss_vowel': 0.018189, 'loss_consonant': 0.014663, 'loss_word': 0.024811}\n",
      "    8 | 0.000291 | 160000/160635 | 12.1307 | 5.8018 |\n",
      "val: {'recall': 0.996642, 'recall_grapheme': 0.995553, 'recall_vowel': 0.997536, 'recall_consonant': 0.997927, 'recall_word': 0.994652, 'acc_grapheme': 0.99505, 'acc_vowel': 0.997712, 'acc_consonant': 0.998831, 'acc_word': 0.994628, 'loss_grapheme': 0.035789, 'loss_vowel': 0.028107, 'loss_consonant': 0.021278, 'loss_word': 0.032579}\n",
      "    9 | 0.000289 | 160000/160635 | 12.3953 | 5.7546 |\n",
      "val: {'recall': 0.996142, 'recall_grapheme': 0.995326, 'recall_vowel': 0.997139, 'recall_consonant': 0.996779, 'recall_word': 0.994275, 'acc_grapheme': 0.994851, 'acc_vowel': 0.997538, 'acc_consonant': 0.998334, 'acc_word': 0.994205, 'loss_grapheme': 0.057471, 'loss_vowel': 0.0485, 'loss_consonant': 0.035097, 'loss_word': 0.049266}\n",
      "   10 | 0.000286 | 160000/160635 | 7.4901 | 6.1173 ||\n",
      "val: {'recall': 0.996721, 'recall_grapheme': 0.995896, 'recall_vowel': 0.99764, 'recall_consonant': 0.997453, 'recall_word': 0.994458, 'acc_grapheme': 0.995299, 'acc_vowel': 0.99796, 'acc_consonant': 0.998408, 'acc_word': 0.994429, 'loss_grapheme': 0.040364, 'loss_vowel': 0.030868, 'loss_consonant': 0.022577, 'loss_word': 0.037289}\n",
      "   11 | 0.000284 | 160000/160635 | 0.3660 | 5.7605 ||\n",
      "val: {'recall': 0.996625, 'recall_grapheme': 0.995324, 'recall_vowel': 0.997817, 'recall_consonant': 0.998038, 'recall_word': 0.994955, 'acc_grapheme': 0.995473, 'acc_vowel': 0.998035, 'acc_consonant': 0.998756, 'acc_word': 0.994876, 'loss_grapheme': 0.030141, 'loss_vowel': 0.021492, 'loss_consonant': 0.018181, 'loss_word': 0.028343}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   12 | 0.000281 | 160000/160635 | 0.6086 | 5.4475 ||\n",
      "val: {'recall': 0.996957, 'recall_grapheme': 0.996197, 'recall_vowel': 0.99735, 'recall_consonant': 0.998085, 'recall_word': 0.995213, 'acc_grapheme': 0.995772, 'acc_vowel': 0.998135, 'acc_consonant': 0.998856, 'acc_word': 0.995224, 'loss_grapheme': 0.027646, 'loss_vowel': 0.019468, 'loss_consonant': 0.015116, 'loss_word': 0.026632}\n",
      "   13 | 0.000278 | 160000/160635 | 15.2891 | 6.0466 ||\n",
      "val: {'recall': 0.99703, 'recall_grapheme': 0.996172, 'recall_vowel': 0.997988, 'recall_consonant': 0.997789, 'recall_word': 0.995326, 'acc_grapheme': 0.995871, 'acc_vowel': 0.998085, 'acc_consonant': 0.998707, 'acc_word': 0.995299, 'loss_grapheme': 0.023654, 'loss_vowel': 0.01606, 'loss_consonant': 0.012201, 'loss_word': 0.02388}\n",
      "   14 | 0.000275 | 160000/160635 | 0.2953 | 5.3545 ||\n",
      "val: {'recall': 0.996974, 'recall_grapheme': 0.996006, 'recall_vowel': 0.997939, 'recall_consonant': 0.997947, 'recall_word': 0.995408, 'acc_grapheme': 0.995697, 'acc_vowel': 0.998259, 'acc_consonant': 0.998906, 'acc_word': 0.995374, 'loss_grapheme': 0.020494, 'loss_vowel': 0.011846, 'loss_consonant': 0.009152, 'loss_word': 0.02044}\n",
      "   15 | 0.000271 | 160000/160635 | 13.3852 | 5.5375 ||\n",
      "val: {'recall': 0.996845, 'recall_grapheme': 0.995818, 'recall_vowel': 0.997856, 'recall_consonant': 0.997887, 'recall_word': 0.994847, 'acc_grapheme': 0.995349, 'acc_vowel': 0.99801, 'acc_consonant': 0.998707, 'acc_word': 0.994802, 'loss_grapheme': 0.029374, 'loss_vowel': 0.021508, 'loss_consonant': 0.016713, 'loss_word': 0.026326}\n",
      "   16 | 0.000268 | 160000/160635 | 0.3884 | 5.6543 ||\n",
      "val: {'recall': 0.996957, 'recall_grapheme': 0.996496, 'recall_vowel': 0.997846, 'recall_consonant': 0.996987, 'recall_word': 0.995326, 'acc_grapheme': 0.995896, 'acc_vowel': 0.998085, 'acc_consonant': 0.998756, 'acc_word': 0.995349, 'loss_grapheme': 0.021874, 'loss_vowel': 0.013046, 'loss_consonant': 0.009508, 'loss_word': 0.022779}\n",
      "   17 | 0.000264 | 160000/160635 | 6.3639 | 5.1637 |||\n",
      "val: {'recall': 0.99717, 'recall_grapheme': 0.996372, 'recall_vowel': 0.997853, 'recall_consonant': 0.998083, 'recall_word': 0.995278, 'acc_grapheme': 0.995896, 'acc_vowel': 0.99806, 'acc_consonant': 0.998955, 'acc_word': 0.995224, 'loss_grapheme': 0.027101, 'loss_vowel': 0.019138, 'loss_consonant': 0.013215, 'loss_word': 0.026415}\n",
      "   18 | 0.000260 | 160000/160635 | 16.1156 | 5.5581 |\n",
      "val: {'recall': 0.997076, 'recall_grapheme': 0.996371, 'recall_vowel': 0.997564, 'recall_consonant': 0.997998, 'recall_word': 0.995187, 'acc_grapheme': 0.995697, 'acc_vowel': 0.998035, 'acc_consonant': 0.998856, 'acc_word': 0.9952, 'loss_grapheme': 0.027123, 'loss_vowel': 0.020122, 'loss_consonant': 0.015789, 'loss_word': 0.025662}\n",
      "   19 | 0.000256 | 160000/160635 | 0.2416 | 5.8330 ||\n",
      "val: {'recall': 0.997186, 'recall_grapheme': 0.996756, 'recall_vowel': 0.997636, 'recall_consonant': 0.997596, 'recall_word': 0.995688, 'acc_grapheme': 0.99607, 'acc_vowel': 0.998209, 'acc_consonant': 0.99903, 'acc_word': 0.995722, 'loss_grapheme': 0.024024, 'loss_vowel': 0.016156, 'loss_consonant': 0.012292, 'loss_word': 0.022971}\n",
      "   20 | 0.000252 | 160000/160635 | 0.3534 | 5.6518 ||\n",
      "val: {'recall': 0.99721, 'recall_grapheme': 0.996355, 'recall_vowel': 0.997883, 'recall_consonant': 0.998246, 'recall_word': 0.995559, 'acc_grapheme': 0.99612, 'acc_vowel': 0.997985, 'acc_consonant': 0.998906, 'acc_word': 0.995473, 'loss_grapheme': 0.025936, 'loss_vowel': 0.018542, 'loss_consonant': 0.014099, 'loss_word': 0.024971}\n",
      "   21 | 0.000247 | 160000/160635 | 15.0579 | 6.1605 ||\n",
      "val: {'recall': 0.996772, 'recall_grapheme': 0.996327, 'recall_vowel': 0.998232, 'recall_consonant': 0.996202, 'recall_word': 0.995266, 'acc_grapheme': 0.995896, 'acc_vowel': 0.998159, 'acc_consonant': 0.998682, 'acc_word': 0.995249, 'loss_grapheme': 0.027337, 'loss_vowel': 0.020506, 'loss_consonant': 0.016735, 'loss_word': 0.025913}\n",
      "   22 | 0.000243 | 160000/160635 | 7.0833 | 5.4389 ||\n",
      "val: {'recall': 0.996915, 'recall_grapheme': 0.996089, 'recall_vowel': 0.997983, 'recall_consonant': 0.997498, 'recall_word': 0.995126, 'acc_grapheme': 0.995523, 'acc_vowel': 0.998085, 'acc_consonant': 0.998831, 'acc_word': 0.995075, 'loss_grapheme': 0.036184, 'loss_vowel': 0.026585, 'loss_consonant': 0.019598, 'loss_word': 0.031436}\n",
      "   23 | 0.000238 | 160000/160635 | 9.2142 | 5.6155 ||\n",
      "val: {'recall': 0.996681, 'recall_grapheme': 0.995541, 'recall_vowel': 0.997899, 'recall_consonant': 0.997743, 'recall_word': 0.994647, 'acc_grapheme': 0.995299, 'acc_vowel': 0.997985, 'acc_consonant': 0.998582, 'acc_word': 0.994628, 'loss_grapheme': 0.03657, 'loss_vowel': 0.026973, 'loss_consonant': 0.02217, 'loss_word': 0.033821}\n",
      "   24 | 0.000233 | 160000/160635 | 0.3419 | 5.7339 ||\n",
      "val: {'recall': 0.997373, 'recall_grapheme': 0.996884, 'recall_vowel': 0.998205, 'recall_consonant': 0.997518, 'recall_word': 0.99569, 'acc_grapheme': 0.996344, 'acc_vowel': 0.998458, 'acc_consonant': 0.998856, 'acc_word': 0.995747, 'loss_grapheme': 0.018815, 'loss_vowel': 0.011, 'loss_consonant': 0.008831, 'loss_word': 0.019646}\n",
      "   25 | 0.000228 | 160000/160635 | 1.4523 | 5.6976 |||\n",
      "val: {'recall': 0.997265, 'recall_grapheme': 0.996659, 'recall_vowel': 0.998341, 'recall_consonant': 0.9974, 'recall_word': 0.995304, 'acc_grapheme': 0.995896, 'acc_vowel': 0.998408, 'acc_consonant': 0.998781, 'acc_word': 0.995299, 'loss_grapheme': 0.034032, 'loss_vowel': 0.024817, 'loss_consonant': 0.019608, 'loss_word': 0.031913}\n",
      "   26 | 0.000223 | 160000/160635 | 9.2124 | 5.2081 ||\n",
      "val: {'recall': 0.996942, 'recall_grapheme': 0.996314, 'recall_vowel': 0.998152, 'recall_consonant': 0.99699, 'recall_word': 0.995417, 'acc_grapheme': 0.995996, 'acc_vowel': 0.998184, 'acc_consonant': 0.998831, 'acc_word': 0.995423, 'loss_grapheme': 0.025677, 'loss_vowel': 0.018606, 'loss_consonant': 0.014461, 'loss_word': 0.024694}\n",
      "   27 | 0.000218 | 160000/160635 | 0.2880 | 5.5114 ||\n",
      "val: {'recall': 0.997234, 'recall_grapheme': 0.996605, 'recall_vowel': 0.99746, 'recall_consonant': 0.998265, 'recall_word': 0.995896, 'acc_grapheme': 0.996393, 'acc_vowel': 0.998259, 'acc_consonant': 0.998955, 'acc_word': 0.995921, 'loss_grapheme': 0.020526, 'loss_vowel': 0.013675, 'loss_consonant': 0.010029, 'loss_word': 0.019902}\n",
      "   28 | 0.000213 | 160000/160635 | 0.1729 | 5.6201 ||\n",
      "val: {'recall': 0.997584, 'recall_grapheme': 0.996719, 'recall_vowel': 0.998251, 'recall_consonant': 0.998646, 'recall_word': 0.996046, 'acc_grapheme': 0.996344, 'acc_vowel': 0.998533, 'acc_consonant': 0.999105, 'acc_word': 0.995996, 'loss_grapheme': 0.023025, 'loss_vowel': 0.015454, 'loss_consonant': 0.011873, 'loss_word': 0.022355}\n",
      "   29 | 0.000207 | 160000/160635 | 3.5213 | 5.9593 |||\n",
      "val: {'recall': 0.997407, 'recall_grapheme': 0.99682, 'recall_vowel': 0.997986, 'recall_consonant': 0.998004, 'recall_word': 0.99574, 'acc_grapheme': 0.996443, 'acc_vowel': 0.998284, 'acc_consonant': 0.99893, 'acc_word': 0.995672, 'loss_grapheme': 0.023367, 'loss_vowel': 0.015717, 'loss_consonant': 0.012423, 'loss_word': 0.023468}\n",
      "   30 | 0.000202 | 160000/160635 | 7.3371 | 5.6575 ||\n",
      "val: {'recall': 0.997473, 'recall_grapheme': 0.996919, 'recall_vowel': 0.997711, 'recall_consonant': 0.998342, 'recall_word': 0.995583, 'acc_grapheme': 0.996344, 'acc_vowel': 0.99811, 'acc_consonant': 0.99908, 'acc_word': 0.995598, 'loss_grapheme': 0.020213, 'loss_vowel': 0.012425, 'loss_consonant': 0.009634, 'loss_word': 0.020799}\n",
      "   31 | 0.000196 | 160000/160635 | 6.8095 | 5.6876 ||\n",
      "val: {'recall': 0.997155, 'recall_grapheme': 0.99621, 'recall_vowel': 0.998117, 'recall_consonant': 0.998082, 'recall_word': 0.995361, 'acc_grapheme': 0.995946, 'acc_vowel': 0.998334, 'acc_consonant': 0.998906, 'acc_word': 0.995374, 'loss_grapheme': 0.0313, 'loss_vowel': 0.024204, 'loss_consonant': 0.019869, 'loss_word': 0.029586}\n",
      "   32 | 0.000191 | 160000/160635 | 14.7412 | 5.6809 |\n",
      "val: {'recall': 0.997167, 'recall_grapheme': 0.996557, 'recall_vowel': 0.997945, 'recall_consonant': 0.997607, 'recall_word': 0.995616, 'acc_grapheme': 0.996045, 'acc_vowel': 0.998309, 'acc_consonant': 0.998881, 'acc_word': 0.995598, 'loss_grapheme': 0.029543, 'loss_vowel': 0.022863, 'loss_consonant': 0.017883, 'loss_word': 0.025753}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   33 | 0.000185 | 160000/160635 | 0.5642 | 5.5665 ||\n",
      "val: {'recall': 0.996989, 'recall_grapheme': 0.996197, 'recall_vowel': 0.997905, 'recall_consonant': 0.997658, 'recall_word': 0.99546, 'acc_grapheme': 0.995722, 'acc_vowel': 0.998284, 'acc_consonant': 0.99898, 'acc_word': 0.995423, 'loss_grapheme': 0.027371, 'loss_vowel': 0.019772, 'loss_consonant': 0.016444, 'loss_word': 0.024955}\n",
      "   34 | 0.000179 | 160000/160635 | 0.4136 | 5.7351 ||\n",
      "val: {'recall': 0.997446, 'recall_grapheme': 0.996802, 'recall_vowel': 0.997651, 'recall_consonant': 0.998528, 'recall_word': 0.996025, 'acc_grapheme': 0.996418, 'acc_vowel': 0.998209, 'acc_consonant': 0.99893, 'acc_word': 0.995946, 'loss_grapheme': 0.020784, 'loss_vowel': 0.014078, 'loss_consonant': 0.010538, 'loss_word': 0.01982}\n",
      "   35 | 0.000173 | 160000/160635 | 6.4616 | 5.7280 |||\n",
      "val: {'recall': 0.99743, 'recall_grapheme': 0.996651, 'recall_vowel': 0.998174, 'recall_consonant': 0.998243, 'recall_word': 0.99594, 'acc_grapheme': 0.996294, 'acc_vowel': 0.998358, 'acc_consonant': 0.999105, 'acc_word': 0.995871, 'loss_grapheme': 0.020885, 'loss_vowel': 0.013453, 'loss_consonant': 0.010309, 'loss_word': 0.020844}\n",
      "   36 | 0.000168 | 160000/160635 | 4.5091 | 5.5799 ||\n",
      "val: {'recall': 0.996804, 'recall_grapheme': 0.995999, 'recall_vowel': 0.997841, 'recall_consonant': 0.997376, 'recall_word': 0.995368, 'acc_grapheme': 0.995622, 'acc_vowel': 0.998234, 'acc_consonant': 0.998806, 'acc_word': 0.995324, 'loss_grapheme': 0.030344, 'loss_vowel': 0.023524, 'loss_consonant': 0.018934, 'loss_word': 0.027995}\n",
      "   37 | 0.000162 | 160000/160635 | 5.3150 | 5.8762 |||\n",
      "val: {'recall': 0.997358, 'recall_grapheme': 0.996721, 'recall_vowel': 0.998059, 'recall_consonant': 0.997931, 'recall_word': 0.995635, 'acc_grapheme': 0.996244, 'acc_vowel': 0.998433, 'acc_consonant': 0.99893, 'acc_word': 0.995622, 'loss_grapheme': 0.025066, 'loss_vowel': 0.017317, 'loss_consonant': 0.013493, 'loss_word': 0.024677}\n",
      "   38 | 0.000156 | 160000/160635 | 6.6348 | 5.2739 ||\n",
      "val: {'recall': 0.996929, 'recall_grapheme': 0.996404, 'recall_vowel': 0.997636, 'recall_consonant': 0.997272, 'recall_word': 0.995424, 'acc_grapheme': 0.995996, 'acc_vowel': 0.998209, 'acc_consonant': 0.998781, 'acc_word': 0.995374, 'loss_grapheme': 0.03087, 'loss_vowel': 0.024229, 'loss_consonant': 0.019193, 'loss_word': 0.028877}\n",
      "   39 | 0.000150 | 160000/160635 | 5.8434 | 5.6309 |||\n",
      "val: {'recall': 0.997062, 'recall_grapheme': 0.996072, 'recall_vowel': 0.998184, 'recall_consonant': 0.99792, 'recall_word': 0.99523, 'acc_grapheme': 0.995821, 'acc_vowel': 0.998209, 'acc_consonant': 0.998732, 'acc_word': 0.9952, 'loss_grapheme': 0.032467, 'loss_vowel': 0.02602, 'loss_consonant': 0.020482, 'loss_word': 0.030409}\n",
      "   40 | 0.000144 | 160000/160635 | 4.5441 | 4.4094 ||\n",
      "val: {'recall': 0.996924, 'recall_grapheme': 0.996031, 'recall_vowel': 0.99776, 'recall_consonant': 0.997873, 'recall_word': 0.9951, 'acc_grapheme': 0.995349, 'acc_vowel': 0.997786, 'acc_consonant': 0.998632, 'acc_word': 0.994976, 'loss_grapheme': 0.071971, 'loss_vowel': 0.055483, 'loss_consonant': 0.037752, 'loss_word': 0.060143}\n",
      "   41 | 0.000138 | 160000/160635 | 0.4095 | 5.5469 ||\n",
      "val: {'recall': 0.997286, 'recall_grapheme': 0.996443, 'recall_vowel': 0.998142, 'recall_consonant': 0.998114, 'recall_word': 0.995564, 'acc_grapheme': 0.99602, 'acc_vowel': 0.998284, 'acc_consonant': 0.998955, 'acc_word': 0.995548, 'loss_grapheme': 0.02775, 'loss_vowel': 0.020977, 'loss_consonant': 0.016724, 'loss_word': 0.026193}\n",
      "   42 | 0.000132 | 160000/160635 | 0.4594 | 5.8179 ||\n",
      "val: {'recall': 0.997086, 'recall_grapheme': 0.996402, 'recall_vowel': 0.997858, 'recall_consonant': 0.997682, 'recall_word': 0.995397, 'acc_grapheme': 0.995971, 'acc_vowel': 0.998159, 'acc_consonant': 0.998881, 'acc_word': 0.995399, 'loss_grapheme': 0.03499, 'loss_vowel': 0.030577, 'loss_consonant': 0.024331, 'loss_word': 0.029466}\n",
      "   43 | 0.000127 | 160000/160635 | 7.4803 | 5.8106 ||\n",
      "val: {'recall': 0.997561, 'recall_grapheme': 0.996934, 'recall_vowel': 0.998159, 'recall_consonant': 0.998216, 'recall_word': 0.995624, 'acc_grapheme': 0.996344, 'acc_vowel': 0.998259, 'acc_consonant': 0.999005, 'acc_word': 0.995598, 'loss_grapheme': 0.027745, 'loss_vowel': 0.021033, 'loss_consonant': 0.016436, 'loss_word': 0.026817}\n",
      "   44 | 0.000121 | 160000/160635 | 0.4132 | 5.9138 |||\n",
      "val: {'recall': 0.997269, 'recall_grapheme': 0.996716, 'recall_vowel': 0.998034, 'recall_consonant': 0.997609, 'recall_word': 0.995872, 'acc_grapheme': 0.99617, 'acc_vowel': 0.998234, 'acc_consonant': 0.999055, 'acc_word': 0.995871, 'loss_grapheme': 0.026422, 'loss_vowel': 0.020107, 'loss_consonant': 0.016695, 'loss_word': 0.025133}\n",
      "   45 | 0.000115 | 160000/160635 | 14.9582 | 5.3481 |\n",
      "val: {'recall': 0.996963, 'recall_grapheme': 0.996179, 'recall_vowel': 0.997921, 'recall_consonant': 0.997574, 'recall_word': 0.995218, 'acc_grapheme': 0.995672, 'acc_vowel': 0.998135, 'acc_consonant': 0.998906, 'acc_word': 0.995249, 'loss_grapheme': 0.032843, 'loss_vowel': 0.027118, 'loss_consonant': 0.021073, 'loss_word': 0.02902}\n",
      "   46 | 0.000109 | 160000/160635 | 4.1549 | 5.9096 ||\n",
      "val: {'recall': 0.997004, 'recall_grapheme': 0.99637, 'recall_vowel': 0.997846, 'recall_consonant': 0.997429, 'recall_word': 0.995055, 'acc_grapheme': 0.995448, 'acc_vowel': 0.99801, 'acc_consonant': 0.998756, 'acc_word': 0.995075, 'loss_grapheme': 0.04947, 'loss_vowel': 0.043426, 'loss_consonant': 0.032819, 'loss_word': 0.040209}\n",
      "   47 | 0.000104 | 160000/160635 | 0.5174 | 5.3455 ||\n",
      "val: {'recall': 0.997386, 'recall_grapheme': 0.997048, 'recall_vowel': 0.998039, 'recall_consonant': 0.99741, 'recall_word': 0.995747, 'acc_grapheme': 0.996393, 'acc_vowel': 0.998234, 'acc_consonant': 0.99893, 'acc_word': 0.995722, 'loss_grapheme': 0.02139, 'loss_vowel': 0.014238, 'loss_consonant': 0.010899, 'loss_word': 0.022228}\n",
      "   48 | 0.000098 | 160000/160635 | 2.5449 | 5.5721 ||\n",
      "val: {'recall': 0.997536, 'recall_grapheme': 0.997159, 'recall_vowel': 0.998251, 'recall_consonant': 0.997573, 'recall_word': 0.99596, 'acc_grapheme': 0.996393, 'acc_vowel': 0.998358, 'acc_consonant': 0.999005, 'acc_word': 0.995971, 'loss_grapheme': 0.027223, 'loss_vowel': 0.020398, 'loss_consonant': 0.015707, 'loss_word': 0.026138}\n",
      "   49 | 0.000093 | 160000/160635 | 13.1638 | 5.6108 ||\n",
      "val: {'recall': 0.997623, 'recall_grapheme': 0.996843, 'recall_vowel': 0.998242, 'recall_consonant': 0.998565, 'recall_word': 0.995838, 'acc_grapheme': 0.996493, 'acc_vowel': 0.998309, 'acc_consonant': 0.999055, 'acc_word': 0.995797, 'loss_grapheme': 0.021104, 'loss_vowel': 0.01341, 'loss_consonant': 0.010519, 'loss_word': 0.021653}\n",
      "   50 | 0.000087 | 160000/160635 | 5.9572 | 5.3786 ||\n",
      "val: {'recall': 0.997552, 'recall_grapheme': 0.99684, 'recall_vowel': 0.998238, 'recall_consonant': 0.998292, 'recall_word': 0.995834, 'acc_grapheme': 0.996294, 'acc_vowel': 0.998433, 'acc_consonant': 0.999105, 'acc_word': 0.995772, 'loss_grapheme': 0.021056, 'loss_vowel': 0.013344, 'loss_consonant': 0.009814, 'loss_word': 0.021553}\n",
      "   51 | 0.000082 | 160000/160635 | 14.2697 | 6.0575 |\n",
      "val: {'recall': 0.996637, 'recall_grapheme': 0.995973, 'recall_vowel': 0.997321, 'recall_consonant': 0.997279, 'recall_word': 0.995107, 'acc_grapheme': 0.995647, 'acc_vowel': 0.997861, 'acc_consonant': 0.998557, 'acc_word': 0.99515, 'loss_grapheme': 0.046025, 'loss_vowel': 0.04214, 'loss_consonant': 0.032997, 'loss_word': 0.034895}\n",
      "   52 | 0.000077 | 160000/160635 | 0.3505 | 4.9399 ||\n",
      "val: {'recall': 0.997326, 'recall_grapheme': 0.99664, 'recall_vowel': 0.997891, 'recall_consonant': 0.998131, 'recall_word': 0.995761, 'acc_grapheme': 0.996095, 'acc_vowel': 0.998184, 'acc_consonant': 0.99903, 'acc_word': 0.995722, 'loss_grapheme': 0.022874, 'loss_vowel': 0.015425, 'loss_consonant': 0.012339, 'loss_word': 0.022632}\n",
      "   53 | 0.000072 | 160000/160635 | 12.8646 | 5.7844 |\n",
      "val: {'recall': 0.997131, 'recall_grapheme': 0.996427, 'recall_vowel': 0.997624, 'recall_consonant': 0.998046, 'recall_word': 0.995344, 'acc_grapheme': 0.995896, 'acc_vowel': 0.998159, 'acc_consonant': 0.998856, 'acc_word': 0.995349, 'loss_grapheme': 0.031973, 'loss_vowel': 0.025892, 'loss_consonant': 0.020296, 'loss_word': 0.029373}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   54 | 0.000067 | 160000/160635 | 12.4477 | 5.5955 ||\n",
      "val: {'recall': 0.997802, 'recall_grapheme': 0.997165, 'recall_vowel': 0.998296, 'recall_consonant': 0.99858, 'recall_word': 0.996048, 'acc_grapheme': 0.996642, 'acc_vowel': 0.998383, 'acc_consonant': 0.999005, 'acc_word': 0.996045, 'loss_grapheme': 0.028503, 'loss_vowel': 0.021739, 'loss_consonant': 0.016533, 'loss_word': 0.026119}\n",
      "###>>>>> saved\n",
      "   55 | 0.000062 | 160000/160635 | 6.3102 | 5.7337 |||\n",
      "val: {'recall': 0.997353, 'recall_grapheme': 0.996639, 'recall_vowel': 0.998037, 'recall_consonant': 0.998095, 'recall_word': 0.995913, 'acc_grapheme': 0.996294, 'acc_vowel': 0.998259, 'acc_consonant': 0.99898, 'acc_word': 0.995896, 'loss_grapheme': 0.021151, 'loss_vowel': 0.013977, 'loss_consonant': 0.010984, 'loss_word': 0.021681}\n",
      "   56 | 0.000057 | 160000/160635 | 13.6307 | 5.3485 ||\n",
      "val: {'recall': 0.997341, 'recall_grapheme': 0.996861, 'recall_vowel': 0.998059, 'recall_consonant': 0.997583, 'recall_word': 0.995731, 'acc_grapheme': 0.996269, 'acc_vowel': 0.998284, 'acc_consonant': 0.998955, 'acc_word': 0.995722, 'loss_grapheme': 0.021163, 'loss_vowel': 0.01434, 'loss_consonant': 0.011215, 'loss_word': 0.0215}\n",
      "   57 | 0.000053 | 160000/160635 | 15.8077 | 6.2781 ||\n",
      "val: {'recall': 0.997248, 'recall_grapheme': 0.996783, 'recall_vowel': 0.997917, 'recall_consonant': 0.99751, 'recall_word': 0.995578, 'acc_grapheme': 0.996145, 'acc_vowel': 0.998309, 'acc_consonant': 0.998856, 'acc_word': 0.995622, 'loss_grapheme': 0.031311, 'loss_vowel': 0.025837, 'loss_consonant': 0.020697, 'loss_word': 0.027408}\n",
      "   58 | 0.000048 | 160000/160635 | 0.4686 | 6.1337 |||\n",
      "val: {'recall': 0.997651, 'recall_grapheme': 0.997312, 'recall_vowel': 0.998166, 'recall_consonant': 0.997815, 'recall_word': 0.996427, 'acc_grapheme': 0.996891, 'acc_vowel': 0.998433, 'acc_consonant': 0.999179, 'acc_word': 0.996393, 'loss_grapheme': 0.017192, 'loss_vowel': 0.010665, 'loss_consonant': 0.007818, 'loss_word': 0.018546}\n",
      "   59 | 0.000044 | 160000/160635 | 0.3220 | 5.6151 ||\n",
      "val: {'recall': 0.997381, 'recall_grapheme': 0.996669, 'recall_vowel': 0.998034, 'recall_consonant': 0.998154, 'recall_word': 0.995699, 'acc_grapheme': 0.996095, 'acc_vowel': 0.998209, 'acc_consonant': 0.999005, 'acc_word': 0.995697, 'loss_grapheme': 0.025079, 'loss_vowel': 0.019079, 'loss_consonant': 0.015211, 'loss_word': 0.023333}\n",
      "   60 | 0.000040 | 160000/160635 | 6.1986 | 5.4526 ||\n",
      "val: {'recall': 0.997618, 'recall_grapheme': 0.997096, 'recall_vowel': 0.998133, 'recall_consonant': 0.998148, 'recall_word': 0.996357, 'acc_grapheme': 0.996592, 'acc_vowel': 0.998458, 'acc_consonant': 0.999129, 'acc_word': 0.996344, 'loss_grapheme': 0.025759, 'loss_vowel': 0.018924, 'loss_consonant': 0.014335, 'loss_word': 0.02602}\n",
      "   61 | 0.000036 | 160000/160635 | 4.7680 | 6.0602 |||\n",
      "val: {'recall': 0.99724, 'recall_grapheme': 0.996526, 'recall_vowel': 0.997942, 'recall_consonant': 0.997966, 'recall_word': 0.995408, 'acc_grapheme': 0.995971, 'acc_vowel': 0.998085, 'acc_consonant': 0.998856, 'acc_word': 0.995423, 'loss_grapheme': 0.031879, 'loss_vowel': 0.025071, 'loss_consonant': 0.019495, 'loss_word': 0.029218}\n",
      "   62 | 0.000032 | 160000/160635 | 0.2177 | 5.4031 |||\n",
      "val: {'recall': 0.997514, 'recall_grapheme': 0.996952, 'recall_vowel': 0.998071, 'recall_consonant': 0.998083, 'recall_word': 0.996122, 'acc_grapheme': 0.996493, 'acc_vowel': 0.998358, 'acc_consonant': 0.99903, 'acc_word': 0.996095, 'loss_grapheme': 0.024926, 'loss_vowel': 0.018159, 'loss_consonant': 0.013991, 'loss_word': 0.023652}\n",
      "   63 | 0.000029 | 160000/160635 | 11.8108 | 5.6427 |\n",
      "val: {'recall': 0.997443, 'recall_grapheme': 0.9968, 'recall_vowel': 0.998086, 'recall_consonant': 0.998085, 'recall_word': 0.995754, 'acc_grapheme': 0.996195, 'acc_vowel': 0.998309, 'acc_consonant': 0.999055, 'acc_word': 0.995722, 'loss_grapheme': 0.028215, 'loss_vowel': 0.022017, 'loss_consonant': 0.018317, 'loss_word': 0.026247}\n",
      "   64 | 0.000025 | 160000/160635 | 0.2350 | 5.7211 ||\n",
      "val: {'recall': 0.997488, 'recall_grapheme': 0.996932, 'recall_vowel': 0.998028, 'recall_consonant': 0.998061, 'recall_word': 0.995994, 'acc_grapheme': 0.996443, 'acc_vowel': 0.998309, 'acc_consonant': 0.999055, 'acc_word': 0.995971, 'loss_grapheme': 0.030792, 'loss_vowel': 0.024072, 'loss_consonant': 0.019213, 'loss_word': 0.029329}\n",
      "   65 | 0.000022 | 160000/160635 | 11.1926 | 5.2507 ||\n",
      "val: {'recall': 0.997573, 'recall_grapheme': 0.996875, 'recall_vowel': 0.99823, 'recall_consonant': 0.998313, 'recall_word': 0.996175, 'acc_grapheme': 0.996493, 'acc_vowel': 0.998383, 'acc_consonant': 0.999179, 'acc_word': 0.996145, 'loss_grapheme': 0.023105, 'loss_vowel': 0.016303, 'loss_consonant': 0.012595, 'loss_word': 0.023069}\n",
      "   66 | 0.000019 | 160000/160635 | 0.2053 | 5.8429 ||\n",
      "val: {'recall': 0.997479, 'recall_grapheme': 0.996757, 'recall_vowel': 0.99822, 'recall_consonant': 0.998182, 'recall_word': 0.996156, 'acc_grapheme': 0.996493, 'acc_vowel': 0.998383, 'acc_consonant': 0.999129, 'acc_word': 0.996095, 'loss_grapheme': 0.026941, 'loss_vowel': 0.02008, 'loss_consonant': 0.015703, 'loss_word': 0.026109}\n",
      "   67 | 0.000017 | 136960/160635 | 0.3392 | 5.1627 ||"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-6df1386d1bbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-194d18ae66fb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcycle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_cycles\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CYCLE:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtrain_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         scheduler = WarmupCyclicalLR(\n\u001b[1;32m     17\u001b[0m             \"cos\", args.base_lr, args.num_epochs, iters_per_epoch=len(train_loader), warmup_epochs=args.warmup_epochs)\n",
      "\u001b[0;32m<ipython-input-31-194d18ae66fb>\u001b[0m in \u001b[0;36mtrain_cycle\u001b[0;34m(args, model, optimizer, lr_scheduler)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;31m#loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/apex/amp/handle.py\u001b[0m in \u001b[0;36mscale_loss\u001b[0;34m(loss, optimizers, loss_id, model, delay_unscale, delay_overflow_check)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;31m# For future fused optimizers that enable sync-free dynamic loss scaling,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# should_skip will always be False.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mshould_skip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdelay_overflow_check\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mloss_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_skip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/apex/amp/scaler.py\u001b[0m in \u001b[0;36mupdate_scale\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# If the fused kernel is available, we only need one D2H memcopy and sync.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mLossScaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_fused_kernel\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_overflow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_overflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_overflow_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_overflow\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(args, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CYCLE: 1\n",
      "{'recall': 0.996308, 'recall_grapheme': 0.995273, 'recall_vowel': 0.997201, 'recall_consonant': 0.997485, 'recall_word': 0.993975, 'acc_grapheme': 0.994603, 'acc_vowel': 0.997289, 'acc_consonant': 0.998159, 'acc_word': 0.993956, 'loss_grapheme': 0.041434, 'loss_vowel': 0.029393, 'loss_consonant': 0.021052, 'loss_word': 0.035974}\n",
      "    0 | 0.000060 | 160000/160635 | 0.2920 | 5.5432 ||\n",
      "val: {'recall': 0.995986, 'recall_grapheme': 0.995068, 'recall_vowel': 0.997316, 'recall_consonant': 0.996493, 'recall_word': 0.993781, 'acc_grapheme': 0.99423, 'acc_vowel': 0.997189, 'acc_consonant': 0.998159, 'acc_word': 0.993832, 'loss_grapheme': 0.075465, 'loss_vowel': 0.059914, 'loss_consonant': 0.04144, 'loss_word': 0.053402}\n",
      "    1 | 0.000120 | 160000/160635 | 8.0131 | 5.9631 ||\n",
      "val: {'recall': 0.994908, 'recall_grapheme': 0.993831, 'recall_vowel': 0.996551, 'recall_consonant': 0.995419, 'recall_word': 0.993102, 'acc_grapheme': 0.992762, 'acc_vowel': 0.996393, 'acc_consonant': 0.997463, 'acc_word': 0.99316, 'loss_grapheme': 0.176614, 'loss_vowel': 0.142537, 'loss_consonant': 0.088436, 'loss_word': 0.11241}\n",
      "    2 | 0.000179 | 160000/160635 | 0.1732 | 5.9475 ||\n",
      "val: {'recall': 0.995882, 'recall_grapheme': 0.994701, 'recall_vowel': 0.996803, 'recall_consonant': 0.997323, 'recall_word': 0.993429, 'acc_grapheme': 0.993682, 'acc_vowel': 0.99699, 'acc_consonant': 0.997985, 'acc_word': 0.993483, 'loss_grapheme': 0.074381, 'loss_vowel': 0.057001, 'loss_consonant': 0.039057, 'loss_word': 0.055967}\n",
      "    3 | 0.000238 | 160000/160635 | 8.7027 | 5.5610 ||\n",
      "val: {'recall': 0.995126, 'recall_grapheme': 0.993991, 'recall_vowel': 0.996819, 'recall_consonant': 0.995704, 'recall_word': 0.993657, 'acc_grapheme': 0.993508, 'acc_vowel': 0.99699, 'acc_consonant': 0.997911, 'acc_word': 0.993633, 'loss_grapheme': 0.110065, 'loss_vowel': 0.084925, 'loss_consonant': 0.056431, 'loss_word': 0.07788}\n",
      "    4 | 0.000297 | 160000/160635 | 5.9917 | 5.9049 |||\n",
      "val: {'recall': 0.994122, 'recall_grapheme': 0.992009, 'recall_vowel': 0.996379, 'recall_consonant': 0.996092, 'recall_word': 0.992397, 'acc_grapheme': 0.991817, 'acc_vowel': 0.996045, 'acc_consonant': 0.997214, 'acc_word': 0.992364, 'loss_grapheme': 0.176269, 'loss_vowel': 0.142994, 'loss_consonant': 0.091283, 'loss_word': 0.114991}\n",
      "    5 | 0.000296 | 160000/160635 | 0.5533 | 5.6067 ||\n",
      "val: {'recall': 0.995805, 'recall_grapheme': 0.994791, 'recall_vowel': 0.996474, 'recall_consonant': 0.997163, 'recall_word': 0.993624, 'acc_grapheme': 0.994055, 'acc_vowel': 0.99709, 'acc_consonant': 0.998184, 'acc_word': 0.993682, 'loss_grapheme': 0.045728, 'loss_vowel': 0.034023, 'loss_consonant': 0.024576, 'loss_word': 0.037374}\n",
      "    6 | 0.000294 | 160000/160635 | 0.4095 | 5.6448 ||\n",
      "val: {'recall': 0.996011, 'recall_grapheme': 0.994937, 'recall_vowel': 0.997311, 'recall_consonant': 0.99686, 'recall_word': 0.994012, 'acc_grapheme': 0.994652, 'acc_vowel': 0.997488, 'acc_consonant': 0.99801, 'acc_word': 0.99408, 'loss_grapheme': 0.056927, 'loss_vowel': 0.043763, 'loss_consonant': 0.031793, 'loss_word': 0.046373}\n",
      "    7 | 0.000293 | 160000/160635 | 1.8368 | 5.6466 ||\n",
      "val: {'recall': 0.995821, 'recall_grapheme': 0.99464, 'recall_vowel': 0.996932, 'recall_consonant': 0.99707, 'recall_word': 0.993968, 'acc_grapheme': 0.993732, 'acc_vowel': 0.99704, 'acc_consonant': 0.99796, 'acc_word': 0.994006, 'loss_grapheme': 0.178157, 'loss_vowel': 0.134679, 'loss_consonant': 0.085596, 'loss_word': 0.117392}\n",
      "    8 | 0.000291 | 160000/160635 | 11.2649 | 5.4366 |\n",
      "val: {'recall': 0.995434, 'recall_grapheme': 0.994113, 'recall_vowel': 0.996535, 'recall_consonant': 0.996976, 'recall_word': 0.99359, 'acc_grapheme': 0.993682, 'acc_vowel': 0.99714, 'acc_consonant': 0.997886, 'acc_word': 0.993558, 'loss_grapheme': 0.091663, 'loss_vowel': 0.076477, 'loss_consonant': 0.052568, 'loss_word': 0.056936}\n",
      "    9 | 0.000289 | 160000/160635 | 2.9879 | 5.5486 |||\n",
      "val: {'recall': 0.995917, 'recall_grapheme': 0.994711, 'recall_vowel': 0.997214, 'recall_consonant': 0.997034, 'recall_word': 0.994013, 'acc_grapheme': 0.994031, 'acc_vowel': 0.997239, 'acc_consonant': 0.998284, 'acc_word': 0.994006, 'loss_grapheme': 0.078277, 'loss_vowel': 0.057007, 'loss_consonant': 0.039322, 'loss_word': 0.061238}\n",
      "   10 | 0.000286 | 160000/160635 | 3.4264 | 5.6172 ||\n",
      "val: {'recall': 0.995816, 'recall_grapheme': 0.994442, 'recall_vowel': 0.997152, 'recall_consonant': 0.997227, 'recall_word': 0.993799, 'acc_grapheme': 0.994055, 'acc_vowel': 0.997438, 'acc_consonant': 0.998209, 'acc_word': 0.993881, 'loss_grapheme': 0.055518, 'loss_vowel': 0.041369, 'loss_consonant': 0.029152, 'loss_word': 0.045644}\n",
      "   11 | 0.000284 | 160000/160635 | 4.4019 | 5.7903 ||\n",
      "val: {'recall': 0.995434, 'recall_grapheme': 0.994262, 'recall_vowel': 0.996693, 'recall_consonant': 0.996522, 'recall_word': 0.993829, 'acc_grapheme': 0.993807, 'acc_vowel': 0.997115, 'acc_consonant': 0.998085, 'acc_word': 0.993881, 'loss_grapheme': 0.088709, 'loss_vowel': 0.07023, 'loss_consonant': 0.046573, 'loss_word': 0.06156}\n",
      "   12 | 0.000281 | 160000/160635 | 6.1060 | 5.6165 ||\n",
      "val: {'recall': 0.995579, 'recall_grapheme': 0.994161, 'recall_vowel': 0.99741, 'recall_consonant': 0.996582, 'recall_word': 0.993612, 'acc_grapheme': 0.993185, 'acc_vowel': 0.99709, 'acc_consonant': 0.997985, 'acc_word': 0.993583, 'loss_grapheme': 0.114253, 'loss_vowel': 0.091396, 'loss_consonant': 0.058856, 'loss_word': 0.080076}\n",
      "   13 | 0.000278 | 160000/160635 | 10.9762 | 5.3619 |\n",
      "val: {'recall': 0.995676, 'recall_grapheme': 0.994445, 'recall_vowel': 0.997023, 'recall_consonant': 0.996792, 'recall_word': 0.994024, 'acc_grapheme': 0.994031, 'acc_vowel': 0.997339, 'acc_consonant': 0.99806, 'acc_word': 0.994055, 'loss_grapheme': 0.073983, 'loss_vowel': 0.059903, 'loss_consonant': 0.040654, 'loss_word': 0.050992}\n",
      "   14 | 0.000275 | 160000/160635 | 1.5190 | 5.7692 ||\n",
      "val: {'recall': 0.995572, 'recall_grapheme': 0.994064, 'recall_vowel': 0.996883, 'recall_consonant': 0.997277, 'recall_word': 0.993813, 'acc_grapheme': 0.994006, 'acc_vowel': 0.99714, 'acc_consonant': 0.998309, 'acc_word': 0.993832, 'loss_grapheme': 0.083603, 'loss_vowel': 0.06286, 'loss_consonant': 0.044022, 'loss_word': 0.06174}\n",
      "   15 | 0.000271 | 160000/160635 | 0.3872 | 5.7756 ||\n",
      "val: {'recall': 0.995816, 'recall_grapheme': 0.994797, 'recall_vowel': 0.99689, 'recall_consonant': 0.996779, 'recall_word': 0.994011, 'acc_grapheme': 0.994254, 'acc_vowel': 0.997189, 'acc_consonant': 0.99806, 'acc_word': 0.994006, 'loss_grapheme': 0.093531, 'loss_vowel': 0.076656, 'loss_consonant': 0.051941, 'loss_word': 0.065255}\n",
      "   16 | 0.000268 | 160000/160635 | 8.9184 | 5.3672 ||\n",
      "val: {'recall': 0.996181, 'recall_grapheme': 0.994458, 'recall_vowel': 0.997389, 'recall_consonant': 0.99842, 'recall_word': 0.993958, 'acc_grapheme': 0.994379, 'acc_vowel': 0.997165, 'acc_consonant': 0.998508, 'acc_word': 0.994006, 'loss_grapheme': 0.072784, 'loss_vowel': 0.05807, 'loss_consonant': 0.04193, 'loss_word': 0.05452}\n",
      "   17 | 0.000264 | 160000/160635 | 0.5527 | 5.3118 |||\n",
      "val: {'recall': 0.995993, 'recall_grapheme': 0.994716, 'recall_vowel': 0.997308, 'recall_consonant': 0.997231, 'recall_word': 0.993506, 'acc_grapheme': 0.994006, 'acc_vowel': 0.997214, 'acc_consonant': 0.998234, 'acc_word': 0.993558, 'loss_grapheme': 0.063986, 'loss_vowel': 0.048306, 'loss_consonant': 0.034186, 'loss_word': 0.050645}\n",
      "   18 | 0.000260 | 160000/160635 | 10.4046 | 5.9387 |\n",
      "val: {'recall': 0.996345, 'recall_grapheme': 0.995232, 'recall_vowel': 0.997274, 'recall_consonant': 0.997642, 'recall_word': 0.994213, 'acc_grapheme': 0.994553, 'acc_vowel': 0.997364, 'acc_consonant': 0.998284, 'acc_word': 0.99423, 'loss_grapheme': 0.042648, 'loss_vowel': 0.030761, 'loss_consonant': 0.023372, 'loss_word': 0.035177}\n",
      "###>>>>> saved\n",
      "   19 | 0.000256 | 160000/160635 | 0.3633 | 6.1210 |||\n",
      "val: {'recall': 0.995804, 'recall_grapheme': 0.994536, 'recall_vowel': 0.997028, 'recall_consonant': 0.997116, 'recall_word': 0.994233, 'acc_grapheme': 0.994155, 'acc_vowel': 0.997189, 'acc_consonant': 0.99806, 'acc_word': 0.994254, 'loss_grapheme': 0.060873, 'loss_vowel': 0.046799, 'loss_consonant': 0.03427, 'loss_word': 0.044163}\n",
      "   20 | 0.000252 | 160000/160635 | 5.5345 | 5.4379 ||\n",
      "val: {'recall': 0.995771, 'recall_grapheme': 0.994347, 'recall_vowel': 0.997276, 'recall_consonant': 0.997114, 'recall_word': 0.994018, 'acc_grapheme': 0.993732, 'acc_vowel': 0.997214, 'acc_consonant': 0.998184, 'acc_word': 0.994031, 'loss_grapheme': 0.115604, 'loss_vowel': 0.094662, 'loss_consonant': 0.061123, 'loss_word': 0.072083}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   21 | 0.000247 | 160000/160635 | 11.2543 | 5.3236 |\n",
      "val: {'recall': 0.995887, 'recall_grapheme': 0.994196, 'recall_vowel': 0.997398, 'recall_consonant': 0.997757, 'recall_word': 0.993472, 'acc_grapheme': 0.993483, 'acc_vowel': 0.99714, 'acc_consonant': 0.998209, 'acc_word': 0.993558, 'loss_grapheme': 0.120974, 'loss_vowel': 0.09259, 'loss_consonant': 0.062598, 'loss_word': 0.075114}\n",
      "   22 | 0.000243 | 160000/160635 | 5.3731 | 5.5523 |||\n",
      "val: {'recall': 0.996105, 'recall_grapheme': 0.995217, 'recall_vowel': 0.996771, 'recall_consonant': 0.997215, 'recall_word': 0.994017, 'acc_grapheme': 0.99418, 'acc_vowel': 0.99714, 'acc_consonant': 0.998234, 'acc_word': 0.994055, 'loss_grapheme': 0.091323, 'loss_vowel': 0.075162, 'loss_consonant': 0.053502, 'loss_word': 0.061462}\n",
      "   23 | 0.000238 | 160000/160635 | 3.4371 | 5.6049 ||\n",
      "val: {'recall': 0.99578, 'recall_grapheme': 0.994146, 'recall_vowel': 0.997398, 'recall_consonant': 0.997431, 'recall_word': 0.993795, 'acc_grapheme': 0.993508, 'acc_vowel': 0.997314, 'acc_consonant': 0.99801, 'acc_word': 0.993782, 'loss_grapheme': 0.119867, 'loss_vowel': 0.09587, 'loss_consonant': 0.063059, 'loss_word': 0.079089}\n",
      "   24 | 0.000233 | 160000/160635 | 6.7817 | 5.3426 ||\n",
      "val: {'recall': 0.99647, 'recall_grapheme': 0.995257, 'recall_vowel': 0.997661, 'recall_consonant': 0.997704, 'recall_word': 0.994135, 'acc_grapheme': 0.994329, 'acc_vowel': 0.997513, 'acc_consonant': 0.998159, 'acc_word': 0.99418, 'loss_grapheme': 0.080266, 'loss_vowel': 0.062627, 'loss_consonant': 0.042616, 'loss_word': 0.058304}\n",
      "###>>>>> saved\n",
      "   25 | 0.000228 | 160000/160635 | 0.5523 | 5.6110 ||\n",
      "val: {'recall': 0.996014, 'recall_grapheme': 0.994979, 'recall_vowel': 0.996966, 'recall_consonant': 0.997134, 'recall_word': 0.994155, 'acc_grapheme': 0.994354, 'acc_vowel': 0.997339, 'acc_consonant': 0.998309, 'acc_word': 0.99413, 'loss_grapheme': 0.05177, 'loss_vowel': 0.040655, 'loss_consonant': 0.028761, 'loss_word': 0.040956}\n",
      "   26 | 0.000223 | 160000/160635 | 2.0794 | 5.6653 |||\n",
      "val: {'recall': 0.996474, 'recall_grapheme': 0.995419, 'recall_vowel': 0.997046, 'recall_consonant': 0.998013, 'recall_word': 0.994477, 'acc_grapheme': 0.994727, 'acc_vowel': 0.997562, 'acc_consonant': 0.998632, 'acc_word': 0.994453, 'loss_grapheme': 0.062244, 'loss_vowel': 0.048878, 'loss_consonant': 0.035186, 'loss_word': 0.046229}\n",
      "###>>>>> saved\n",
      "   27 | 0.000218 | 160000/160635 | 10.8414 | 5.5434 |\n",
      "val: {'recall': 0.995862, 'recall_grapheme': 0.994782, 'recall_vowel': 0.996783, 'recall_consonant': 0.997101, 'recall_word': 0.994161, 'acc_grapheme': 0.994354, 'acc_vowel': 0.997189, 'acc_consonant': 0.998234, 'acc_word': 0.994205, 'loss_grapheme': 0.07529, 'loss_vowel': 0.058376, 'loss_consonant': 0.040203, 'loss_word': 0.051046}\n",
      "   28 | 0.000213 | 160000/160635 | 7.9860 | 5.3983 ||\n",
      "val: {'recall': 0.996021, 'recall_grapheme': 0.995233, 'recall_vowel': 0.996726, 'recall_consonant': 0.99689, 'recall_word': 0.994508, 'acc_grapheme': 0.994652, 'acc_vowel': 0.997189, 'acc_consonant': 0.99806, 'acc_word': 0.994528, 'loss_grapheme': 0.062024, 'loss_vowel': 0.053148, 'loss_consonant': 0.037536, 'loss_word': 0.043064}\n",
      "   29 | 0.000207 | 160000/160635 | 0.5259 | 5.0248 ||\n",
      "val: {'recall': 0.996428, 'recall_grapheme': 0.995524, 'recall_vowel': 0.997456, 'recall_consonant': 0.997208, 'recall_word': 0.99502, 'acc_grapheme': 0.995249, 'acc_vowel': 0.997861, 'acc_consonant': 0.998632, 'acc_word': 0.995025, 'loss_grapheme': 0.023604, 'loss_vowel': 0.015727, 'loss_consonant': 0.011516, 'loss_word': 0.022122}\n",
      "   30 | 0.000202 | 160000/160635 | 10.2710 | 5.2353 ||\n",
      "val: {'recall': 0.996224, 'recall_grapheme': 0.994842, 'recall_vowel': 0.99746, 'recall_consonant': 0.997751, 'recall_word': 0.994087, 'acc_grapheme': 0.994329, 'acc_vowel': 0.997339, 'acc_consonant': 0.998408, 'acc_word': 0.994155, 'loss_grapheme': 0.096573, 'loss_vowel': 0.080586, 'loss_consonant': 0.054351, 'loss_word': 0.068318}\n",
      "   31 | 0.000199 | 090880/160635 | 3.9736 | 5.4539 ||\n",
      "val: {'recall': 0.996143, 'recall_grapheme': 0.994721, 'recall_vowel': 0.997181, 'recall_consonant': 0.997947, 'recall_word': 0.994106, 'acc_grapheme': 0.994503, 'acc_vowel': 0.997239, 'acc_consonant': 0.998458, 'acc_word': 0.99413, 'loss_grapheme': 0.064099, 'loss_vowel': 0.049045, 'loss_consonant': 0.034053, 'loss_word': 0.045168}\n",
      "   32 | 0.000191 | 160000/160635 | 0.3907 | 5.6783 ||\n",
      "val: {'recall': 0.996332, 'recall_grapheme': 0.994807, 'recall_vowel': 0.997271, 'recall_consonant': 0.998444, 'recall_word': 0.994721, 'acc_grapheme': 0.994429, 'acc_vowel': 0.997388, 'acc_consonant': 0.998483, 'acc_word': 0.994727, 'loss_grapheme': 0.071149, 'loss_vowel': 0.05869, 'loss_consonant': 0.042017, 'loss_word': 0.05007}\n",
      "   33 | 0.000185 | 160000/160635 | 2.8553 | 5.1793 |||\n",
      "val: {'recall': 0.996524, 'recall_grapheme': 0.995558, 'recall_vowel': 0.996897, 'recall_consonant': 0.998081, 'recall_word': 0.994592, 'acc_grapheme': 0.994951, 'acc_vowel': 0.997538, 'acc_consonant': 0.998334, 'acc_word': 0.994603, 'loss_grapheme': 0.042059, 'loss_vowel': 0.029534, 'loss_consonant': 0.02233, 'loss_word': 0.036868}\n",
      "###>>>>> saved\n",
      "   34 | 0.000179 | 160000/160635 | 5.8951 | 5.4515 ||\n",
      "val: {'recall': 0.99629, 'recall_grapheme': 0.99497, 'recall_vowel': 0.997448, 'recall_consonant': 0.997774, 'recall_word': 0.994395, 'acc_grapheme': 0.994478, 'acc_vowel': 0.997264, 'acc_consonant': 0.998358, 'acc_word': 0.994429, 'loss_grapheme': 0.076925, 'loss_vowel': 0.062451, 'loss_consonant': 0.045326, 'loss_word': 0.05266}\n",
      "   35 | 0.000173 | 160000/160635 | 7.9302 | 5.4694 ||\n",
      "val: {'recall': 0.996545, 'recall_grapheme': 0.995492, 'recall_vowel': 0.997526, 'recall_consonant': 0.997669, 'recall_word': 0.994671, 'acc_grapheme': 0.994926, 'acc_vowel': 0.997562, 'acc_consonant': 0.998458, 'acc_word': 0.994677, 'loss_grapheme': 0.070102, 'loss_vowel': 0.05628, 'loss_consonant': 0.038955, 'loss_word': 0.050288}\n",
      "###>>>>> saved\n",
      "   36 | 0.000168 | 160000/160635 | 10.4502 | 5.7255 |\n",
      "val: {'recall': 0.996476, 'recall_grapheme': 0.995265, 'recall_vowel': 0.996923, 'recall_consonant': 0.998453, 'recall_word': 0.994472, 'acc_grapheme': 0.994429, 'acc_vowel': 0.997165, 'acc_consonant': 0.998508, 'acc_word': 0.994528, 'loss_grapheme': 0.083481, 'loss_vowel': 0.071119, 'loss_consonant': 0.050032, 'loss_word': 0.053807}\n",
      "   37 | 0.000162 | 160000/160635 | 10.3229 | 5.7070 |\n",
      "val: {'recall': 0.996259, 'recall_grapheme': 0.99503, 'recall_vowel': 0.997654, 'recall_consonant': 0.997322, 'recall_word': 0.994471, 'acc_grapheme': 0.994279, 'acc_vowel': 0.997488, 'acc_consonant': 0.998408, 'acc_word': 0.994478, 'loss_grapheme': 0.11006, 'loss_vowel': 0.096748, 'loss_consonant': 0.065948, 'loss_word': 0.069098}\n",
      "   39 | 0.000150 | 160000/160635 | 0.4677 | 5.0468 ||\n",
      "val: {'recall': 0.996943, 'recall_grapheme': 0.996159, 'recall_vowel': 0.997969, 'recall_consonant': 0.997484, 'recall_word': 0.995238, 'acc_grapheme': 0.995573, 'acc_vowel': 0.997836, 'acc_consonant': 0.998632, 'acc_word': 0.995249, 'loss_grapheme': 0.028018, 'loss_vowel': 0.020928, 'loss_consonant': 0.015793, 'loss_word': 0.024066}\n",
      "###>>>>> saved\n",
      "   40 | 0.000144 | 160000/160635 | 7.3789 | 5.5667 |||\n",
      "val: {'recall': 0.996083, 'recall_grapheme': 0.994674, 'recall_vowel': 0.997578, 'recall_consonant': 0.997405, 'recall_word': 0.994127, 'acc_grapheme': 0.994006, 'acc_vowel': 0.997339, 'acc_consonant': 0.998135, 'acc_word': 0.994155, 'loss_grapheme': 0.159022, 'loss_vowel': 0.139314, 'loss_consonant': 0.090065, 'loss_word': 0.095258}\n",
      "   41 | 0.000138 | 160000/160635 | 0.4841 | 5.4774 ||\n",
      "val: {'recall': 0.996701, 'recall_grapheme': 0.995883, 'recall_vowel': 0.99768, 'recall_consonant': 0.997359, 'recall_word': 0.995002, 'acc_grapheme': 0.9951, 'acc_vowel': 0.997936, 'acc_consonant': 0.998557, 'acc_word': 0.995001, 'loss_grapheme': 0.040608, 'loss_vowel': 0.030594, 'loss_consonant': 0.022102, 'loss_word': 0.033309}\n",
      "   42 | 0.000132 | 160000/160635 | 4.0020 | 5.1872 ||\n",
      "val: {'recall': 0.996642, 'recall_grapheme': 0.995356, 'recall_vowel': 0.997249, 'recall_consonant': 0.998607, 'recall_word': 0.994447, 'acc_grapheme': 0.994777, 'acc_vowel': 0.997587, 'acc_consonant': 0.998707, 'acc_word': 0.994453, 'loss_grapheme': 0.052109, 'loss_vowel': 0.040431, 'loss_consonant': 0.030428, 'loss_word': 0.042593}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   43 | 0.000127 | 160000/160635 | 8.1133 | 5.8494 ||\n",
      "val: {'recall': 0.996222, 'recall_grapheme': 0.994756, 'recall_vowel': 0.997538, 'recall_consonant': 0.997839, 'recall_word': 0.994193, 'acc_grapheme': 0.994205, 'acc_vowel': 0.997314, 'acc_consonant': 0.998408, 'acc_word': 0.994155, 'loss_grapheme': 0.101491, 'loss_vowel': 0.085259, 'loss_consonant': 0.059546, 'loss_word': 0.066926}\n",
      "   44 | 0.000121 | 160000/160635 | 11.0252 | 5.4188 |\n",
      "val: {'recall': 0.996076, 'recall_grapheme': 0.99475, 'recall_vowel': 0.99736, 'recall_consonant': 0.997446, 'recall_word': 0.994224, 'acc_grapheme': 0.99408, 'acc_vowel': 0.997413, 'acc_consonant': 0.998433, 'acc_word': 0.99423, 'loss_grapheme': 0.125227, 'loss_vowel': 0.112274, 'loss_consonant': 0.078334, 'loss_word': 0.074809}\n",
      "   45 | 0.000115 | 160000/160635 | 5.5866 | 4.9891 ||\n",
      "val: {'recall': 0.996647, 'recall_grapheme': 0.995427, 'recall_vowel': 0.997382, 'recall_consonant': 0.99835, 'recall_word': 0.994493, 'acc_grapheme': 0.994752, 'acc_vowel': 0.997562, 'acc_consonant': 0.998582, 'acc_word': 0.994528, 'loss_grapheme': 0.063596, 'loss_vowel': 0.051138, 'loss_consonant': 0.036867, 'loss_word': 0.045921}\n",
      "   46 | 0.000109 | 160000/160635 | 12.9348 | 5.3796 |\n",
      "val: {'recall': 0.996151, 'recall_grapheme': 0.995504, 'recall_vowel': 0.99728, 'recall_consonant': 0.996316, 'recall_word': 0.994331, 'acc_grapheme': 0.994702, 'acc_vowel': 0.997587, 'acc_consonant': 0.998582, 'acc_word': 0.994354, 'loss_grapheme': 0.05839, 'loss_vowel': 0.049022, 'loss_consonant': 0.034483, 'loss_word': 0.043407}\n",
      "   47 | 0.000104 | 160000/160635 | 0.4433 | 5.3231 ||\n",
      "val: {'recall': 0.996863, 'recall_grapheme': 0.995979, 'recall_vowel': 0.997559, 'recall_consonant': 0.997935, 'recall_word': 0.995155, 'acc_grapheme': 0.995473, 'acc_vowel': 0.997637, 'acc_consonant': 0.998707, 'acc_word': 0.995175, 'loss_grapheme': 0.042468, 'loss_vowel': 0.034355, 'loss_consonant': 0.025287, 'loss_word': 0.032396}\n",
      "   48 | 0.000098 | 160000/160635 | 6.3948 | 5.2672 ||\n",
      "val: {'recall': 0.996713, 'recall_grapheme': 0.995615, 'recall_vowel': 0.997702, 'recall_consonant': 0.997919, 'recall_word': 0.994954, 'acc_grapheme': 0.995025, 'acc_vowel': 0.997712, 'acc_consonant': 0.998607, 'acc_word': 0.994951, 'loss_grapheme': 0.04589, 'loss_vowel': 0.034173, 'loss_consonant': 0.025536, 'loss_word': 0.037092}\n",
      "   49 | 0.000093 | 160000/160635 | 7.4416 | 5.3322 ||\n",
      "val: {'recall': 0.996907, 'recall_grapheme': 0.995848, 'recall_vowel': 0.9974, 'recall_consonant': 0.99853, 'recall_word': 0.995175, 'acc_grapheme': 0.9951, 'acc_vowel': 0.997761, 'acc_consonant': 0.998781, 'acc_word': 0.9952, 'loss_grapheme': 0.052116, 'loss_vowel': 0.042679, 'loss_consonant': 0.030696, 'loss_word': 0.041017}\n",
      "   50 | 0.000087 | 160000/160635 | 7.5281 | 4.8152 ||\n",
      "val: {'recall': 0.996913, 'recall_grapheme': 0.995831, 'recall_vowel': 0.997587, 'recall_consonant': 0.998402, 'recall_word': 0.995018, 'acc_grapheme': 0.995224, 'acc_vowel': 0.997737, 'acc_consonant': 0.998707, 'acc_word': 0.9951, 'loss_grapheme': 0.032249, 'loss_vowel': 0.024111, 'loss_consonant': 0.017652, 'loss_word': 0.028295}\n",
      "   51 | 0.000082 | 160000/160635 | 0.4440 | 4.8536 ||\n",
      "val: {'recall': 0.99719, 'recall_grapheme': 0.996382, 'recall_vowel': 0.997862, 'recall_consonant': 0.998134, 'recall_word': 0.99567, 'acc_grapheme': 0.995871, 'acc_vowel': 0.998085, 'acc_consonant': 0.99898, 'acc_word': 0.995672, 'loss_grapheme': 0.022536, 'loss_vowel': 0.014581, 'loss_consonant': 0.010935, 'loss_word': 0.021575}\n",
      "###>>>>> saved\n",
      "   52 | 0.000077 | 160000/160635 | 9.5821 | 5.6442 |||\n",
      "val: {'recall': 0.996278, 'recall_grapheme': 0.995398, 'recall_vowel': 0.99698, 'recall_consonant': 0.997335, 'recall_word': 0.99473, 'acc_grapheme': 0.994503, 'acc_vowel': 0.997314, 'acc_consonant': 0.998383, 'acc_word': 0.994752, 'loss_grapheme': 0.100484, 'loss_vowel': 0.089382, 'loss_consonant': 0.06226, 'loss_word': 0.062502}\n",
      "   53 | 0.000072 | 160000/160635 | 0.4053 | 5.5622 ||\n",
      "val: {'recall': 0.996297, 'recall_grapheme': 0.99538, 'recall_vowel': 0.997029, 'recall_consonant': 0.9974, 'recall_word': 0.994399, 'acc_grapheme': 0.994727, 'acc_vowel': 0.997388, 'acc_consonant': 0.998533, 'acc_word': 0.994453, 'loss_grapheme': 0.058185, 'loss_vowel': 0.047315, 'loss_consonant': 0.034076, 'loss_word': 0.041959}\n",
      "   54 | 0.000067 | 160000/160635 | 0.3997 | 5.1147 ||\n",
      "val: {'recall': 0.996901, 'recall_grapheme': 0.995815, 'recall_vowel': 0.99756, 'recall_consonant': 0.998415, 'recall_word': 0.994943, 'acc_grapheme': 0.995299, 'acc_vowel': 0.997761, 'acc_consonant': 0.998682, 'acc_word': 0.995025, 'loss_grapheme': 0.031176, 'loss_vowel': 0.022899, 'loss_consonant': 0.017278, 'loss_word': 0.02713}\n",
      "   55 | 0.000062 | 160000/160635 | 0.4007 | 5.6189 |||\n",
      "val: {'recall': 0.997126, 'recall_grapheme': 0.996072, 'recall_vowel': 0.997858, 'recall_consonant': 0.998503, 'recall_word': 0.995096, 'acc_grapheme': 0.995423, 'acc_vowel': 0.997886, 'acc_consonant': 0.998906, 'acc_word': 0.995125, 'loss_grapheme': 0.033068, 'loss_vowel': 0.024519, 'loss_consonant': 0.017649, 'loss_word': 0.028228}\n",
      "   56 | 0.000057 | 160000/160635 | 0.3395 | 5.0708 ||\n",
      "val: {'recall': 0.996411, 'recall_grapheme': 0.995588, 'recall_vowel': 0.997157, 'recall_consonant': 0.997311, 'recall_word': 0.994764, 'acc_grapheme': 0.994901, 'acc_vowel': 0.997438, 'acc_consonant': 0.998433, 'acc_word': 0.994777, 'loss_grapheme': 0.068867, 'loss_vowel': 0.055135, 'loss_consonant': 0.040171, 'loss_word': 0.047352}\n",
      "   57 | 0.000053 | 160000/160635 | 11.1076 | 5.3866 |\n",
      "val: {'recall': 0.996398, 'recall_grapheme': 0.995196, 'recall_vowel': 0.997293, 'recall_consonant': 0.997906, 'recall_word': 0.994457, 'acc_grapheme': 0.994578, 'acc_vowel': 0.997587, 'acc_consonant': 0.998582, 'acc_word': 0.994503, 'loss_grapheme': 0.082952, 'loss_vowel': 0.07019, 'loss_consonant': 0.048493, 'loss_word': 0.059274}\n",
      "   58 | 0.000048 | 160000/160635 | 0.4522 | 5.3112 ||\n",
      "val: {'recall': 0.996555, 'recall_grapheme': 0.995491, 'recall_vowel': 0.997447, 'recall_consonant': 0.997792, 'recall_word': 0.994692, 'acc_grapheme': 0.994926, 'acc_vowel': 0.997562, 'acc_consonant': 0.998582, 'acc_word': 0.994727, 'loss_grapheme': 0.040733, 'loss_vowel': 0.032837, 'loss_consonant': 0.024412, 'loss_word': 0.030585}\n",
      "   59 | 0.000044 | 160000/160635 | 0.3312 | 5.6384 ||\n",
      "val: {'recall': 0.996505, 'recall_grapheme': 0.995424, 'recall_vowel': 0.99718, 'recall_consonant': 0.997993, 'recall_word': 0.994524, 'acc_grapheme': 0.994926, 'acc_vowel': 0.997513, 'acc_consonant': 0.998732, 'acc_word': 0.994553, 'loss_grapheme': 0.068252, 'loss_vowel': 0.052644, 'loss_consonant': 0.035886, 'loss_word': 0.049877}\n",
      "   60 | 0.000040 | 160000/160635 | 0.4630 | 4.7237 |||\n",
      "val: {'recall': 0.997006, 'recall_grapheme': 0.996342, 'recall_vowel': 0.997804, 'recall_consonant': 0.997535, 'recall_word': 0.99556, 'acc_grapheme': 0.995672, 'acc_vowel': 0.997886, 'acc_consonant': 0.998781, 'acc_word': 0.995598, 'loss_grapheme': 0.024058, 'loss_vowel': 0.016213, 'loss_consonant': 0.012216, 'loss_word': 0.022614}\n",
      "   61 | 0.000036 | 160000/160635 | 0.3415 | 4.7982 ||\n",
      "val: {'recall': 0.996759, 'recall_grapheme': 0.995863, 'recall_vowel': 0.997404, 'recall_consonant': 0.997904, 'recall_word': 0.995012, 'acc_grapheme': 0.995274, 'acc_vowel': 0.997637, 'acc_consonant': 0.998707, 'acc_word': 0.995075, 'loss_grapheme': 0.038223, 'loss_vowel': 0.029757, 'loss_consonant': 0.022119, 'loss_word': 0.030413}\n",
      "   62 | 0.000032 | 160000/160635 | 0.4038 | 4.9844 ||\n",
      "val: {'recall': 0.997174, 'recall_grapheme': 0.99635, 'recall_vowel': 0.997597, 'recall_consonant': 0.998399, 'recall_word': 0.995174, 'acc_grapheme': 0.995647, 'acc_vowel': 0.997911, 'acc_consonant': 0.998756, 'acc_word': 0.995299, 'loss_grapheme': 0.023074, 'loss_vowel': 0.01598, 'loss_consonant': 0.012538, 'loss_word': 0.021508}\n",
      "   63 | 0.000029 | 160000/160635 | 0.3201 | 4.9410 ||\n",
      "val: {'recall': 0.996783, 'recall_grapheme': 0.995865, 'recall_vowel': 0.997478, 'recall_consonant': 0.997926, 'recall_word': 0.994803, 'acc_grapheme': 0.99515, 'acc_vowel': 0.997562, 'acc_consonant': 0.998682, 'acc_word': 0.994827, 'loss_grapheme': 0.042436, 'loss_vowel': 0.03296, 'loss_consonant': 0.024075, 'loss_word': 0.034123}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   64 | 0.000025 | 160000/160635 | 4.3875 | 5.0573 ||\n",
      "val: {'recall': 0.996591, 'recall_grapheme': 0.995529, 'recall_vowel': 0.997283, 'recall_consonant': 0.998023, 'recall_word': 0.994801, 'acc_grapheme': 0.994951, 'acc_vowel': 0.997513, 'acc_consonant': 0.998756, 'acc_word': 0.994851, 'loss_grapheme': 0.059175, 'loss_vowel': 0.048154, 'loss_consonant': 0.035569, 'loss_word': 0.044043}\n",
      "   65 | 0.000022 | 160000/160635 | 10.3529 | 5.1616 |\n",
      "val: {'recall': 0.996847, 'recall_grapheme': 0.995767, 'recall_vowel': 0.997461, 'recall_consonant': 0.998393, 'recall_word': 0.99491, 'acc_grapheme': 0.995175, 'acc_vowel': 0.997687, 'acc_consonant': 0.998707, 'acc_word': 0.995001, 'loss_grapheme': 0.049876, 'loss_vowel': 0.042581, 'loss_consonant': 0.031549, 'loss_word': 0.038443}\n",
      "   66 | 0.000019 | 160000/160635 | 0.4634 | 5.1400 ||\n",
      "val: {'recall': 0.99694, 'recall_grapheme': 0.995895, 'recall_vowel': 0.997479, 'recall_consonant': 0.998492, 'recall_word': 0.994923, 'acc_grapheme': 0.995175, 'acc_vowel': 0.997761, 'acc_consonant': 0.998682, 'acc_word': 0.994976, 'loss_grapheme': 0.057958, 'loss_vowel': 0.047771, 'loss_consonant': 0.03486, 'loss_word': 0.042167}\n",
      "   67 | 0.000016 | 160000/160635 | 1.9272 | 4.4586 ||\n",
      "val: {'recall': 0.996832, 'recall_grapheme': 0.995746, 'recall_vowel': 0.997799, 'recall_consonant': 0.998038, 'recall_word': 0.995168, 'acc_grapheme': 0.995324, 'acc_vowel': 0.997786, 'acc_consonant': 0.998806, 'acc_word': 0.995224, 'loss_grapheme': 0.035428, 'loss_vowel': 0.026663, 'loss_consonant': 0.020103, 'loss_word': 0.031183}\n",
      "   68 | 0.000014 | 160000/160635 | 0.2784 | 5.5225 ||\n",
      "val: {'recall': 0.99688, 'recall_grapheme': 0.995681, 'recall_vowel': 0.997547, 'recall_consonant': 0.998612, 'recall_word': 0.994994, 'acc_grapheme': 0.99515, 'acc_vowel': 0.997662, 'acc_consonant': 0.998881, 'acc_word': 0.995025, 'loss_grapheme': 0.044469, 'loss_vowel': 0.034682, 'loss_consonant': 0.025638, 'loss_word': 0.034741}\n",
      "   69 | 0.000011 | 160000/160635 | 2.1655 | 5.2795 ||"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-6df1386d1bbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-54a79e2dff4c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcycle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_cycles\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CYCLE:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtrain_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         scheduler = WarmupCyclicalLR(\n\u001b[1;32m     17\u001b[0m             \"cos\", args.base_lr, args.num_epochs, iters_per_epoch=len(train_loader), warmup_epochs=args.warmup_epochs)\n",
      "\u001b[0;32m<ipython-input-33-54a79e2dff4c>\u001b[0m in \u001b[0;36mtrain_cycle\u001b[0;34m(args, model, optimizer, lr_scheduler)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#train_iter > 0 and train_iter % args.iter_val == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nval:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-5ca1aeb200dd>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, val_loader)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m168\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1295\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0m_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_tup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(args, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CYCLE: 1\n",
      "{'recall': 0.996308, 'recall_grapheme': 0.995273, 'recall_vowel': 0.997201, 'recall_consonant': 0.997485, 'recall_word': 0.993975, 'acc_grapheme': 0.994603, 'acc_vowel': 0.997289, 'acc_consonant': 0.998159, 'acc_word': 0.993956, 'loss_grapheme': 0.041434, 'loss_vowel': 0.029393, 'loss_consonant': 0.021052, 'loss_word': 0.035974}\n",
      "    0 | 0.000060 | 160000/160635 | 14.4489 | 10.7515 |\n",
      "val: {'recall': 0.994211, 'recall_grapheme': 0.992448, 'recall_vowel': 0.995722, 'recall_consonant': 0.996226, 'recall_word': 0.993265, 'acc_grapheme': 0.991369, 'acc_vowel': 0.995274, 'acc_consonant': 0.99617, 'acc_word': 0.993309, 'loss_grapheme': 0.240193, 'loss_vowel': 0.208128, 'loss_consonant': 0.133368, 'loss_word': 0.122518}\n",
      "    1 | 0.000120 | 160000/160635 | 7.3266 | 10.6809 ||\n",
      "val: {'recall': 0.991872, 'recall_grapheme': 0.990698, 'recall_vowel': 0.993411, 'recall_consonant': 0.992682, 'recall_word': 0.993069, 'acc_grapheme': 0.990275, 'acc_vowel': 0.990524, 'acc_consonant': 0.981818, 'acc_word': 0.993135, 'loss_grapheme': 0.408427, 'loss_vowel': 0.359806, 'loss_consonant': 0.226054, 'loss_word': 0.192264}\n",
      "    2 | 0.000179 | 160000/160635 | 13.7436 | 10.5099 |\n",
      "val: {'recall': 0.991217, 'recall_grapheme': 0.988986, 'recall_vowel': 0.993036, 'recall_consonant': 0.993862, 'recall_word': 0.993264, 'acc_grapheme': 0.990175, 'acc_vowel': 0.990126, 'acc_consonant': 0.985698, 'acc_word': 0.993334, 'loss_grapheme': 0.402349, 'loss_vowel': 0.361942, 'loss_consonant': 0.222877, 'loss_word': 0.1723}\n",
      "    3 | 0.000238 | 160000/160635 | 13.9601 | 10.2741 |\n",
      "val: {'recall': 0.993177, 'recall_grapheme': 0.990849, 'recall_vowel': 0.995022, 'recall_consonant': 0.995988, 'recall_word': 0.993189, 'acc_grapheme': 0.990648, 'acc_vowel': 0.99418, 'acc_consonant': 0.995598, 'acc_word': 0.99321, 'loss_grapheme': 0.344309, 'loss_vowel': 0.302169, 'loss_consonant': 0.194945, 'loss_word': 0.144634}\n",
      "    4 | 0.000297 | 160000/160635 | 7.4207 | 10.4398 ||\n",
      "val: {'recall': 0.991686, 'recall_grapheme': 0.988809, 'recall_vowel': 0.993588, 'recall_consonant': 0.995539, 'recall_word': 0.993567, 'acc_grapheme': 0.989728, 'acc_vowel': 0.992588, 'acc_consonant': 0.990673, 'acc_word': 0.993533, 'loss_grapheme': 0.361295, 'loss_vowel': 0.348796, 'loss_consonant': 0.205379, 'loss_word': 0.145527}\n",
      "    5 | 0.000296 | 160000/160635 | 1.0284 | 9.8438 |||\n",
      "val: {'recall': 0.99436, 'recall_grapheme': 0.993042, 'recall_vowel': 0.995007, 'recall_consonant': 0.996351, 'recall_word': 0.99417, 'acc_grapheme': 0.99229, 'acc_vowel': 0.994827, 'acc_consonant': 0.995996, 'acc_word': 0.994205, 'loss_grapheme': 0.239249, 'loss_vowel': 0.225999, 'loss_consonant': 0.143288, 'loss_word': 0.101472}\n",
      "    6 | 0.000294 | 160000/160635 | 11.1939 | 9.8899 ||\n",
      "val: {'recall': 0.991813, 'recall_grapheme': 0.989755, 'recall_vowel': 0.993341, 'recall_consonant': 0.994401, 'recall_word': 0.992988, 'acc_grapheme': 0.990325, 'acc_vowel': 0.993633, 'acc_consonant': 0.993682, 'acc_word': 0.992911, 'loss_grapheme': 0.366075, 'loss_vowel': 0.324188, 'loss_consonant': 0.209766, 'loss_word': 0.15235}\n",
      "    7 | 0.000293 | 160000/160635 | 13.4602 | 10.0077 |\n",
      "val: {'recall': 0.992916, 'recall_grapheme': 0.991267, 'recall_vowel': 0.994892, 'recall_consonant': 0.99424, 'recall_word': 0.993622, 'acc_grapheme': 0.990772, 'acc_vowel': 0.994628, 'acc_consonant': 0.994528, 'acc_word': 0.993707, 'loss_grapheme': 0.290983, 'loss_vowel': 0.252559, 'loss_consonant': 0.162059, 'loss_word': 0.122138}\n",
      "    8 | 0.000291 | 160000/160635 | 13.4298 | 10.0139 |\n",
      "val: {'recall': 0.994279, 'recall_grapheme': 0.992214, 'recall_vowel': 0.996211, 'recall_consonant': 0.996476, 'recall_word': 0.994174, 'acc_grapheme': 0.991842, 'acc_vowel': 0.995896, 'acc_consonant': 0.997015, 'acc_word': 0.994279, 'loss_grapheme': 0.257348, 'loss_vowel': 0.230749, 'loss_consonant': 0.144891, 'loss_word': 0.119894}\n",
      "    9 | 0.000289 | 160000/160635 | 14.1266 | 10.1235 |\n",
      "val: {'recall': 0.99397, 'recall_grapheme': 0.992443, 'recall_vowel': 0.995499, 'recall_consonant': 0.995496, 'recall_word': 0.993575, 'acc_grapheme': 0.991643, 'acc_vowel': 0.995846, 'acc_consonant': 0.996767, 'acc_word': 0.993608, 'loss_grapheme': 0.230955, 'loss_vowel': 0.210417, 'loss_consonant': 0.134851, 'loss_word': 0.102065}\n",
      "   10 | 0.000286 | 160000/160635 | 12.3798 | 9.8836 ||\n",
      "val: {'recall': 0.993077, 'recall_grapheme': 0.991436, 'recall_vowel': 0.994248, 'recall_consonant': 0.995186, 'recall_word': 0.993425, 'acc_grapheme': 0.991121, 'acc_vowel': 0.994827, 'acc_consonant': 0.994379, 'acc_word': 0.993434, 'loss_grapheme': 0.301564, 'loss_vowel': 0.276656, 'loss_consonant': 0.181955, 'loss_word': 0.122437}\n",
      "   11 | 0.000284 | 160000/160635 | 11.0405 | 9.5523 |\n",
      "val: {'recall': 0.994619, 'recall_grapheme': 0.992869, 'recall_vowel': 0.995273, 'recall_consonant': 0.997463, 'recall_word': 0.993434, 'acc_grapheme': 0.99224, 'acc_vowel': 0.995821, 'acc_consonant': 0.997264, 'acc_word': 0.993558, 'loss_grapheme': 0.2356, 'loss_vowel': 0.1989, 'loss_consonant': 0.129465, 'loss_word': 0.110564}\n",
      "   12 | 0.000281 | 160000/160635 | 13.5031 | 9.8159 |\n",
      "val: {'recall': 0.994627, 'recall_grapheme': 0.992919, 'recall_vowel': 0.995349, 'recall_consonant': 0.997324, 'recall_word': 0.993979, 'acc_grapheme': 0.992016, 'acc_vowel': 0.995598, 'acc_consonant': 0.996866, 'acc_word': 0.994006, 'loss_grapheme': 0.238057, 'loss_vowel': 0.213187, 'loss_consonant': 0.135013, 'loss_word': 0.106593}\n",
      "   13 | 0.000278 | 160000/160635 | 1.9928 | 9.7900 |||\n",
      "val: {'recall': 0.994886, 'recall_grapheme': 0.993397, 'recall_vowel': 0.995232, 'recall_consonant': 0.99752, 'recall_word': 0.993993, 'acc_grapheme': 0.992464, 'acc_vowel': 0.995498, 'acc_consonant': 0.997339, 'acc_word': 0.994055, 'loss_grapheme': 0.201959, 'loss_vowel': 0.174557, 'loss_consonant': 0.109248, 'loss_word': 0.093553}\n",
      "   14 | 0.000275 | 160000/160635 | 14.2428 | 9.8996 ||\n",
      "val: {'recall': 0.992118, 'recall_grapheme': 0.989167, 'recall_vowel': 0.994716, 'recall_consonant': 0.995423, 'recall_word': 0.99358, 'acc_grapheme': 0.990598, 'acc_vowel': 0.993658, 'acc_consonant': 0.990598, 'acc_word': 0.993658, 'loss_grapheme': 0.308241, 'loss_vowel': 0.275936, 'loss_consonant': 0.175595, 'loss_word': 0.131263}\n",
      "   15 | 0.000271 | 160000/160635 | 13.9897 | 10.3223 |\n",
      "val: {'recall': 0.993298, 'recall_grapheme': 0.991515, 'recall_vowel': 0.99415, 'recall_consonant': 0.996014, 'recall_word': 0.994095, 'acc_grapheme': 0.991046, 'acc_vowel': 0.994006, 'acc_consonant': 0.99423, 'acc_word': 0.994105, 'loss_grapheme': 0.282367, 'loss_vowel': 0.270302, 'loss_consonant': 0.172064, 'loss_word': 0.115484}\n",
      "   16 | 0.000268 | 160000/160635 | 13.8832 | 9.6644 ||\n",
      "val: {'recall': 0.994198, 'recall_grapheme': 0.992378, 'recall_vowel': 0.99468, 'recall_consonant': 0.997356, 'recall_word': 0.993729, 'acc_grapheme': 0.991593, 'acc_vowel': 0.99418, 'acc_consonant': 0.995025, 'acc_word': 0.993732, 'loss_grapheme': 0.237156, 'loss_vowel': 0.221758, 'loss_consonant': 0.140917, 'loss_word': 0.117033}\n",
      "   17 | 0.000264 | 160000/160635 | 1.5335 | 10.1207 ||\n",
      "val: {'recall': 0.994621, 'recall_grapheme': 0.992976, 'recall_vowel': 0.995996, 'recall_consonant': 0.996539, 'recall_word': 0.993864, 'acc_grapheme': 0.992215, 'acc_vowel': 0.995772, 'acc_consonant': 0.996966, 'acc_word': 0.993832, 'loss_grapheme': 0.19443, 'loss_vowel': 0.170272, 'loss_consonant': 0.110217, 'loss_word': 0.093619}\n",
      "   18 | 0.000260 | 160000/160635 | 0.9752 | 9.7751 |||\n",
      "val: {'recall': 0.995195, 'recall_grapheme': 0.993426, 'recall_vowel': 0.996582, 'recall_consonant': 0.997344, 'recall_word': 0.994238, 'acc_grapheme': 0.992812, 'acc_vowel': 0.996344, 'acc_consonant': 0.997538, 'acc_word': 0.994279, 'loss_grapheme': 0.17792, 'loss_vowel': 0.14878, 'loss_consonant': 0.097215, 'loss_word': 0.085747}\n",
      "   19 | 0.000256 | 160000/160635 | 10.7648 | 9.8282 ||\n",
      "val: {'recall': 0.994005, 'recall_grapheme': 0.991681, 'recall_vowel': 0.995255, 'recall_consonant': 0.997404, 'recall_word': 0.993663, 'acc_grapheme': 0.991295, 'acc_vowel': 0.995821, 'acc_consonant': 0.995722, 'acc_word': 0.993608, 'loss_grapheme': 0.207539, 'loss_vowel': 0.197542, 'loss_consonant': 0.135406, 'loss_word': 0.096293}\n",
      "   20 | 0.000252 | 160000/160635 | 6.3005 | 9.8705 |||\n",
      "val: {'recall': 0.99345, 'recall_grapheme': 0.991519, 'recall_vowel': 0.993842, 'recall_consonant': 0.996918, 'recall_word': 0.99404, 'acc_grapheme': 0.991568, 'acc_vowel': 0.993533, 'acc_consonant': 0.994528, 'acc_word': 0.994105, 'loss_grapheme': 0.266409, 'loss_vowel': 0.252038, 'loss_consonant': 0.15632, 'loss_word': 0.119081}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   21 | 0.000247 | 160000/160635 | 12.6005 | 9.5486 ||\n",
      "val: {'recall': 0.994709, 'recall_grapheme': 0.993185, 'recall_vowel': 0.99537, 'recall_consonant': 0.997094, 'recall_word': 0.994178, 'acc_grapheme': 0.991966, 'acc_vowel': 0.995622, 'acc_consonant': 0.994827, 'acc_word': 0.99423, 'loss_grapheme': 0.20841, 'loss_vowel': 0.18953, 'loss_consonant': 0.128183, 'loss_word': 0.099562}\n",
      "   22 | 0.000243 | 160000/160635 | 13.5231 | 9.7619 ||\n",
      "val: {'recall': 0.99494, 'recall_grapheme': 0.993457, 'recall_vowel': 0.996012, 'recall_consonant': 0.996835, 'recall_word': 0.993631, 'acc_grapheme': 0.992886, 'acc_vowel': 0.996219, 'acc_consonant': 0.997314, 'acc_word': 0.993707, 'loss_grapheme': 0.182655, 'loss_vowel': 0.164524, 'loss_consonant': 0.106536, 'loss_word': 0.09894}\n",
      "   23 | 0.000238 | 160000/160635 | 1.7195 | 10.0520 ||\n",
      "val: {'recall': 0.994585, 'recall_grapheme': 0.992382, 'recall_vowel': 0.995836, 'recall_consonant': 0.997742, 'recall_word': 0.994032, 'acc_grapheme': 0.992414, 'acc_vowel': 0.995821, 'acc_consonant': 0.997438, 'acc_word': 0.994055, 'loss_grapheme': 0.199392, 'loss_vowel': 0.183394, 'loss_consonant': 0.11617, 'loss_word': 0.092404}\n",
      "   24 | 0.000233 | 160000/160635 | 10.3677 | 9.7034 ||\n",
      "val: {'recall': 0.994218, 'recall_grapheme': 0.991909, 'recall_vowel': 0.995519, 'recall_consonant': 0.997537, 'recall_word': 0.993859, 'acc_grapheme': 0.991693, 'acc_vowel': 0.994777, 'acc_consonant': 0.995871, 'acc_word': 0.993881, 'loss_grapheme': 0.236307, 'loss_vowel': 0.223455, 'loss_consonant': 0.146042, 'loss_word': 0.106755}\n",
      "   25 | 0.000231 | 071680/160635 | 13.5115 | 9.7757 ||"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-6df1386d1bbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-22af81ed83eb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcycle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_cycles\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CYCLE:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtrain_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         scheduler = WarmupCyclicalLR(\n\u001b[1;32m     17\u001b[0m             \"cos\", args.base_lr, args.num_epochs, iters_per_epoch=len(train_loader), warmup_epochs=args.warmup_epochs)\n",
      "\u001b[0;32m<ipython-input-33-22af81ed83eb>\u001b[0m in \u001b[0;36mtrain_cycle\u001b[0;34m(args, model, optimizer, lr_scheduler)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mlam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbx2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbbx1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbby2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbby1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;31m# compute output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_aux1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_aux2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0mloss_primary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_a\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_b\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mloss_aux1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_aux1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_a\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_aux1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_b\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0m_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_tup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(args, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CYCLE: 1\n",
      "{'recall': 0.992515, 'recall_grapheme': 0.990199, 'recall_vowel': 0.995289, 'recall_consonant': 0.994371, 'recall_word': 0.989434, 'acc_grapheme': 0.989379, 'acc_vowel': 0.995249, 'acc_consonant': 0.996219, 'acc_word': 0.989504, 'loss_grapheme': 0.180598, 'loss_vowel': 0.129995, 'loss_consonant': 0.083757, 'loss_word': 0.13174}\n",
      "    0 | 0.000080 | 160000/160635 | 13.4792 | 10.0275 |\n",
      "val: {'recall': 0.991383, 'recall_grapheme': 0.987877, 'recall_vowel': 0.994628, 'recall_consonant': 0.995149, 'recall_word': 0.989062, 'acc_grapheme': 0.98724, 'acc_vowel': 0.994553, 'acc_consonant': 0.995498, 'acc_word': 0.989106, 'loss_grapheme': 0.297682, 'loss_vowel': 0.239045, 'loss_consonant': 0.1532, 'loss_word': 0.170055}\n",
      "    1 | 0.000160 | 160000/160635 | 11.0585 | 10.1233 |\n",
      "val: {'recall': 0.990933, 'recall_grapheme': 0.987167, 'recall_vowel': 0.994776, 'recall_consonant': 0.99462, 'recall_word': 0.98901, 'acc_grapheme': 0.986867, 'acc_vowel': 0.994478, 'acc_consonant': 0.995125, 'acc_word': 0.989031, 'loss_grapheme': 0.319473, 'loss_vowel': 0.263596, 'loss_consonant': 0.168053, 'loss_word': 0.176635}\n",
      "    2 | 0.000239 | 160000/160635 | 10.0962 | 10.0043 |\n",
      "val: {'recall': 0.991856, 'recall_grapheme': 0.988862, 'recall_vowel': 0.994589, 'recall_consonant': 0.995108, 'recall_word': 0.989868, 'acc_grapheme': 0.988036, 'acc_vowel': 0.994851, 'acc_consonant': 0.995672, 'acc_word': 0.989902, 'loss_grapheme': 0.269111, 'loss_vowel': 0.211171, 'loss_consonant': 0.138408, 'loss_word': 0.161402}\n",
      "    3 | 0.000318 | 160000/160635 | 9.2281 | 10.2820 ||\n",
      "val: {'recall': 0.991434, 'recall_grapheme': 0.987733, 'recall_vowel': 0.994496, 'recall_consonant': 0.995772, 'recall_word': 0.988801, 'acc_grapheme': 0.986867, 'acc_vowel': 0.994603, 'acc_consonant': 0.995548, 'acc_word': 0.988782, 'loss_grapheme': 0.311352, 'loss_vowel': 0.243663, 'loss_consonant': 0.157493, 'loss_word': 0.187811}\n",
      "    4 | 0.000397 | 160000/160635 | 3.1227 | 10.0422 ||\n",
      "val: {'recall': 0.992451, 'recall_grapheme': 0.989586, 'recall_vowel': 0.995083, 'recall_consonant': 0.995548, 'recall_word': 0.989366, 'acc_grapheme': 0.987812, 'acc_vowel': 0.994951, 'acc_consonant': 0.995622, 'acc_word': 0.989355, 'loss_grapheme': 0.272678, 'loss_vowel': 0.199107, 'loss_consonant': 0.12204, 'loss_word': 0.17007}\n",
      "    5 | 0.000396 | 160000/160635 | 10.4410 | 10.0275 |\n",
      "val: {'recall': 0.992003, 'recall_grapheme': 0.988867, 'recall_vowel': 0.994657, 'recall_consonant': 0.99562, 'recall_word': 0.989265, 'acc_grapheme': 0.987489, 'acc_vowel': 0.994478, 'acc_consonant': 0.995896, 'acc_word': 0.98928, 'loss_grapheme': 0.221302, 'loss_vowel': 0.171606, 'loss_consonant': 0.11154, 'loss_word': 0.138541}\n",
      "    6 | 0.000395 | 160000/160635 | 12.5297 | 10.1125 |\n",
      "val: {'recall': 0.991781, 'recall_grapheme': 0.988482, 'recall_vowel': 0.994978, 'recall_consonant': 0.99518, 'recall_word': 0.989263, 'acc_grapheme': 0.987788, 'acc_vowel': 0.994901, 'acc_consonant': 0.995896, 'acc_word': 0.989205, 'loss_grapheme': 0.278794, 'loss_vowel': 0.201476, 'loss_consonant': 0.130128, 'loss_word': 0.180896}\n",
      "    7 | 0.000394 | 160000/160635 | 13.6802 | 10.0328 |\n",
      "val: {'recall': 0.991484, 'recall_grapheme': 0.988022, 'recall_vowel': 0.994082, 'recall_consonant': 0.995808, 'recall_word': 0.988145, 'acc_grapheme': 0.987041, 'acc_vowel': 0.994329, 'acc_consonant': 0.995349, 'acc_word': 0.988036, 'loss_grapheme': 0.259381, 'loss_vowel': 0.192231, 'loss_consonant': 0.129269, 'loss_word': 0.157968}\n",
      "    8 | 0.000392 | 160000/160635 | 4.7620 | 10.0751 ||\n",
      "val: {'recall': 0.991454, 'recall_grapheme': 0.988273, 'recall_vowel': 0.993855, 'recall_consonant': 0.995417, 'recall_word': 0.989133, 'acc_grapheme': 0.987539, 'acc_vowel': 0.994031, 'acc_consonant': 0.995622, 'acc_word': 0.989205, 'loss_grapheme': 0.22367, 'loss_vowel': 0.171702, 'loss_consonant': 0.11121, 'loss_word': 0.135771}\n",
      "    9 | 0.000390 | 160000/160635 | 2.3399 | 10.2863 ||\n",
      "val: {'recall': 0.992543, 'recall_grapheme': 0.99, 'recall_vowel': 0.99497, 'recall_consonant': 0.995202, 'recall_word': 0.989918, 'acc_grapheme': 0.988708, 'acc_vowel': 0.995025, 'acc_consonant': 0.995996, 'acc_word': 0.989976, 'loss_grapheme': 0.203958, 'loss_vowel': 0.157991, 'loss_consonant': 0.10412, 'loss_word': 0.13212}\n",
      "###>>>>> saved\n",
      "   10 | 0.000388 | 160000/160635 | 10.9818 | 10.0407 |\n",
      "val: {'recall': 0.992131, 'recall_grapheme': 0.989023, 'recall_vowel': 0.99517, 'recall_consonant': 0.995306, 'recall_word': 0.989045, 'acc_grapheme': 0.987763, 'acc_vowel': 0.995075, 'acc_consonant': 0.995871, 'acc_word': 0.989031, 'loss_grapheme': 0.229764, 'loss_vowel': 0.170772, 'loss_consonant': 0.116051, 'loss_word': 0.159387}\n",
      "   11 | 0.000386 | 160000/160635 | 6.8940 | 9.8569 |||\n",
      "val: {'recall': 0.992194, 'recall_grapheme': 0.988807, 'recall_vowel': 0.99494, 'recall_consonant': 0.996221, 'recall_word': 0.99008, 'acc_grapheme': 0.987962, 'acc_vowel': 0.994876, 'acc_consonant': 0.995622, 'acc_word': 0.990101, 'loss_grapheme': 0.272238, 'loss_vowel': 0.217124, 'loss_consonant': 0.139223, 'loss_word': 0.15957}\n",
      "   12 | 0.000384 | 160000/160635 | 11.7857 | 9.8688 ||\n",
      "val: {'recall': 0.992918, 'recall_grapheme': 0.99079, 'recall_vowel': 0.995815, 'recall_consonant': 0.994277, 'recall_word': 0.990486, 'acc_grapheme': 0.989777, 'acc_vowel': 0.995548, 'acc_consonant': 0.996369, 'acc_word': 0.990573, 'loss_grapheme': 0.177174, 'loss_vowel': 0.141439, 'loss_consonant': 0.093967, 'loss_word': 0.117174}\n",
      "###>>>>> saved\n",
      "   13 | 0.000381 | 160000/160635 | 11.5042 | 10.0651 |\n",
      "val: {'recall': 0.992226, 'recall_grapheme': 0.989272, 'recall_vowel': 0.994549, 'recall_consonant': 0.99581, 'recall_word': 0.989892, 'acc_grapheme': 0.987862, 'acc_vowel': 0.994503, 'acc_consonant': 0.995573, 'acc_word': 0.989852, 'loss_grapheme': 0.277475, 'loss_vowel': 0.221528, 'loss_consonant': 0.132372, 'loss_word': 0.169297}\n",
      "   14 | 0.000378 | 160000/160635 | 3.6849 | 10.1924 ||\n",
      "val: {'recall': 0.99284, 'recall_grapheme': 0.989853, 'recall_vowel': 0.99539, 'recall_consonant': 0.996265, 'recall_word': 0.990272, 'acc_grapheme': 0.988658, 'acc_vowel': 0.995125, 'acc_consonant': 0.996045, 'acc_word': 0.99025, 'loss_grapheme': 0.227748, 'loss_vowel': 0.168419, 'loss_consonant': 0.110215, 'loss_word': 0.145002}\n",
      "   15 | 0.000375 | 160000/160635 | 12.0697 | 9.7864 ||\n",
      "val: {'recall': 0.992956, 'recall_grapheme': 0.991033, 'recall_vowel': 0.995346, 'recall_consonant': 0.994413, 'recall_word': 0.990951, 'acc_grapheme': 0.989827, 'acc_vowel': 0.995598, 'acc_consonant': 0.996692, 'acc_word': 0.990946, 'loss_grapheme': 0.174855, 'loss_vowel': 0.132498, 'loss_consonant': 0.088829, 'loss_word': 0.119938}\n",
      "###>>>>> saved\n",
      "   16 | 0.000372 | 160000/160635 | 11.7119 | 9.9208 ||\n",
      "val: {'recall': 0.992807, 'recall_grapheme': 0.990273, 'recall_vowel': 0.995369, 'recall_consonant': 0.995316, 'recall_word': 0.989923, 'acc_grapheme': 0.989006, 'acc_vowel': 0.99515, 'acc_consonant': 0.996319, 'acc_word': 0.989976, 'loss_grapheme': 0.23277, 'loss_vowel': 0.172198, 'loss_consonant': 0.11093, 'loss_word': 0.155006}\n",
      "   17 | 0.000369 | 160000/160635 | 13.9626 | 10.0381 |\n",
      "val: {'recall': 0.991391, 'recall_grapheme': 0.987741, 'recall_vowel': 0.994045, 'recall_consonant': 0.996037, 'recall_word': 0.989613, 'acc_grapheme': 0.986693, 'acc_vowel': 0.993459, 'acc_consonant': 0.994354, 'acc_word': 0.989554, 'loss_grapheme': 0.313285, 'loss_vowel': 0.249873, 'loss_consonant': 0.160817, 'loss_word': 0.168622}\n",
      "   18 | 0.000365 | 160000/160635 | 12.1919 | 9.7373 ||\n",
      "val: {'recall': 0.992331, 'recall_grapheme': 0.989228, 'recall_vowel': 0.994892, 'recall_consonant': 0.995974, 'recall_word': 0.989433, 'acc_grapheme': 0.988161, 'acc_vowel': 0.994802, 'acc_consonant': 0.995598, 'acc_word': 0.989479, 'loss_grapheme': 0.284573, 'loss_vowel': 0.214611, 'loss_consonant': 0.145287, 'loss_word': 0.1714}\n",
      "   19 | 0.000362 | 160000/160635 | 4.3841 | 9.9413 |||\n",
      "val: {'recall': 0.992341, 'recall_grapheme': 0.989285, 'recall_vowel': 0.994946, 'recall_consonant': 0.995847, 'recall_word': 0.990461, 'acc_grapheme': 0.988161, 'acc_vowel': 0.994901, 'acc_consonant': 0.995548, 'acc_word': 0.990424, 'loss_grapheme': 0.270443, 'loss_vowel': 0.217641, 'loss_consonant': 0.135263, 'loss_word': 0.14764}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20 | 0.000358 | 160000/160635 | 8.7820 | 9.9976 | |\n",
      "val: {'recall': 0.992955, 'recall_grapheme': 0.990539, 'recall_vowel': 0.995185, 'recall_consonant': 0.995559, 'recall_word': 0.991181, 'acc_grapheme': 0.989379, 'acc_vowel': 0.995647, 'acc_consonant': 0.996344, 'acc_word': 0.99127, 'loss_grapheme': 0.24776, 'loss_vowel': 0.199267, 'loss_consonant': 0.132784, 'loss_word': 0.147324}\n",
      "   21 | 0.000354 | 160000/160635 | 7.8672 | 9.7938 ||\n",
      "val: {'recall': 0.99301, 'recall_grapheme': 0.990275, 'recall_vowel': 0.994856, 'recall_consonant': 0.996636, 'recall_word': 0.990716, 'acc_grapheme': 0.989031, 'acc_vowel': 0.995249, 'acc_consonant': 0.995946, 'acc_word': 0.990723, 'loss_grapheme': 0.254739, 'loss_vowel': 0.201508, 'loss_consonant': 0.127749, 'loss_word': 0.157405}\n",
      "###>>>>> saved\n",
      "   22 | 0.000350 | 160000/160635 | 13.4232 | 10.5280 |\n",
      "val: {'recall': 0.992497, 'recall_grapheme': 0.989834, 'recall_vowel': 0.994887, 'recall_consonant': 0.995432, 'recall_word': 0.991315, 'acc_grapheme': 0.989479, 'acc_vowel': 0.995374, 'acc_consonant': 0.996344, 'acc_word': 0.991369, 'loss_grapheme': 0.217134, 'loss_vowel': 0.17493, 'loss_consonant': 0.11313, 'loss_word': 0.136023}\n",
      "   23 | 0.000346 | 160000/160635 | 10.3648 | 9.5530 |\n",
      "val: {'recall': 0.992608, 'recall_grapheme': 0.9907, 'recall_vowel': 0.995347, 'recall_consonant': 0.993685, 'recall_word': 0.991001, 'acc_grapheme': 0.989728, 'acc_vowel': 0.995598, 'acc_consonant': 0.99612, 'acc_word': 0.991021, 'loss_grapheme': 0.232655, 'loss_vowel': 0.186447, 'loss_consonant': 0.119793, 'loss_word': 0.143409}\n",
      "   24 | 0.000341 | 160000/160635 | 10.9972 | 10.0844 |\n",
      "val: {'recall': 0.992868, 'recall_grapheme': 0.990592, 'recall_vowel': 0.99527, 'recall_consonant': 0.99502, 'recall_word': 0.990902, 'acc_grapheme': 0.990126, 'acc_vowel': 0.995523, 'acc_consonant': 0.996319, 'acc_word': 0.991021, 'loss_grapheme': 0.233846, 'loss_vowel': 0.19012, 'loss_consonant': 0.133287, 'loss_word': 0.147797}\n",
      "   25 | 0.000337 | 160000/160635 | 12.9120 | 9.8394 ||\n",
      "val: {'recall': 0.993071, 'recall_grapheme': 0.991129, 'recall_vowel': 0.995452, 'recall_consonant': 0.994573, 'recall_word': 0.991498, 'acc_grapheme': 0.990698, 'acc_vowel': 0.995797, 'acc_consonant': 0.996742, 'acc_word': 0.991543, 'loss_grapheme': 0.179079, 'loss_vowel': 0.131735, 'loss_consonant': 0.086343, 'loss_word': 0.117945}\n",
      "###>>>>> saved\n",
      "   26 | 0.000332 | 160000/160635 | 8.6894 | 10.1744 ||\n",
      "val: {'recall': 0.992679, 'recall_grapheme': 0.989512, 'recall_vowel': 0.995082, 'recall_consonant': 0.996609, 'recall_word': 0.991062, 'acc_grapheme': 0.989031, 'acc_vowel': 0.995249, 'acc_consonant': 0.996195, 'acc_word': 0.991096, 'loss_grapheme': 0.262381, 'loss_vowel': 0.221425, 'loss_consonant': 0.143465, 'loss_word': 0.144265}\n",
      "   27 | 0.000328 | 160000/160635 | 9.0925 | 10.3688 ||\n",
      "val: {'recall': 0.993161, 'recall_grapheme': 0.990432, 'recall_vowel': 0.995363, 'recall_consonant': 0.996417, 'recall_word': 0.991344, 'acc_grapheme': 0.989578, 'acc_vowel': 0.995274, 'acc_consonant': 0.996393, 'acc_word': 0.991344, 'loss_grapheme': 0.272649, 'loss_vowel': 0.214437, 'loss_consonant': 0.146355, 'loss_word': 0.16073}\n",
      "###>>>>> saved\n",
      "   28 | 0.000323 | 160000/160635 | 13.6908 | 10.0265 |\n",
      "val: {'recall': 0.993373, 'recall_grapheme': 0.990828, 'recall_vowel': 0.995274, 'recall_consonant': 0.996561, 'recall_word': 0.991137, 'acc_grapheme': 0.989628, 'acc_vowel': 0.995448, 'acc_consonant': 0.996369, 'acc_word': 0.99122, 'loss_grapheme': 0.262583, 'loss_vowel': 0.202202, 'loss_consonant': 0.129759, 'loss_word': 0.162574}\n",
      "###>>>>> saved\n",
      "   29 | 0.000318 | 160000/160635 | 12.7567 | 10.0457 |\n",
      "val: {'recall': 0.992804, 'recall_grapheme': 0.990085, 'recall_vowel': 0.995472, 'recall_consonant': 0.995574, 'recall_word': 0.991338, 'acc_grapheme': 0.989205, 'acc_vowel': 0.995498, 'acc_consonant': 0.995996, 'acc_word': 0.991419, 'loss_grapheme': 0.283145, 'loss_vowel': 0.238563, 'loss_consonant': 0.157477, 'loss_word': 0.143642}\n",
      "   30 | 0.000312 | 160000/160635 | 14.1878 | 9.5747 ||\n",
      "val: {'recall': 0.993051, 'recall_grapheme': 0.990516, 'recall_vowel': 0.994936, 'recall_consonant': 0.996238, 'recall_word': 0.991585, 'acc_grapheme': 0.989529, 'acc_vowel': 0.9951, 'acc_consonant': 0.996145, 'acc_word': 0.991618, 'loss_grapheme': 0.258909, 'loss_vowel': 0.205917, 'loss_consonant': 0.136179, 'loss_word': 0.144668}\n",
      "   31 | 0.000307 | 160000/160635 | 13.6699 | 9.6918 ||\n",
      "val: {'recall': 0.992987, 'recall_grapheme': 0.990059, 'recall_vowel': 0.995403, 'recall_consonant': 0.996428, 'recall_word': 0.990957, 'acc_grapheme': 0.989379, 'acc_vowel': 0.995498, 'acc_consonant': 0.996468, 'acc_word': 0.991071, 'loss_grapheme': 0.233756, 'loss_vowel': 0.185074, 'loss_consonant': 0.119017, 'loss_word': 0.129692}\n",
      "   32 | 0.000302 | 160000/160635 | 6.6460 | 9.5218 |||\n",
      "val: {'recall': 0.993647, 'recall_grapheme': 0.9912, 'recall_vowel': 0.995481, 'recall_consonant': 0.996708, 'recall_word': 0.991622, 'acc_grapheme': 0.990673, 'acc_vowel': 0.995846, 'acc_consonant': 0.99699, 'acc_word': 0.991618, 'loss_grapheme': 0.198292, 'loss_vowel': 0.153833, 'loss_consonant': 0.098364, 'loss_word': 0.132768}\n",
      "###>>>>> saved\n",
      "   33 | 0.000296 | 160000/160635 | 6.6773 | 9.6933 |||\n",
      "val: {'recall': 0.994276, 'recall_grapheme': 0.992389, 'recall_vowel': 0.995727, 'recall_consonant': 0.996601, 'recall_word': 0.991616, 'acc_grapheme': 0.991494, 'acc_vowel': 0.995946, 'acc_consonant': 0.99699, 'acc_word': 0.991717, 'loss_grapheme': 0.189029, 'loss_vowel': 0.140657, 'loss_consonant': 0.090103, 'loss_word': 0.137277}\n",
      "###>>>>> saved\n",
      "   34 | 0.000291 | 160000/160635 | 10.0072 | 9.9656 ||\n",
      "val: {'recall': 0.993803, 'recall_grapheme': 0.991466, 'recall_vowel': 0.995419, 'recall_consonant': 0.996862, 'recall_word': 0.992187, 'acc_grapheme': 0.990349, 'acc_vowel': 0.995324, 'acc_consonant': 0.996443, 'acc_word': 0.992265, 'loss_grapheme': 0.272591, 'loss_vowel': 0.227155, 'loss_consonant': 0.148113, 'loss_word': 0.147664}\n",
      "   35 | 0.000285 | 160000/160635 | 13.3702 | 9.7907 ||\n",
      "val: {'recall': 0.993232, 'recall_grapheme': 0.990416, 'recall_vowel': 0.995446, 'recall_consonant': 0.996649, 'recall_word': 0.99127, 'acc_grapheme': 0.990051, 'acc_vowel': 0.995299, 'acc_consonant': 0.996767, 'acc_word': 0.99127, 'loss_grapheme': 0.214602, 'loss_vowel': 0.165284, 'loss_consonant': 0.114251, 'loss_word': 0.133102}\n",
      "   36 | 0.000279 | 160000/160635 | 7.5751 | 9.8057 |||\n",
      "val: {'recall': 0.993006, 'recall_grapheme': 0.990384, 'recall_vowel': 0.995443, 'recall_consonant': 0.99581, 'recall_word': 0.991617, 'acc_grapheme': 0.989877, 'acc_vowel': 0.995697, 'acc_consonant': 0.996219, 'acc_word': 0.991643, 'loss_grapheme': 0.224796, 'loss_vowel': 0.177922, 'loss_consonant': 0.116275, 'loss_word': 0.129126}\n",
      "   37 | 0.000274 | 160000/160635 | 7.9950 | 9.7381 |||\n",
      "val: {'recall': 0.993865, 'recall_grapheme': 0.991568, 'recall_vowel': 0.995447, 'recall_consonant': 0.996877, 'recall_word': 0.991526, 'acc_grapheme': 0.990175, 'acc_vowel': 0.995772, 'acc_consonant': 0.996742, 'acc_word': 0.991568, 'loss_grapheme': 0.22781, 'loss_vowel': 0.186351, 'loss_consonant': 0.120493, 'loss_word': 0.130153}\n",
      "   38 | 0.000268 | 160000/160635 | 3.6710 | 9.9445 |||\n",
      "val: {'recall': 0.994213, 'recall_grapheme': 0.991973, 'recall_vowel': 0.996117, 'recall_consonant': 0.99679, 'recall_word': 0.991593, 'acc_grapheme': 0.991071, 'acc_vowel': 0.995921, 'acc_consonant': 0.996642, 'acc_word': 0.991668, 'loss_grapheme': 0.214006, 'loss_vowel': 0.158804, 'loss_consonant': 0.10313, 'loss_word': 0.144276}\n",
      "   39 | 0.000262 | 160000/160635 | 2.6181 | 9.5211 |||\n",
      "val: {'recall': 0.994162, 'recall_grapheme': 0.992364, 'recall_vowel': 0.995761, 'recall_consonant': 0.99616, 'recall_word': 0.991984, 'acc_grapheme': 0.991494, 'acc_vowel': 0.995996, 'acc_consonant': 0.99699, 'acc_word': 0.992041, 'loss_grapheme': 0.154674, 'loss_vowel': 0.116742, 'loss_consonant': 0.079193, 'loss_word': 0.097421}\n",
      "   40 | 0.000256 | 160000/160635 | 11.0181 | 9.4207 ||\n",
      "val: {'recall': 0.993423, 'recall_grapheme': 0.990895, 'recall_vowel': 0.995322, 'recall_consonant': 0.996581, 'recall_word': 0.99134, 'acc_grapheme': 0.990051, 'acc_vowel': 0.995498, 'acc_consonant': 0.996667, 'acc_word': 0.991394, 'loss_grapheme': 0.252451, 'loss_vowel': 0.200117, 'loss_consonant': 0.128933, 'loss_word': 0.146062}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   41 | 0.000250 | 160000/160635 | 10.8335 | 9.4911 ||\n",
      "val: {'recall': 0.99404, 'recall_grapheme': 0.991965, 'recall_vowel': 0.995324, 'recall_consonant': 0.996904, 'recall_word': 0.991899, 'acc_grapheme': 0.991121, 'acc_vowel': 0.995747, 'acc_consonant': 0.996742, 'acc_word': 0.991991, 'loss_grapheme': 0.225798, 'loss_vowel': 0.175805, 'loss_consonant': 0.115398, 'loss_word': 0.133651}\n",
      "   42 | 0.000244 | 160000/160635 | 12.6424 | 9.3731 ||\n",
      "val: {'recall': 0.993692, 'recall_grapheme': 0.991466, 'recall_vowel': 0.995279, 'recall_consonant': 0.996557, 'recall_word': 0.991887, 'acc_grapheme': 0.990747, 'acc_vowel': 0.995498, 'acc_consonant': 0.996667, 'acc_word': 0.991892, 'loss_grapheme': 0.231146, 'loss_vowel': 0.185245, 'loss_consonant': 0.118939, 'loss_word': 0.121904}\n",
      "   43 | 0.000238 | 160000/160635 | 7.5078 | 10.0733 ||\n",
      "val: {'recall': 0.993566, 'recall_grapheme': 0.991236, 'recall_vowel': 0.995098, 'recall_consonant': 0.996695, 'recall_word': 0.991906, 'acc_grapheme': 0.990648, 'acc_vowel': 0.99515, 'acc_consonant': 0.996468, 'acc_word': 0.991966, 'loss_grapheme': 0.230911, 'loss_vowel': 0.190622, 'loss_consonant': 0.129805, 'loss_word': 0.118827}\n",
      "   44 | 0.000231 | 160000/160635 | 13.1592 | 9.5725 ||\n",
      "val: {'recall': 0.993348, 'recall_grapheme': 0.990486, 'recall_vowel': 0.995457, 'recall_consonant': 0.996965, 'recall_word': 0.991698, 'acc_grapheme': 0.990126, 'acc_vowel': 0.995672, 'acc_consonant': 0.996866, 'acc_word': 0.991792, 'loss_grapheme': 0.214804, 'loss_vowel': 0.179529, 'loss_consonant': 0.1164, 'loss_word': 0.11788}\n",
      "   45 | 0.000225 | 160000/160635 | 12.3198 | 9.1766 ||\n",
      "val: {'recall': 0.993919, 'recall_grapheme': 0.991841, 'recall_vowel': 0.995734, 'recall_consonant': 0.996263, 'recall_word': 0.99177, 'acc_grapheme': 0.990971, 'acc_vowel': 0.995846, 'acc_consonant': 0.996916, 'acc_word': 0.991867, 'loss_grapheme': 0.182102, 'loss_vowel': 0.15093, 'loss_consonant': 0.102593, 'loss_word': 0.111568}\n",
      "   46 | 0.000219 | 160000/160635 | 9.2142 | 9.3909 |||\n",
      "val: {'recall': 0.993968, 'recall_grapheme': 0.991602, 'recall_vowel': 0.995689, 'recall_consonant': 0.99698, 'recall_word': 0.991913, 'acc_grapheme': 0.990922, 'acc_vowel': 0.995921, 'acc_consonant': 0.996692, 'acc_word': 0.991966, 'loss_grapheme': 0.225797, 'loss_vowel': 0.186485, 'loss_consonant': 0.122609, 'loss_word': 0.128285}\n",
      "   47 | 0.000213 | 160000/160635 | 10.7523 | 9.6313 |\n",
      "val: {'recall': 0.993655, 'recall_grapheme': 0.991381, 'recall_vowel': 0.995223, 'recall_consonant': 0.996637, 'recall_word': 0.992342, 'acc_grapheme': 0.990946, 'acc_vowel': 0.995423, 'acc_consonant': 0.996891, 'acc_word': 0.992364, 'loss_grapheme': 0.237703, 'loss_vowel': 0.196602, 'loss_consonant': 0.130956, 'loss_word': 0.127373}\n",
      "   48 | 0.000206 | 160000/160635 | 6.6645 | 9.1435 |||\n",
      "val: {'recall': 0.993615, 'recall_grapheme': 0.99107, 'recall_vowel': 0.995651, 'recall_consonant': 0.996669, 'recall_word': 0.991476, 'acc_grapheme': 0.990499, 'acc_vowel': 0.995846, 'acc_consonant': 0.997165, 'acc_word': 0.991518, 'loss_grapheme': 0.181829, 'loss_vowel': 0.151468, 'loss_consonant': 0.103343, 'loss_word': 0.105263}\n",
      "   49 | 0.000200 | 160000/160635 | 1.5597 | 9.9199 |||\n",
      "val: {'recall': 0.994153, 'recall_grapheme': 0.992055, 'recall_vowel': 0.996112, 'recall_consonant': 0.996389, 'recall_word': 0.992347, 'acc_grapheme': 0.991618, 'acc_vowel': 0.996219, 'acc_consonant': 0.997264, 'acc_word': 0.992439, 'loss_grapheme': 0.153261, 'loss_vowel': 0.121376, 'loss_consonant': 0.08364, 'loss_word': 0.09774}\n",
      "   50 | 0.000194 | 160000/160635 | 12.7318 | 9.6752 |\n",
      "val: {'recall': 0.993548, 'recall_grapheme': 0.991131, 'recall_vowel': 0.995604, 'recall_consonant': 0.996327, 'recall_word': 0.99224, 'acc_grapheme': 0.990673, 'acc_vowel': 0.995896, 'acc_consonant': 0.996816, 'acc_word': 0.99229, 'loss_grapheme': 0.213723, 'loss_vowel': 0.177619, 'loss_consonant': 0.119755, 'loss_word': 0.11491}\n",
      "   51 | 0.000187 | 160000/160635 | 12.8625 | 9.6517 ||\n",
      "val: {'recall': 0.993448, 'recall_grapheme': 0.990859, 'recall_vowel': 0.995601, 'recall_consonant': 0.996475, 'recall_word': 0.99241, 'acc_grapheme': 0.990499, 'acc_vowel': 0.995697, 'acc_consonant': 0.997015, 'acc_word': 0.992513, 'loss_grapheme': 0.228534, 'loss_vowel': 0.192193, 'loss_consonant': 0.126282, 'loss_word': 0.11868}\n",
      "   52 | 0.000181 | 160000/160635 | 12.7010 | 9.4821 ||\n",
      "val: {'recall': 0.994424, 'recall_grapheme': 0.992234, 'recall_vowel': 0.995879, 'recall_consonant': 0.997349, 'recall_word': 0.992673, 'acc_grapheme': 0.991618, 'acc_vowel': 0.995996, 'acc_consonant': 0.997637, 'acc_word': 0.992687, 'loss_grapheme': 0.182684, 'loss_vowel': 0.151152, 'loss_consonant': 0.099227, 'loss_word': 0.110699}\n",
      "###>>>>> saved\n",
      "   53 | 0.000175 | 160000/160635 | 3.8668 | 9.3561 |||\n",
      "val: {'recall': 0.994082, 'recall_grapheme': 0.992026, 'recall_vowel': 0.995837, 'recall_consonant': 0.996441, 'recall_word': 0.9922, 'acc_grapheme': 0.991245, 'acc_vowel': 0.996045, 'acc_consonant': 0.997115, 'acc_word': 0.99229, 'loss_grapheme': 0.198658, 'loss_vowel': 0.161872, 'loss_consonant': 0.104503, 'loss_word': 0.118411}\n",
      "   54 | 0.000169 | 160000/160635 | 9.8241 | 9.6599 |||\n",
      "val: {'recall': 0.993802, 'recall_grapheme': 0.991174, 'recall_vowel': 0.995809, 'recall_consonant': 0.99705, 'recall_word': 0.992053, 'acc_grapheme': 0.990747, 'acc_vowel': 0.995821, 'acc_consonant': 0.99699, 'acc_word': 0.99214, 'loss_grapheme': 0.235028, 'loss_vowel': 0.196576, 'loss_consonant': 0.128347, 'loss_word': 0.13393}\n",
      "   55 | 0.000163 | 160000/160635 | 10.1678 | 9.3377 ||\n",
      "val: {'recall': 0.993974, 'recall_grapheme': 0.991784, 'recall_vowel': 0.995322, 'recall_consonant': 0.997006, 'recall_word': 0.992044, 'acc_grapheme': 0.990971, 'acc_vowel': 0.995747, 'acc_consonant': 0.997115, 'acc_word': 0.992115, 'loss_grapheme': 0.235942, 'loss_vowel': 0.201051, 'loss_consonant': 0.135022, 'loss_word': 0.126925}\n",
      "   56 | 0.000156 | 160000/160635 | 5.0407 | 9.7209 ||\n",
      "val: {'recall': 0.994445, 'recall_grapheme': 0.992393, 'recall_vowel': 0.995812, 'recall_consonant': 0.997182, 'recall_word': 0.99239, 'acc_grapheme': 0.991518, 'acc_vowel': 0.996195, 'acc_consonant': 0.997488, 'acc_word': 0.992439, 'loss_grapheme': 0.170619, 'loss_vowel': 0.129086, 'loss_consonant': 0.08833, 'loss_word': 0.109674}\n",
      "###>>>>> saved\n",
      "   57 | 0.000150 | 160000/160635 | 8.2842 | 9.2758 |||\n",
      "val: {'recall': 0.99374, 'recall_grapheme': 0.991303, 'recall_vowel': 0.995382, 'recall_consonant': 0.996971, 'recall_word': 0.991725, 'acc_grapheme': 0.991195, 'acc_vowel': 0.995797, 'acc_consonant': 0.99709, 'acc_word': 0.991817, 'loss_grapheme': 0.199259, 'loss_vowel': 0.165577, 'loss_consonant': 0.108309, 'loss_word': 0.12015}\n",
      "   58 | 0.000144 | 160000/160635 | 12.4229 | 9.5717 ||\n",
      "val: {'recall': 0.993809, 'recall_grapheme': 0.991642, 'recall_vowel': 0.995074, 'recall_consonant': 0.996877, 'recall_word': 0.992598, 'acc_grapheme': 0.991021, 'acc_vowel': 0.995622, 'acc_consonant': 0.997015, 'acc_word': 0.992638, 'loss_grapheme': 0.240979, 'loss_vowel': 0.196256, 'loss_consonant': 0.127309, 'loss_word': 0.129072}\n",
      "   59 | 0.000138 | 160000/160635 | 13.3817 | 9.4023 ||\n",
      "val: {'recall': 0.993736, 'recall_grapheme': 0.991378, 'recall_vowel': 0.995723, 'recall_consonant': 0.996465, 'recall_word': 0.992428, 'acc_grapheme': 0.991121, 'acc_vowel': 0.995871, 'acc_consonant': 0.99704, 'acc_word': 0.992464, 'loss_grapheme': 0.212764, 'loss_vowel': 0.170839, 'loss_consonant': 0.114067, 'loss_word': 0.122584}\n",
      "   60 | 0.000132 | 160000/160635 | 9.1349 | 9.3936 |||\n",
      "val: {'recall': 0.993716, 'recall_grapheme': 0.991401, 'recall_vowel': 0.995294, 'recall_consonant': 0.996768, 'recall_word': 0.992236, 'acc_grapheme': 0.991096, 'acc_vowel': 0.995747, 'acc_consonant': 0.99699, 'acc_word': 0.99224, 'loss_grapheme': 0.230279, 'loss_vowel': 0.192017, 'loss_consonant': 0.133739, 'loss_word': 0.120058}\n",
      "   61 | 0.000126 | 160000/160635 | 1.7635 | 9.2099 |||\n",
      "val: {'recall': 0.994941, 'recall_grapheme': 0.993583, 'recall_vowel': 0.995711, 'recall_consonant': 0.996884, 'recall_word': 0.992837, 'acc_grapheme': 0.992687, 'acc_vowel': 0.996269, 'acc_consonant': 0.997413, 'acc_word': 0.992886, 'loss_grapheme': 0.140332, 'loss_vowel': 0.108547, 'loss_consonant': 0.075345, 'loss_word': 0.09205}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###>>>>> saved\n",
      "   62 | 0.000121 | 160000/160635 | 12.4054 | 9.0314 ||\n",
      "val: {'recall': 0.993898, 'recall_grapheme': 0.991441, 'recall_vowel': 0.995691, 'recall_consonant': 0.997018, 'recall_word': 0.992546, 'acc_grapheme': 0.991469, 'acc_vowel': 0.99602, 'acc_consonant': 0.997438, 'acc_word': 0.992613, 'loss_grapheme': 0.216529, 'loss_vowel': 0.187651, 'loss_consonant': 0.122937, 'loss_word': 0.118099}\n",
      "   63 | 0.000115 | 160000/160635 | 10.9154 | 9.4967 ||\n",
      "val: {'recall': 0.993406, 'recall_grapheme': 0.990455, 'recall_vowel': 0.995609, 'recall_consonant': 0.997106, 'recall_word': 0.992729, 'acc_grapheme': 0.990872, 'acc_vowel': 0.995946, 'acc_consonant': 0.99709, 'acc_word': 0.992712, 'loss_grapheme': 0.212273, 'loss_vowel': 0.174297, 'loss_consonant': 0.11408, 'loss_word': 0.111553}\n",
      "   64 | 0.000109 | 160000/160635 | 10.9567 | 9.5122 ||\n",
      "val: {'recall': 0.994446, 'recall_grapheme': 0.992557, 'recall_vowel': 0.995568, 'recall_consonant': 0.997102, 'recall_word': 0.992528, 'acc_grapheme': 0.99214, 'acc_vowel': 0.995871, 'acc_consonant': 0.997438, 'acc_word': 0.992563, 'loss_grapheme': 0.165928, 'loss_vowel': 0.132348, 'loss_consonant': 0.093598, 'loss_word': 0.106379}\n",
      "   65 | 0.000104 | 160000/160635 | 10.0198 | 9.5005 |\n",
      "val: {'recall': 0.993635, 'recall_grapheme': 0.991537, 'recall_vowel': 0.995365, 'recall_consonant': 0.996103, 'recall_word': 0.992648, 'acc_grapheme': 0.991319, 'acc_vowel': 0.995797, 'acc_consonant': 0.997015, 'acc_word': 0.992687, 'loss_grapheme': 0.214351, 'loss_vowel': 0.179556, 'loss_consonant': 0.122308, 'loss_word': 0.115264}\n",
      "   66 | 0.000098 | 160000/160635 | 11.4964 | 9.2030 |\n",
      "val: {'recall': 0.994069, 'recall_grapheme': 0.991783, 'recall_vowel': 0.995645, 'recall_consonant': 0.997067, 'recall_word': 0.992257, 'acc_grapheme': 0.991419, 'acc_vowel': 0.995921, 'acc_consonant': 0.997289, 'acc_word': 0.992314, 'loss_grapheme': 0.182035, 'loss_vowel': 0.150631, 'loss_consonant': 0.098993, 'loss_word': 0.102804}\n",
      "   67 | 0.000093 | 160000/160635 | 12.6759 | 9.5738 ||\n",
      "val: {'recall': 0.993922, 'recall_grapheme': 0.991671, 'recall_vowel': 0.99562, 'recall_consonant': 0.996726, 'recall_word': 0.992383, 'acc_grapheme': 0.991394, 'acc_vowel': 0.995722, 'acc_consonant': 0.996767, 'acc_word': 0.992414, 'loss_grapheme': 0.20257, 'loss_vowel': 0.179092, 'loss_consonant': 0.119853, 'loss_word': 0.1134}\n",
      "   68 | 0.000088 | 160000/160635 | 4.8280 | 9.2512 |||\n",
      "val: {'recall': 0.994633, 'recall_grapheme': 0.992713, 'recall_vowel': 0.996127, 'recall_consonant': 0.996978, 'recall_word': 0.992951, 'acc_grapheme': 0.99214, 'acc_vowel': 0.996294, 'acc_consonant': 0.997339, 'acc_word': 0.992986, 'loss_grapheme': 0.182713, 'loss_vowel': 0.150232, 'loss_consonant': 0.10231, 'loss_word': 0.112572}\n",
      "   69 | 0.000082 | 160000/160635 | 12.3538 | 9.1606 |\n",
      "val: {'recall': 0.994276, 'recall_grapheme': 0.992281, 'recall_vowel': 0.995919, 'recall_consonant': 0.996624, 'recall_word': 0.9926, 'acc_grapheme': 0.991867, 'acc_vowel': 0.996195, 'acc_consonant': 0.997388, 'acc_word': 0.992663, 'loss_grapheme': 0.167018, 'loss_vowel': 0.142157, 'loss_consonant': 0.09515, 'loss_word': 0.102001}\n",
      "   70 | 0.000077 | 160000/160635 | 6.5570 | 9.3689 |||\n",
      "val: {'recall': 0.994469, 'recall_grapheme': 0.992519, 'recall_vowel': 0.996311, 'recall_consonant': 0.996525, 'recall_word': 0.992915, 'acc_grapheme': 0.992091, 'acc_vowel': 0.996294, 'acc_consonant': 0.997339, 'acc_word': 0.992986, 'loss_grapheme': 0.159376, 'loss_vowel': 0.136485, 'loss_consonant': 0.089118, 'loss_word': 0.095057}\n",
      "   71 | 0.000073 | 160000/160635 | 12.3331 | 9.3517 ||\n",
      "val: {'recall': 0.994244, 'recall_grapheme': 0.991958, 'recall_vowel': 0.995998, 'recall_consonant': 0.997061, 'recall_word': 0.992461, 'acc_grapheme': 0.991419, 'acc_vowel': 0.996244, 'acc_consonant': 0.997264, 'acc_word': 0.992538, 'loss_grapheme': 0.18332, 'loss_vowel': 0.155956, 'loss_consonant': 0.105378, 'loss_word': 0.104405}\n",
      "   72 | 0.000068 | 160000/160635 | 11.7247 | 9.4438 |\n",
      "val: {'recall': 0.99363, 'recall_grapheme': 0.991168, 'recall_vowel': 0.995083, 'recall_consonant': 0.997101, 'recall_word': 0.992625, 'acc_grapheme': 0.990822, 'acc_vowel': 0.995598, 'acc_consonant': 0.996791, 'acc_word': 0.992638, 'loss_grapheme': 0.231386, 'loss_vowel': 0.209343, 'loss_consonant': 0.140498, 'loss_word': 0.109434}\n",
      "   73 | 0.000063 | 160000/160635 | 10.0709 | 9.3223 |\n",
      "val: {'recall': 0.994264, 'recall_grapheme': 0.992121, 'recall_vowel': 0.995566, 'recall_consonant': 0.997249, 'recall_word': 0.992666, 'acc_grapheme': 0.991767, 'acc_vowel': 0.996145, 'acc_consonant': 0.997538, 'acc_word': 0.992737, 'loss_grapheme': 0.187547, 'loss_vowel': 0.164623, 'loss_consonant': 0.111555, 'loss_word': 0.109045}\n",
      "   74 | 0.000059 | 160000/160635 | 10.9766 | 9.2617 ||\n",
      "val: {'recall': 0.994639, 'recall_grapheme': 0.992774, 'recall_vowel': 0.995829, 'recall_consonant': 0.997181, 'recall_word': 0.992891, 'acc_grapheme': 0.992389, 'acc_vowel': 0.996443, 'acc_consonant': 0.997587, 'acc_word': 0.992936, 'loss_grapheme': 0.150156, 'loss_vowel': 0.128943, 'loss_consonant': 0.085735, 'loss_word': 0.092643}\n",
      "   75 | 0.000054 | 160000/160635 | 11.4716 | 9.2860 |\n",
      "val: {'recall': 0.994499, 'recall_grapheme': 0.992469, 'recall_vowel': 0.995926, 'recall_consonant': 0.997134, 'recall_word': 0.992875, 'acc_grapheme': 0.992016, 'acc_vowel': 0.996195, 'acc_consonant': 0.997388, 'acc_word': 0.992936, 'loss_grapheme': 0.163403, 'loss_vowel': 0.142604, 'loss_consonant': 0.09122, 'loss_word': 0.089498}\n",
      "   76 | 0.000050 | 160000/160635 | 1.3270 | 8.8925 ||\n",
      "val: {'recall': 0.994246, 'recall_grapheme': 0.992237, 'recall_vowel': 0.995439, 'recall_consonant': 0.997072, 'recall_word': 0.992981, 'acc_grapheme': 0.991991, 'acc_vowel': 0.996095, 'acc_consonant': 0.997314, 'acc_word': 0.993011, 'loss_grapheme': 0.161589, 'loss_vowel': 0.139192, 'loss_consonant': 0.091733, 'loss_word': 0.088874}\n",
      "   77 | 0.000046 | 160000/160635 | 9.8096 | 8.9910 |||\n",
      "val: {'recall': 0.994541, 'recall_grapheme': 0.992426, 'recall_vowel': 0.995964, 'recall_consonant': 0.997348, 'recall_word': 0.9929, 'acc_grapheme': 0.991767, 'acc_vowel': 0.996344, 'acc_consonant': 0.997538, 'acc_word': 0.992936, 'loss_grapheme': 0.178411, 'loss_vowel': 0.152548, 'loss_consonant': 0.104368, 'loss_word': 0.105311}\n",
      "   78 | 0.000042 | 160000/160635 | 9.2365 | 9.1279 |||\n",
      "val: {'recall': 0.994336, 'recall_grapheme': 0.992213, 'recall_vowel': 0.995807, 'recall_consonant': 0.997109, 'recall_word': 0.992821, 'acc_grapheme': 0.991494, 'acc_vowel': 0.996195, 'acc_consonant': 0.997115, 'acc_word': 0.992886, 'loss_grapheme': 0.216822, 'loss_vowel': 0.186867, 'loss_consonant': 0.125268, 'loss_word': 0.118789}\n",
      "   79 | 0.000038 | 160000/160635 | 10.0933 | 9.1317 ||\n",
      "val: {'recall': 0.994381, 'recall_grapheme': 0.992244, 'recall_vowel': 0.995839, 'recall_consonant': 0.997198, 'recall_word': 0.992861, 'acc_grapheme': 0.991892, 'acc_vowel': 0.996319, 'acc_consonant': 0.997438, 'acc_word': 0.992911, 'loss_grapheme': 0.172493, 'loss_vowel': 0.147034, 'loss_consonant': 0.100802, 'loss_word': 0.102823}\n",
      "   80 | 0.000035 | 160000/160635 | 11.6887 | 9.1048 ||\n",
      "val: {'recall': 0.994145, 'recall_grapheme': 0.991888, 'recall_vowel': 0.995646, 'recall_consonant': 0.997158, 'recall_word': 0.99295, 'acc_grapheme': 0.991568, 'acc_vowel': 0.996219, 'acc_consonant': 0.997339, 'acc_word': 0.993011, 'loss_grapheme': 0.191098, 'loss_vowel': 0.162842, 'loss_consonant': 0.106668, 'loss_word': 0.102666}\n",
      "   81 | 0.000031 | 160000/160635 | 2.2193 | 9.3112 |||\n",
      "val: {'recall': 0.994393, 'recall_grapheme': 0.992349, 'recall_vowel': 0.995713, 'recall_consonant': 0.997162, 'recall_word': 0.993061, 'acc_grapheme': 0.992016, 'acc_vowel': 0.996244, 'acc_consonant': 0.997463, 'acc_word': 0.993085, 'loss_grapheme': 0.16525, 'loss_vowel': 0.1432, 'loss_consonant': 0.096746, 'loss_word': 0.093464}\n",
      "   82 | 0.000028 | 160000/160635 | 11.9390 | 9.3552 ||\n",
      "val: {'recall': 0.994565, 'recall_grapheme': 0.992569, 'recall_vowel': 0.99574, 'recall_consonant': 0.997381, 'recall_word': 0.993113, 'acc_grapheme': 0.99214, 'acc_vowel': 0.99617, 'acc_consonant': 0.997687, 'acc_word': 0.99316, 'loss_grapheme': 0.169887, 'loss_vowel': 0.149563, 'loss_consonant': 0.096973, 'loss_word': 0.096864}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   83 | 0.000025 | 160000/160635 | 1.7417 | 9.0950 ||\n",
      "val: {'recall': 0.994536, 'recall_grapheme': 0.992502, 'recall_vowel': 0.995866, 'recall_consonant': 0.997276, 'recall_word': 0.99306, 'acc_grapheme': 0.99229, 'acc_vowel': 0.996319, 'acc_consonant': 0.997562, 'acc_word': 0.993135, 'loss_grapheme': 0.156126, 'loss_vowel': 0.138185, 'loss_consonant': 0.088311, 'loss_word': 0.096059}\n",
      "   84 | 0.000022 | 160000/160635 | 11.5469 | 9.0634 |\n",
      "val: {'recall': 0.994213, 'recall_grapheme': 0.991834, 'recall_vowel': 0.995919, 'recall_consonant': 0.997266, 'recall_word': 0.992682, 'acc_grapheme': 0.991693, 'acc_vowel': 0.996319, 'acc_consonant': 0.997538, 'acc_word': 0.992712, 'loss_grapheme': 0.196601, 'loss_vowel': 0.165296, 'loss_consonant': 0.1079, 'loss_word': 0.119002}\n",
      "   85 | 0.000019 | 160000/160635 | 10.2474 | 9.4698 ||\n",
      "val: {'recall': 0.994482, 'recall_grapheme': 0.992396, 'recall_vowel': 0.99588, 'recall_consonant': 0.997256, 'recall_word': 0.992692, 'acc_grapheme': 0.991842, 'acc_vowel': 0.996219, 'acc_consonant': 0.997463, 'acc_word': 0.992737, 'loss_grapheme': 0.18859, 'loss_vowel': 0.158033, 'loss_consonant': 0.105534, 'loss_word': 0.107349}\n",
      "   86 | 0.000016 | 160000/160635 | 6.1207 | 9.0041 ||\n",
      "val: {'recall': 0.99393, 'recall_grapheme': 0.991437, 'recall_vowel': 0.995669, 'recall_consonant': 0.997176, 'recall_word': 0.992693, 'acc_grapheme': 0.991195, 'acc_vowel': 0.99612, 'acc_consonant': 0.997189, 'acc_word': 0.992712, 'loss_grapheme': 0.221146, 'loss_vowel': 0.191816, 'loss_consonant': 0.12504, 'loss_word': 0.112158}\n",
      "   87 | 0.000014 | 160000/160635 | 10.2027 | 9.3289 ||\n",
      "val: {'recall': 0.994616, 'recall_grapheme': 0.992548, 'recall_vowel': 0.996008, 'recall_consonant': 0.997361, 'recall_word': 0.993095, 'acc_grapheme': 0.992215, 'acc_vowel': 0.996468, 'acc_consonant': 0.997811, 'acc_word': 0.993085, 'loss_grapheme': 0.161871, 'loss_vowel': 0.133722, 'loss_consonant': 0.092069, 'loss_word': 0.100735}\n",
      "   88 | 0.000012 | 160000/160635 | 7.1119 | 9.2143 ||\n",
      "val: {'recall': 0.993996, 'recall_grapheme': 0.991621, 'recall_vowel': 0.995663, 'recall_consonant': 0.997081, 'recall_word': 0.992607, 'acc_grapheme': 0.991494, 'acc_vowel': 0.99612, 'acc_consonant': 0.997339, 'acc_word': 0.992638, 'loss_grapheme': 0.188778, 'loss_vowel': 0.165183, 'loss_consonant': 0.108868, 'loss_word': 0.102063}\n",
      "   89 | 0.000010 | 160000/160635 | 3.3585 | 9.2323 ||\n",
      "val: {'recall': 0.995042, 'recall_grapheme': 0.9932, 'recall_vowel': 0.99638, 'recall_consonant': 0.997386, 'recall_word': 0.993183, 'acc_grapheme': 0.993085, 'acc_vowel': 0.996791, 'acc_consonant': 0.997886, 'acc_word': 0.993235, 'loss_grapheme': 0.126014, 'loss_vowel': 0.098738, 'loss_consonant': 0.068079, 'loss_word': 0.086976}\n",
      "###>>>>> saved\n",
      "   90 | 0.000008 | 160000/160635 | 12.7958 | 9.1361 ||\n",
      "val: {'recall': 0.994242, 'recall_grapheme': 0.992048, 'recall_vowel': 0.995676, 'recall_consonant': 0.997196, 'recall_word': 0.992773, 'acc_grapheme': 0.991593, 'acc_vowel': 0.99617, 'acc_consonant': 0.997339, 'acc_word': 0.992812, 'loss_grapheme': 0.193323, 'loss_vowel': 0.168512, 'loss_consonant': 0.110012, 'loss_word': 0.102811}\n",
      "   91 | 0.000006 | 160000/160635 | 9.9836 | 9.4412 ||\n",
      "val: {'recall': 0.994386, 'recall_grapheme': 0.992248, 'recall_vowel': 0.995775, 'recall_consonant': 0.997272, 'recall_word': 0.993051, 'acc_grapheme': 0.991991, 'acc_vowel': 0.996319, 'acc_consonant': 0.997513, 'acc_word': 0.993085, 'loss_grapheme': 0.162551, 'loss_vowel': 0.137452, 'loss_consonant': 0.09128, 'loss_word': 0.096152}\n",
      "   92 | 0.000005 | 160000/160635 | 10.2662 | 9.3923 ||\n",
      "val: {'recall': 0.994442, 'recall_grapheme': 0.992327, 'recall_vowel': 0.995739, 'recall_consonant': 0.997374, 'recall_word': 0.993019, 'acc_grapheme': 0.992066, 'acc_vowel': 0.996418, 'acc_consonant': 0.997562, 'acc_word': 0.993036, 'loss_grapheme': 0.18054, 'loss_vowel': 0.154677, 'loss_consonant': 0.105267, 'loss_word': 0.108019}\n",
      "   93 | 0.000004 | 160000/160635 | 11.4964 | 9.5970 ||\n",
      "val: {'recall': 0.994246, 'recall_grapheme': 0.991992, 'recall_vowel': 0.995612, 'recall_consonant': 0.99739, 'recall_word': 0.992899, 'acc_grapheme': 0.991668, 'acc_vowel': 0.99617, 'acc_consonant': 0.997438, 'acc_word': 0.992936, 'loss_grapheme': 0.192514, 'loss_vowel': 0.167989, 'loss_consonant': 0.113735, 'loss_word': 0.107215}\n",
      "   94 | 0.000002 | 160000/160635 | 11.0100 | 9.4324 ||\n",
      "val: {'recall': 0.994077, 'recall_grapheme': 0.991776, 'recall_vowel': 0.995425, 'recall_consonant': 0.99733, 'recall_word': 0.992641, 'acc_grapheme': 0.991494, 'acc_vowel': 0.99602, 'acc_consonant': 0.997388, 'acc_word': 0.992663, 'loss_grapheme': 0.197575, 'loss_vowel': 0.173621, 'loss_consonant': 0.11586, 'loss_word': 0.104648}\n",
      "   95 | 0.000002 | 160000/160635 | 9.6268 | 8.9912 ||\n",
      "val: {'recall': 0.994387, 'recall_grapheme': 0.992303, 'recall_vowel': 0.995676, 'recall_consonant': 0.997267, 'recall_word': 0.993047, 'acc_grapheme': 0.992091, 'acc_vowel': 0.996219, 'acc_consonant': 0.997538, 'acc_word': 0.993085, 'loss_grapheme': 0.16711, 'loss_vowel': 0.146225, 'loss_consonant': 0.101267, 'loss_word': 0.092817}\n",
      "   96 | 0.000001 | 160000/160635 | 6.9227 | 9.0748 |||\n",
      "val: {'recall': 0.994017, 'recall_grapheme': 0.991694, 'recall_vowel': 0.995624, 'recall_consonant': 0.997055, 'recall_word': 0.992861, 'acc_grapheme': 0.991369, 'acc_vowel': 0.996145, 'acc_consonant': 0.99709, 'acc_word': 0.992862, 'loss_grapheme': 0.210125, 'loss_vowel': 0.182381, 'loss_consonant': 0.124038, 'loss_word': 0.109493}\n",
      "   97 | 0.000000 | 160000/160635 | 11.8493 | 9.1297 |\n",
      "val: {'recall': 0.994153, 'recall_grapheme': 0.991923, 'recall_vowel': 0.995486, 'recall_consonant': 0.997281, 'recall_word': 0.993023, 'acc_grapheme': 0.991717, 'acc_vowel': 0.996095, 'acc_consonant': 0.997388, 'acc_word': 0.993085, 'loss_grapheme': 0.179973, 'loss_vowel': 0.157425, 'loss_consonant': 0.104635, 'loss_word': 0.099476}\n",
      "   98 | 0.000000 | 160000/160635 | 12.4740 | 9.3555 ||\n",
      "val: {'recall': 0.994311, 'recall_grapheme': 0.992069, 'recall_vowel': 0.995905, 'recall_consonant': 0.997198, 'recall_word': 0.992644, 'acc_grapheme': 0.991643, 'acc_vowel': 0.996269, 'acc_consonant': 0.997289, 'acc_word': 0.992663, 'loss_grapheme': 0.205121, 'loss_vowel': 0.17765, 'loss_consonant': 0.117876, 'loss_word': 0.117174}\n",
      "   99 | 0.000000 | 160000/160635 | 9.3513 | 9.0789 ||\n",
      "val: {'recall': 0.994331, 'recall_grapheme': 0.992155, 'recall_vowel': 0.995693, 'recall_consonant': 0.99732, 'recall_word': 0.992833, 'acc_grapheme': 0.992066, 'acc_vowel': 0.996344, 'acc_consonant': 0.997587, 'acc_word': 0.992862, 'loss_grapheme': 0.173128, 'loss_vowel': 0.146718, 'loss_consonant': 0.099489, 'loss_word': 0.106011}\n",
      "CYCLE: 2\n",
      "{'recall': 0.994331, 'recall_grapheme': 0.992155, 'recall_vowel': 0.995693, 'recall_consonant': 0.99732, 'recall_word': 0.992833, 'acc_grapheme': 0.992066, 'acc_vowel': 0.996344, 'acc_consonant': 0.997587, 'acc_word': 0.992862, 'loss_grapheme': 0.173128, 'loss_vowel': 0.146718, 'loss_consonant': 0.099489, 'loss_word': 0.106011}\n",
      "    0 | 0.000040 | 160000/160635 | 11.8413 | 9.3187 |\n",
      "val: {'recall': 0.994148, 'recall_grapheme': 0.991843, 'recall_vowel': 0.995798, 'recall_consonant': 0.997109, 'recall_word': 0.992625, 'acc_grapheme': 0.991593, 'acc_vowel': 0.996195, 'acc_consonant': 0.997339, 'acc_word': 0.992638, 'loss_grapheme': 0.190224, 'loss_vowel': 0.165107, 'loss_consonant': 0.109291, 'loss_word': 0.107595}\n",
      "    1 | 0.000080 | 160000/160635 | 0.7505 | 9.2016 |||\n",
      "val: {'recall': 0.994784, 'recall_grapheme': 0.992744, 'recall_vowel': 0.99624, 'recall_consonant': 0.997408, 'recall_word': 0.993031, 'acc_grapheme': 0.992464, 'acc_vowel': 0.996468, 'acc_consonant': 0.997786, 'acc_word': 0.993085, 'loss_grapheme': 0.148791, 'loss_vowel': 0.119319, 'loss_consonant': 0.082894, 'loss_word': 0.091396}\n",
      "    2 | 0.000120 | 160000/160635 | 7.6594 | 8.8652 |||\n",
      "val: {'recall': 0.994821, 'recall_grapheme': 0.992961, 'recall_vowel': 0.995997, 'recall_consonant': 0.997363, 'recall_word': 0.992963, 'acc_grapheme': 0.992538, 'acc_vowel': 0.996717, 'acc_consonant': 0.997637, 'acc_word': 0.993036, 'loss_grapheme': 0.160561, 'loss_vowel': 0.133449, 'loss_consonant': 0.092177, 'loss_word': 0.106569}\n",
      "    3 | 0.000159 | 160000/160635 | 8.4396 | 9.0686 |||\n",
      "val: {'recall': 0.994256, 'recall_grapheme': 0.992214, 'recall_vowel': 0.995938, 'recall_consonant': 0.996657, 'recall_word': 0.992577, 'acc_grapheme': 0.991941, 'acc_vowel': 0.996219, 'acc_consonant': 0.997488, 'acc_word': 0.992588, 'loss_grapheme': 0.172269, 'loss_vowel': 0.145336, 'loss_consonant': 0.097565, 'loss_word': 0.096289}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    4 | 0.000199 | 160000/160635 | 10.8497 | 9.1854 ||\n",
      "val: {'recall': 0.994246, 'recall_grapheme': 0.992051, 'recall_vowel': 0.995728, 'recall_consonant': 0.997154, 'recall_word': 0.992182, 'acc_grapheme': 0.991543, 'acc_vowel': 0.996145, 'acc_consonant': 0.997264, 'acc_word': 0.99224, 'loss_grapheme': 0.183527, 'loss_vowel': 0.165026, 'loss_consonant': 0.108609, 'loss_word': 0.10669}\n",
      "    5 | 0.000238 | 160000/160635 | 0.5802 | 9.3553 ||\n",
      "val: {'recall': 0.994259, 'recall_grapheme': 0.992047, 'recall_vowel': 0.995899, 'recall_consonant': 0.997044, 'recall_word': 0.992634, 'acc_grapheme': 0.991792, 'acc_vowel': 0.996294, 'acc_consonant': 0.997364, 'acc_word': 0.992712, 'loss_grapheme': 0.185249, 'loss_vowel': 0.157057, 'loss_consonant': 0.105629, 'loss_word': 0.096744}\n",
      "    6 | 0.000276 | 160000/160635 | 9.2080 | 9.1878 |||\n",
      "val: {'recall': 0.994189, 'recall_grapheme': 0.991886, 'recall_vowel': 0.995692, 'recall_consonant': 0.997291, 'recall_word': 0.992335, 'acc_grapheme': 0.991195, 'acc_vowel': 0.995971, 'acc_consonant': 0.997189, 'acc_word': 0.992339, 'loss_grapheme': 0.226037, 'loss_vowel': 0.18708, 'loss_consonant': 0.121229, 'loss_word': 0.129629}\n",
      "    7 | 0.000315 | 160000/160635 | 5.8965 | 9.3339 |||\n",
      "val: {'recall': 0.993027, 'recall_grapheme': 0.99046, 'recall_vowel': 0.994951, 'recall_consonant': 0.996236, 'recall_word': 0.992251, 'acc_grapheme': 0.990723, 'acc_vowel': 0.995573, 'acc_consonant': 0.996543, 'acc_word': 0.992215, 'loss_grapheme': 0.231071, 'loss_vowel': 0.209163, 'loss_consonant': 0.134314, 'loss_word': 0.122214}\n",
      "    8 | 0.000353 | 160000/160635 | 10.7699 | 9.5195 ||\n",
      "val: {'recall': 0.993833, 'recall_grapheme': 0.991514, 'recall_vowel': 0.995798, 'recall_consonant': 0.996506, 'recall_word': 0.9923, 'acc_grapheme': 0.990996, 'acc_vowel': 0.996145, 'acc_consonant': 0.997065, 'acc_word': 0.992339, 'loss_grapheme': 0.200051, 'loss_vowel': 0.177579, 'loss_consonant': 0.120774, 'loss_word': 0.115076}\n",
      "    9 | 0.000390 | 160000/160635 | 9.1868 | 9.2566 |||\n",
      "val: {'recall': 0.994582, 'recall_grapheme': 0.993123, 'recall_vowel': 0.995604, 'recall_consonant': 0.996475, 'recall_word': 0.992656, 'acc_grapheme': 0.992414, 'acc_vowel': 0.996219, 'acc_consonant': 0.997513, 'acc_word': 0.992663, 'loss_grapheme': 0.1547, 'loss_vowel': 0.118581, 'loss_consonant': 0.076532, 'loss_word': 0.110466}\n",
      "   10 | 0.000388 | 160000/160635 | 5.9911 | 9.6288 |||\n",
      "val: {'recall': 0.994377, 'recall_grapheme': 0.992279, 'recall_vowel': 0.995742, 'recall_consonant': 0.997207, 'recall_word': 0.992206, 'acc_grapheme': 0.991444, 'acc_vowel': 0.996244, 'acc_consonant': 0.997413, 'acc_word': 0.992265, 'loss_grapheme': 0.199311, 'loss_vowel': 0.165413, 'loss_consonant': 0.113096, 'loss_word': 0.118699}\n",
      "   11 | 0.000386 | 160000/160635 | 11.6687 | 9.8225 ||\n",
      "val: {'recall': 0.994064, 'recall_grapheme': 0.991952, 'recall_vowel': 0.995481, 'recall_consonant': 0.996869, 'recall_word': 0.99187, 'acc_grapheme': 0.990996, 'acc_vowel': 0.995921, 'acc_consonant': 0.997189, 'acc_word': 0.991916, 'loss_grapheme': 0.203124, 'loss_vowel': 0.172853, 'loss_consonant': 0.121212, 'loss_word': 0.125205}\n",
      "   12 | 0.000384 | 160000/160635 | 8.2200 | 9.2976 |||\n",
      "val: {'recall': 0.994107, 'recall_grapheme': 0.992046, 'recall_vowel': 0.995827, 'recall_consonant': 0.996507, 'recall_word': 0.992396, 'acc_grapheme': 0.991394, 'acc_vowel': 0.99617, 'acc_consonant': 0.99709, 'acc_word': 0.992314, 'loss_grapheme': 0.185514, 'loss_vowel': 0.150503, 'loss_consonant': 0.101532, 'loss_word': 0.112189}\n",
      "   13 | 0.000381 | 160000/160635 | 0.4461 | 9.5575 |||\n",
      "val: {'recall': 0.994676, 'recall_grapheme': 0.992605, 'recall_vowel': 0.996382, 'recall_consonant': 0.997112, 'recall_word': 0.992845, 'acc_grapheme': 0.992165, 'acc_vowel': 0.996642, 'acc_consonant': 0.997662, 'acc_word': 0.992961, 'loss_grapheme': 0.13767, 'loss_vowel': 0.109262, 'loss_consonant': 0.077241, 'loss_word': 0.084678}\n",
      "   14 | 0.000378 | 160000/160635 | 4.7091 | 9.3133 |||\n",
      "val: {'recall': 0.993716, 'recall_grapheme': 0.991837, 'recall_vowel': 0.995243, 'recall_consonant': 0.995948, 'recall_word': 0.992519, 'acc_grapheme': 0.99117, 'acc_vowel': 0.995672, 'acc_consonant': 0.99709, 'acc_word': 0.992538, 'loss_grapheme': 0.197511, 'loss_vowel': 0.160164, 'loss_consonant': 0.110685, 'loss_word': 0.114169}\n",
      "   15 | 0.000375 | 160000/160635 | 10.1306 | 8.9873 ||\n",
      "val: {'recall': 0.994479, 'recall_grapheme': 0.992855, 'recall_vowel': 0.99584, 'recall_consonant': 0.996365, 'recall_word': 0.992443, 'acc_grapheme': 0.992215, 'acc_vowel': 0.996269, 'acc_consonant': 0.997264, 'acc_word': 0.992538, 'loss_grapheme': 0.155335, 'loss_vowel': 0.123607, 'loss_consonant': 0.081148, 'loss_word': 0.100487}\n",
      "   16 | 0.000372 | 160000/160635 | 8.2795 | 9.5352 |||\n",
      "val: {'recall': 0.993667, 'recall_grapheme': 0.991666, 'recall_vowel': 0.995932, 'recall_consonant': 0.995405, 'recall_word': 0.992101, 'acc_grapheme': 0.991444, 'acc_vowel': 0.996294, 'acc_consonant': 0.996816, 'acc_word': 0.99214, 'loss_grapheme': 0.198417, 'loss_vowel': 0.161965, 'loss_consonant': 0.107032, 'loss_word': 0.122523}\n",
      "   17 | 0.000369 | 160000/160635 | 11.3032 | 8.9654 ||\n",
      "val: {'recall': 0.993723, 'recall_grapheme': 0.991132, 'recall_vowel': 0.995553, 'recall_consonant': 0.997074, 'recall_word': 0.991859, 'acc_grapheme': 0.990723, 'acc_vowel': 0.995672, 'acc_consonant': 0.996692, 'acc_word': 0.991916, 'loss_grapheme': 0.234425, 'loss_vowel': 0.203159, 'loss_consonant': 0.132996, 'loss_word': 0.129034}\n",
      "   18 | 0.000365 | 160000/160635 | 9.6845 | 9.6894 |||\n",
      "val: {'recall': 0.993987, 'recall_grapheme': 0.99228, 'recall_vowel': 0.996263, 'recall_consonant': 0.995123, 'recall_word': 0.992485, 'acc_grapheme': 0.991344, 'acc_vowel': 0.996592, 'acc_consonant': 0.99699, 'acc_word': 0.992488, 'loss_grapheme': 0.174012, 'loss_vowel': 0.158088, 'loss_consonant': 0.106132, 'loss_word': 0.108187}\n",
      "   19 | 0.000362 | 160000/160635 | 4.5437 | 9.1782 |||\n",
      "val: {'recall': 0.99454, 'recall_grapheme': 0.992767, 'recall_vowel': 0.995744, 'recall_consonant': 0.99688, 'recall_word': 0.992613, 'acc_grapheme': 0.991916, 'acc_vowel': 0.996195, 'acc_consonant': 0.997388, 'acc_word': 0.992613, 'loss_grapheme': 0.188308, 'loss_vowel': 0.155664, 'loss_consonant': 0.10347, 'loss_word': 0.115513}\n",
      "   20 | 0.000358 | 160000/160635 | 13.5634 | 9.5702 ||\n",
      "val: {'recall': 0.9937, 'recall_grapheme': 0.991313, 'recall_vowel': 0.995567, 'recall_consonant': 0.996608, 'recall_word': 0.991999, 'acc_grapheme': 0.990723, 'acc_vowel': 0.995797, 'acc_consonant': 0.99704, 'acc_word': 0.991941, 'loss_grapheme': 0.237242, 'loss_vowel': 0.214491, 'loss_consonant': 0.134705, 'loss_word': 0.115887}\n",
      "   21 | 0.000354 | 160000/160635 | 12.1615 | 9.5157 ||\n",
      "val: {'recall': 0.994134, 'recall_grapheme': 0.991897, 'recall_vowel': 0.995823, 'recall_consonant': 0.996918, 'recall_word': 0.992469, 'acc_grapheme': 0.991319, 'acc_vowel': 0.996045, 'acc_consonant': 0.996891, 'acc_word': 0.992488, 'loss_grapheme': 0.228841, 'loss_vowel': 0.199198, 'loss_consonant': 0.135077, 'loss_word': 0.123571}\n",
      "   22 | 0.000350 | 160000/160635 | 12.8217 | 9.4103 ||\n",
      "val: {'recall': 0.994226, 'recall_grapheme': 0.991662, 'recall_vowel': 0.996361, 'recall_consonant': 0.997218, 'recall_word': 0.992551, 'acc_grapheme': 0.991593, 'acc_vowel': 0.996493, 'acc_consonant': 0.997339, 'acc_word': 0.992538, 'loss_grapheme': 0.187871, 'loss_vowel': 0.163388, 'loss_consonant': 0.109596, 'loss_word': 0.109989}\n",
      "   23 | 0.000346 | 160000/160635 | 5.0318 | 9.6627 |||\n",
      "val: {'recall': 0.994492, 'recall_grapheme': 0.992635, 'recall_vowel': 0.996044, 'recall_consonant': 0.996651, 'recall_word': 0.992356, 'acc_grapheme': 0.992016, 'acc_vowel': 0.996518, 'acc_consonant': 0.997513, 'acc_word': 0.992389, 'loss_grapheme': 0.126507, 'loss_vowel': 0.099941, 'loss_consonant': 0.069866, 'loss_word': 0.090452}\n",
      "   24 | 0.000341 | 160000/160635 | 8.9192 | 9.5586 |||\n",
      "val: {'recall': 0.994813, 'recall_grapheme': 0.992625, 'recall_vowel': 0.996582, 'recall_consonant': 0.997418, 'recall_word': 0.993095, 'acc_grapheme': 0.992414, 'acc_vowel': 0.996617, 'acc_consonant': 0.997861, 'acc_word': 0.993085, 'loss_grapheme': 0.184735, 'loss_vowel': 0.157543, 'loss_consonant': 0.104214, 'loss_word': 0.113774}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   25 | 0.000337 | 160000/160635 | 11.5827 | 9.1823 ||\n",
      "val: {'recall': 0.994926, 'recall_grapheme': 0.99306, 'recall_vowel': 0.996299, 'recall_consonant': 0.997286, 'recall_word': 0.992693, 'acc_grapheme': 0.992165, 'acc_vowel': 0.996543, 'acc_consonant': 0.997861, 'acc_word': 0.992712, 'loss_grapheme': 0.157702, 'loss_vowel': 0.12565, 'loss_consonant': 0.086099, 'loss_word': 0.108678}\n",
      "   26 | 0.000332 | 160000/160635 | 13.6637 | 9.2863 ||\n",
      "val: {'recall': 0.994934, 'recall_grapheme': 0.993271, 'recall_vowel': 0.996165, 'recall_consonant': 0.997028, 'recall_word': 0.993128, 'acc_grapheme': 0.992687, 'acc_vowel': 0.996692, 'acc_consonant': 0.997662, 'acc_word': 0.993135, 'loss_grapheme': 0.127337, 'loss_vowel': 0.105337, 'loss_consonant': 0.069914, 'loss_word': 0.093382}\n",
      "   27 | 0.000328 | 160000/160635 | 9.8412 | 9.2383 |||\n",
      "val: {'recall': 0.99472, 'recall_grapheme': 0.99259, 'recall_vowel': 0.996485, 'recall_consonant': 0.997212, 'recall_word': 0.993065, 'acc_grapheme': 0.992414, 'acc_vowel': 0.996493, 'acc_consonant': 0.997364, 'acc_word': 0.993085, 'loss_grapheme': 0.197355, 'loss_vowel': 0.169916, 'loss_consonant': 0.113076, 'loss_word': 0.110564}\n",
      "   28 | 0.000323 | 160000/160635 | 10.7099 | 9.3032 ||\n",
      "val: {'recall': 0.99483, 'recall_grapheme': 0.993134, 'recall_vowel': 0.995969, 'recall_consonant': 0.997083, 'recall_word': 0.992947, 'acc_grapheme': 0.99219, 'acc_vowel': 0.996393, 'acc_consonant': 0.997513, 'acc_word': 0.992961, 'loss_grapheme': 0.179257, 'loss_vowel': 0.145826, 'loss_consonant': 0.100204, 'loss_word': 0.119219}\n",
      "   29 | 0.000318 | 160000/160635 | 11.6922 | 9.1245 ||\n",
      "val: {'recall': 0.99447, 'recall_grapheme': 0.992358, 'recall_vowel': 0.995764, 'recall_consonant': 0.997401, 'recall_word': 0.99322, 'acc_grapheme': 0.992389, 'acc_vowel': 0.996219, 'acc_consonant': 0.997314, 'acc_word': 0.993235, 'loss_grapheme': 0.193625, 'loss_vowel': 0.163981, 'loss_consonant': 0.110633, 'loss_word': 0.109164}\n",
      "   30 | 0.000312 | 160000/160635 | 11.0267 | 9.2187 |\n",
      "val: {'recall': 0.994179, 'recall_grapheme': 0.992025, 'recall_vowel': 0.995521, 'recall_consonant': 0.997144, 'recall_word': 0.99257, 'acc_grapheme': 0.991469, 'acc_vowel': 0.995921, 'acc_consonant': 0.99714, 'acc_word': 0.992638, 'loss_grapheme': 0.213375, 'loss_vowel': 0.19013, 'loss_consonant': 0.122828, 'loss_word': 0.116857}\n",
      "   31 | 0.000307 | 160000/160635 | 12.4167 | 9.3407 |\n",
      "val: {'recall': 0.994521, 'recall_grapheme': 0.992694, 'recall_vowel': 0.996139, 'recall_consonant': 0.996558, 'recall_word': 0.993451, 'acc_grapheme': 0.99219, 'acc_vowel': 0.996468, 'acc_consonant': 0.997015, 'acc_word': 0.993434, 'loss_grapheme': 0.180727, 'loss_vowel': 0.15978, 'loss_consonant': 0.10506, 'loss_word': 0.096689}\n",
      "   32 | 0.000302 | 160000/160635 | 12.0089 | 9.1397 |\n",
      "val: {'recall': 0.993771, 'recall_grapheme': 0.99154, 'recall_vowel': 0.996165, 'recall_consonant': 0.995841, 'recall_word': 0.992228, 'acc_grapheme': 0.991046, 'acc_vowel': 0.996195, 'acc_consonant': 0.99699, 'acc_word': 0.99224, 'loss_grapheme': 0.183304, 'loss_vowel': 0.162089, 'loss_consonant': 0.107348, 'loss_word': 0.103779}\n",
      "   33 | 0.000296 | 160000/160635 | 10.9751 | 9.3895 ||\n",
      "val: {'recall': 0.994923, 'recall_grapheme': 0.993473, 'recall_vowel': 0.995851, 'recall_consonant': 0.996894, 'recall_word': 0.992913, 'acc_grapheme': 0.992513, 'acc_vowel': 0.996369, 'acc_consonant': 0.997538, 'acc_word': 0.992986, 'loss_grapheme': 0.158009, 'loss_vowel': 0.13243, 'loss_consonant': 0.091611, 'loss_word': 0.096403}\n",
      "   34 | 0.000291 | 160000/160635 | 13.3189 | 9.3498 ||\n",
      "val: {'recall': 0.9951, 'recall_grapheme': 0.993563, 'recall_vowel': 0.996013, 'recall_consonant': 0.997261, 'recall_word': 0.993414, 'acc_grapheme': 0.993061, 'acc_vowel': 0.996767, 'acc_consonant': 0.997936, 'acc_word': 0.993483, 'loss_grapheme': 0.146217, 'loss_vowel': 0.121808, 'loss_consonant': 0.080719, 'loss_word': 0.09264}\n",
      "###>>>>> saved\n",
      "   35 | 0.000285 | 160000/160635 | 11.8242 | 9.1873 |\n",
      "val: {'recall': 0.994853, 'recall_grapheme': 0.993434, 'recall_vowel': 0.996039, 'recall_consonant': 0.996505, 'recall_word': 0.993362, 'acc_grapheme': 0.992712, 'acc_vowel': 0.996568, 'acc_consonant': 0.997413, 'acc_word': 0.993359, 'loss_grapheme': 0.153887, 'loss_vowel': 0.134774, 'loss_consonant': 0.090454, 'loss_word': 0.09529}\n",
      "   36 | 0.000279 | 160000/160635 | 10.8059 | 9.3544 ||\n",
      "val: {'recall': 0.995006, 'recall_grapheme': 0.993001, 'recall_vowel': 0.996691, 'recall_consonant': 0.997332, 'recall_word': 0.993135, 'acc_grapheme': 0.992538, 'acc_vowel': 0.996891, 'acc_consonant': 0.997861, 'acc_word': 0.99326, 'loss_grapheme': 0.125537, 'loss_vowel': 0.110513, 'loss_consonant': 0.080142, 'loss_word': 0.076478}\n",
      "   37 | 0.000274 | 160000/160635 | 6.4471 | 9.3746 |||\n",
      "val: {'recall': 0.994724, 'recall_grapheme': 0.993042, 'recall_vowel': 0.995893, 'recall_consonant': 0.996918, 'recall_word': 0.993446, 'acc_grapheme': 0.992165, 'acc_vowel': 0.996493, 'acc_consonant': 0.997339, 'acc_word': 0.993409, 'loss_grapheme': 0.160635, 'loss_vowel': 0.142966, 'loss_consonant': 0.097621, 'loss_word': 0.091571}\n",
      "   38 | 0.000268 | 160000/160635 | 10.3577 | 8.9553 ||\n",
      "val: {'recall': 0.994252, 'recall_grapheme': 0.991971, 'recall_vowel': 0.99625, 'recall_consonant': 0.996817, 'recall_word': 0.993147, 'acc_grapheme': 0.991792, 'acc_vowel': 0.996592, 'acc_consonant': 0.997189, 'acc_word': 0.993135, 'loss_grapheme': 0.166085, 'loss_vowel': 0.142476, 'loss_consonant': 0.10137, 'loss_word': 0.093247}\n",
      "   39 | 0.000262 | 160000/160635 | 2.8026 | 9.2486 |||\n",
      "val: {'recall': 0.995673, 'recall_grapheme': 0.994414, 'recall_vowel': 0.996951, 'recall_consonant': 0.996913, 'recall_word': 0.993476, 'acc_grapheme': 0.993533, 'acc_vowel': 0.997065, 'acc_consonant': 0.99806, 'acc_word': 0.993508, 'loss_grapheme': 0.122245, 'loss_vowel': 0.092656, 'loss_consonant': 0.06465, 'loss_word': 0.089253}\n",
      "###>>>>> saved\n",
      "   40 | 0.000256 | 160000/160635 | 9.8099 | 9.2152 |||\n",
      "val: {'recall': 0.994602, 'recall_grapheme': 0.992708, 'recall_vowel': 0.996132, 'recall_consonant': 0.996861, 'recall_word': 0.993529, 'acc_grapheme': 0.99229, 'acc_vowel': 0.996717, 'acc_consonant': 0.997687, 'acc_word': 0.993508, 'loss_grapheme': 0.147825, 'loss_vowel': 0.128075, 'loss_consonant': 0.094059, 'loss_word': 0.083671}\n",
      "   41 | 0.000250 | 160000/160635 | 12.1704 | 9.1486 ||\n",
      "val: {'recall': 0.994347, 'recall_grapheme': 0.992407, 'recall_vowel': 0.995767, 'recall_consonant': 0.996807, 'recall_word': 0.992538, 'acc_grapheme': 0.991767, 'acc_vowel': 0.996145, 'acc_consonant': 0.997214, 'acc_word': 0.992464, 'loss_grapheme': 0.189517, 'loss_vowel': 0.170662, 'loss_consonant': 0.117316, 'loss_word': 0.102258}\n",
      "   42 | 0.000244 | 160000/160635 | 8.3278 | 9.1313 |||\n",
      "val: {'recall': 0.995006, 'recall_grapheme': 0.993162, 'recall_vowel': 0.996227, 'recall_consonant': 0.997473, 'recall_word': 0.994041, 'acc_grapheme': 0.992812, 'acc_vowel': 0.996667, 'acc_consonant': 0.997687, 'acc_word': 0.994055, 'loss_grapheme': 0.156936, 'loss_vowel': 0.140205, 'loss_consonant': 0.099354, 'loss_word': 0.082469}\n",
      "   43 | 0.000238 | 160000/160635 | 12.4234 | 9.1484 ||\n",
      "val: {'recall': 0.995357, 'recall_grapheme': 0.99406, 'recall_vowel': 0.996308, 'recall_consonant': 0.996998, 'recall_word': 0.99362, 'acc_grapheme': 0.993284, 'acc_vowel': 0.996841, 'acc_consonant': 0.997985, 'acc_word': 0.993633, 'loss_grapheme': 0.144704, 'loss_vowel': 0.125625, 'loss_consonant': 0.085533, 'loss_word': 0.099128}\n",
      "   44 | 0.000231 | 160000/160635 | 7.0099 | 8.9993 |||\n",
      "val: {'recall': 0.99454, 'recall_grapheme': 0.992369, 'recall_vowel': 0.995932, 'recall_consonant': 0.99749, 'recall_word': 0.993393, 'acc_grapheme': 0.99219, 'acc_vowel': 0.996468, 'acc_consonant': 0.997761, 'acc_word': 0.993459, 'loss_grapheme': 0.19697, 'loss_vowel': 0.170039, 'loss_consonant': 0.115045, 'loss_word': 0.108658}\n",
      "   45 | 0.000225 | 160000/160635 | 11.7492 | 8.8648 |\n",
      "val: {'recall': 0.995513, 'recall_grapheme': 0.994125, 'recall_vowel': 0.996758, 'recall_consonant': 0.997043, 'recall_word': 0.993789, 'acc_grapheme': 0.993309, 'acc_vowel': 0.997065, 'acc_consonant': 0.998035, 'acc_word': 0.993807, 'loss_grapheme': 0.126544, 'loss_vowel': 0.112867, 'loss_consonant': 0.081759, 'loss_word': 0.083042}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   46 | 0.000219 | 160000/160635 | 12.5132 | 9.0792 ||\n",
      "val: {'recall': 0.995411, 'recall_grapheme': 0.994242, 'recall_vowel': 0.996855, 'recall_consonant': 0.996306, 'recall_word': 0.993697, 'acc_grapheme': 0.993284, 'acc_vowel': 0.99704, 'acc_consonant': 0.997836, 'acc_word': 0.993707, 'loss_grapheme': 0.144529, 'loss_vowel': 0.126438, 'loss_consonant': 0.086878, 'loss_word': 0.092528}\n",
      "   47 | 0.000213 | 160000/160635 | 2.8598 | 9.3368 |||\n",
      "val: {'recall': 0.995242, 'recall_grapheme': 0.99384, 'recall_vowel': 0.996306, 'recall_consonant': 0.996984, 'recall_word': 0.993863, 'acc_grapheme': 0.993384, 'acc_vowel': 0.996642, 'acc_consonant': 0.997811, 'acc_word': 0.993881, 'loss_grapheme': 0.152012, 'loss_vowel': 0.133012, 'loss_consonant': 0.09084, 'loss_word': 0.081554}\n",
      "   48 | 0.000206 | 160000/160635 | 7.2941 | 8.8293 |||\n",
      "val: {'recall': 0.995287, 'recall_grapheme': 0.993744, 'recall_vowel': 0.996094, 'recall_consonant': 0.997565, 'recall_word': 0.994095, 'acc_grapheme': 0.993309, 'acc_vowel': 0.996791, 'acc_consonant': 0.997861, 'acc_word': 0.994205, 'loss_grapheme': 0.139298, 'loss_vowel': 0.123197, 'loss_consonant': 0.087194, 'loss_word': 0.080699}\n",
      "   49 | 0.000200 | 160000/160635 | 9.5000 | 8.8624 ||\n",
      "val: {'recall': 0.996022, 'recall_grapheme': 0.994667, 'recall_vowel': 0.997079, 'recall_consonant': 0.997675, 'recall_word': 0.994353, 'acc_grapheme': 0.99418, 'acc_vowel': 0.997239, 'acc_consonant': 0.998234, 'acc_word': 0.994429, 'loss_grapheme': 0.107259, 'loss_vowel': 0.08852, 'loss_consonant': 0.063859, 'loss_word': 0.076456}\n",
      "###>>>>> saved\n",
      "   50 | 0.000194 | 160000/160635 | 9.2882 | 8.8030 |||\n",
      "val: {'recall': 0.995385, 'recall_grapheme': 0.993772, 'recall_vowel': 0.996522, 'recall_consonant': 0.997475, 'recall_word': 0.994188, 'acc_grapheme': 0.993508, 'acc_vowel': 0.996891, 'acc_consonant': 0.99806, 'acc_word': 0.994205, 'loss_grapheme': 0.139703, 'loss_vowel': 0.118957, 'loss_consonant': 0.082668, 'loss_word': 0.079501}\n",
      "   51 | 0.000187 | 160000/160635 | 8.9214 | 9.1165 |||\n",
      "val: {'recall': 0.996246, 'recall_grapheme': 0.994784, 'recall_vowel': 0.997224, 'recall_consonant': 0.998195, 'recall_word': 0.994835, 'acc_grapheme': 0.994031, 'acc_vowel': 0.997513, 'acc_consonant': 0.998184, 'acc_word': 0.994851, 'loss_grapheme': 0.12321, 'loss_vowel': 0.101329, 'loss_consonant': 0.071647, 'loss_word': 0.084629}\n",
      "###>>>>> saved\n",
      "   52 | 0.000181 | 160000/160635 | 9.4000 | 8.9213 |||\n",
      "val: {'recall': 0.995094, 'recall_grapheme': 0.993556, 'recall_vowel': 0.996255, 'recall_consonant': 0.997009, 'recall_word': 0.993759, 'acc_grapheme': 0.99321, 'acc_vowel': 0.996791, 'acc_consonant': 0.998035, 'acc_word': 0.993782, 'loss_grapheme': 0.161889, 'loss_vowel': 0.139665, 'loss_consonant': 0.101713, 'loss_word': 0.091753}\n",
      "   53 | 0.000175 | 160000/160635 | 4.2074 | 9.1735 ||\n",
      "val: {'recall': 0.99628, 'recall_grapheme': 0.995161, 'recall_vowel': 0.997274, 'recall_consonant': 0.997526, 'recall_word': 0.994221, 'acc_grapheme': 0.994379, 'acc_vowel': 0.997463, 'acc_consonant': 0.99811, 'acc_word': 0.994279, 'loss_grapheme': 0.126392, 'loss_vowel': 0.097937, 'loss_consonant': 0.067691, 'loss_word': 0.091791}\n",
      "###>>>>> saved\n",
      "   54 | 0.000169 | 160000/160635 | 3.9958 | 9.1147 ||\n",
      "val: {'recall': 0.995614, 'recall_grapheme': 0.994022, 'recall_vowel': 0.996784, 'recall_consonant': 0.997628, 'recall_word': 0.994136, 'acc_grapheme': 0.993856, 'acc_vowel': 0.997289, 'acc_consonant': 0.99806, 'acc_word': 0.99413, 'loss_grapheme': 0.14884, 'loss_vowel': 0.123076, 'loss_consonant': 0.083042, 'loss_word': 0.101306}\n",
      "   55 | 0.000163 | 160000/160635 | 8.7888 | 8.3632 ||\n",
      "val: {'recall': 0.995886, 'recall_grapheme': 0.994493, 'recall_vowel': 0.996678, 'recall_consonant': 0.99788, 'recall_word': 0.994152, 'acc_grapheme': 0.993583, 'acc_vowel': 0.99709, 'acc_consonant': 0.997911, 'acc_word': 0.99418, 'loss_grapheme': 0.125513, 'loss_vowel': 0.106796, 'loss_consonant': 0.076392, 'loss_word': 0.092143}\n",
      "   56 | 0.000156 | 160000/160635 | 9.0947 | 8.7281 |||\n",
      "val: {'recall': 0.995684, 'recall_grapheme': 0.994459, 'recall_vowel': 0.996326, 'recall_consonant': 0.997492, 'recall_word': 0.993972, 'acc_grapheme': 0.993508, 'acc_vowel': 0.996966, 'acc_consonant': 0.997911, 'acc_word': 0.994006, 'loss_grapheme': 0.142255, 'loss_vowel': 0.126443, 'loss_consonant': 0.084964, 'loss_word': 0.091756}\n",
      "   57 | 0.000150 | 160000/160635 | 4.7946 | 9.0020 ||\n",
      "val: {'recall': 0.995455, 'recall_grapheme': 0.994169, 'recall_vowel': 0.996638, 'recall_consonant': 0.996846, 'recall_word': 0.994047, 'acc_grapheme': 0.993508, 'acc_vowel': 0.99699, 'acc_consonant': 0.997886, 'acc_word': 0.994055, 'loss_grapheme': 0.14236, 'loss_vowel': 0.123223, 'loss_consonant': 0.08829, 'loss_word': 0.088155}\n",
      "   58 | 0.000144 | 160000/160635 | 12.6934 | 8.9820 ||\n",
      "val: {'recall': 0.995243, 'recall_grapheme': 0.993731, 'recall_vowel': 0.996143, 'recall_consonant': 0.997369, 'recall_word': 0.993583, 'acc_grapheme': 0.992886, 'acc_vowel': 0.996617, 'acc_consonant': 0.997737, 'acc_word': 0.993608, 'loss_grapheme': 0.157824, 'loss_vowel': 0.145487, 'loss_consonant': 0.108141, 'loss_word': 0.08348}\n",
      "   59 | 0.000138 | 160000/160635 | 8.8331 | 8.8124 |||\n",
      "val: {'recall': 0.995716, 'recall_grapheme': 0.99471, 'recall_vowel': 0.996529, 'recall_consonant': 0.996914, 'recall_word': 0.993873, 'acc_grapheme': 0.993757, 'acc_vowel': 0.997214, 'acc_consonant': 0.998035, 'acc_word': 0.993931, 'loss_grapheme': 0.114318, 'loss_vowel': 0.097669, 'loss_consonant': 0.069094, 'loss_word': 0.076633}\n",
      "   60 | 0.000132 | 160000/160635 | 2.5662 | 8.9128 ||\n",
      "val: {'recall': 0.995706, 'recall_grapheme': 0.994261, 'recall_vowel': 0.996819, 'recall_consonant': 0.997484, 'recall_word': 0.994055, 'acc_grapheme': 0.993931, 'acc_vowel': 0.997239, 'acc_consonant': 0.998085, 'acc_word': 0.994105, 'loss_grapheme': 0.116986, 'loss_vowel': 0.100878, 'loss_consonant': 0.071124, 'loss_word': 0.081886}\n",
      "   61 | 0.000126 | 160000/160635 | 11.7206 | 8.6265 ||\n",
      "val: {'recall': 0.995516, 'recall_grapheme': 0.993955, 'recall_vowel': 0.996963, 'recall_consonant': 0.99719, 'recall_word': 0.994037, 'acc_grapheme': 0.993782, 'acc_vowel': 0.997314, 'acc_consonant': 0.998234, 'acc_word': 0.994031, 'loss_grapheme': 0.133126, 'loss_vowel': 0.113551, 'loss_consonant': 0.078611, 'loss_word': 0.093023}\n",
      "   62 | 0.000121 | 160000/160635 | 8.8884 | 8.9407 |||\n",
      "val: {'recall': 0.995208, 'recall_grapheme': 0.993747, 'recall_vowel': 0.996415, 'recall_consonant': 0.996924, 'recall_word': 0.993994, 'acc_grapheme': 0.993459, 'acc_vowel': 0.996791, 'acc_consonant': 0.997737, 'acc_word': 0.993981, 'loss_grapheme': 0.142041, 'loss_vowel': 0.135522, 'loss_consonant': 0.095498, 'loss_word': 0.078205}\n",
      "   63 | 0.000115 | 160000/160635 | 8.8259 | 8.6291 ||\n",
      "val: {'recall': 0.995259, 'recall_grapheme': 0.993849, 'recall_vowel': 0.996296, 'recall_consonant': 0.997041, 'recall_word': 0.994072, 'acc_grapheme': 0.993633, 'acc_vowel': 0.996941, 'acc_consonant': 0.99806, 'acc_word': 0.994055, 'loss_grapheme': 0.143062, 'loss_vowel': 0.123, 'loss_consonant': 0.085221, 'loss_word': 0.088301}\n",
      "   64 | 0.000109 | 160000/160635 | 10.2885 | 9.0489 |\n",
      "val: {'recall': 0.995699, 'recall_grapheme': 0.994502, 'recall_vowel': 0.996408, 'recall_consonant': 0.997383, 'recall_word': 0.993863, 'acc_grapheme': 0.993782, 'acc_vowel': 0.99704, 'acc_consonant': 0.997886, 'acc_word': 0.993931, 'loss_grapheme': 0.14363, 'loss_vowel': 0.13347, 'loss_consonant': 0.092123, 'loss_word': 0.092166}\n",
      "   65 | 0.000104 | 160000/160635 | 9.6301 | 9.0198 |||\n",
      "val: {'recall': 0.99499, 'recall_grapheme': 0.992944, 'recall_vowel': 0.99664, 'recall_consonant': 0.997433, 'recall_word': 0.99399, 'acc_grapheme': 0.992911, 'acc_vowel': 0.99699, 'acc_consonant': 0.997712, 'acc_word': 0.993981, 'loss_grapheme': 0.153583, 'loss_vowel': 0.141532, 'loss_consonant': 0.097147, 'loss_word': 0.087961}\n",
      "   66 | 0.000098 | 160000/160635 | 9.1628 | 8.9532 ||\n",
      "val: {'recall': 0.995281, 'recall_grapheme': 0.993767, 'recall_vowel': 0.996392, 'recall_consonant': 0.997201, 'recall_word': 0.993771, 'acc_grapheme': 0.993434, 'acc_vowel': 0.996791, 'acc_consonant': 0.997761, 'acc_word': 0.993807, 'loss_grapheme': 0.139524, 'loss_vowel': 0.132136, 'loss_consonant': 0.09317, 'loss_word': 0.078758}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   67 | 0.000093 | 160000/160635 | 8.6760 | 9.0319 ||\n",
      "val: {'recall': 0.995371, 'recall_grapheme': 0.993666, 'recall_vowel': 0.996861, 'recall_consonant': 0.997291, 'recall_word': 0.99387, 'acc_grapheme': 0.993185, 'acc_vowel': 0.996816, 'acc_consonant': 0.997886, 'acc_word': 0.993956, 'loss_grapheme': 0.126874, 'loss_vowel': 0.121188, 'loss_consonant': 0.085572, 'loss_word': 0.071764}\n",
      "   68 | 0.000088 | 160000/160635 | 10.0958 | 8.5945 ||\n",
      "val: {'recall': 0.995642, 'recall_grapheme': 0.994465, 'recall_vowel': 0.996733, 'recall_consonant': 0.996903, 'recall_word': 0.993941, 'acc_grapheme': 0.993757, 'acc_vowel': 0.997065, 'acc_consonant': 0.99806, 'acc_word': 0.994006, 'loss_grapheme': 0.116781, 'loss_vowel': 0.102646, 'loss_consonant': 0.073468, 'loss_word': 0.071353}\n",
      "   69 | 0.000082 | 160000/160635 | 11.0556 | 8.8254 ||\n",
      "val: {'recall': 0.995677, 'recall_grapheme': 0.994449, 'recall_vowel': 0.996937, 'recall_consonant': 0.996873, 'recall_word': 0.994316, 'acc_grapheme': 0.993981, 'acc_vowel': 0.99714, 'acc_consonant': 0.99796, 'acc_word': 0.994329, 'loss_grapheme': 0.130958, 'loss_vowel': 0.119693, 'loss_consonant': 0.084655, 'loss_word': 0.080069}\n",
      "   70 | 0.000077 | 160000/160635 | 1.3144 | 8.9432 |||\n",
      "val: {'recall': 0.996215, 'recall_grapheme': 0.995057, 'recall_vowel': 0.997175, 'recall_consonant': 0.99757, 'recall_word': 0.994707, 'acc_grapheme': 0.994677, 'acc_vowel': 0.997388, 'acc_consonant': 0.998259, 'acc_word': 0.994727, 'loss_grapheme': 0.096363, 'loss_vowel': 0.084218, 'loss_consonant': 0.057732, 'loss_word': 0.067871}\n",
      "   71 | 0.000073 | 160000/160635 | 11.5451 | 8.8459 |\n",
      "val: {'recall': 0.995823, 'recall_grapheme': 0.994394, 'recall_vowel': 0.997026, 'recall_consonant': 0.997476, 'recall_word': 0.99452, 'acc_grapheme': 0.994055, 'acc_vowel': 0.997314, 'acc_consonant': 0.99811, 'acc_word': 0.994553, 'loss_grapheme': 0.127976, 'loss_vowel': 0.113727, 'loss_consonant': 0.080134, 'loss_word': 0.083501}\n",
      "   72 | 0.000068 | 160000/160635 | 9.4468 | 8.8518 |||\n",
      "val: {'recall': 0.995365, 'recall_grapheme': 0.993718, 'recall_vowel': 0.996501, 'recall_consonant': 0.997521, 'recall_word': 0.994049, 'acc_grapheme': 0.99321, 'acc_vowel': 0.996891, 'acc_consonant': 0.998035, 'acc_word': 0.994055, 'loss_grapheme': 0.151386, 'loss_vowel': 0.144255, 'loss_consonant': 0.098867, 'loss_word': 0.086907}\n",
      "   73 | 0.000063 | 160000/160635 | 12.1113 | 8.7746 ||\n",
      "val: {'recall': 0.995719, 'recall_grapheme': 0.994524, 'recall_vowel': 0.996982, 'recall_consonant': 0.996845, 'recall_word': 0.9942, 'acc_grapheme': 0.993707, 'acc_vowel': 0.997115, 'acc_consonant': 0.99796, 'acc_word': 0.994205, 'loss_grapheme': 0.167169, 'loss_vowel': 0.149454, 'loss_consonant': 0.104664, 'loss_word': 0.099458}\n",
      "   74 | 0.000059 | 160000/160635 | 11.7687 | 8.8027 ||\n",
      "val: {'recall': 0.995609, 'recall_grapheme': 0.993986, 'recall_vowel': 0.996885, 'recall_consonant': 0.99758, 'recall_word': 0.993841, 'acc_grapheme': 0.993633, 'acc_vowel': 0.997239, 'acc_consonant': 0.99806, 'acc_word': 0.993856, 'loss_grapheme': 0.131503, 'loss_vowel': 0.115188, 'loss_consonant': 0.08035, 'loss_word': 0.087955}\n",
      "   75 | 0.000054 | 160000/160635 | 10.5506 | 8.9371 ||\n",
      "val: {'recall': 0.995086, 'recall_grapheme': 0.993302, 'recall_vowel': 0.99645, 'recall_consonant': 0.997289, 'recall_word': 0.993717, 'acc_grapheme': 0.993185, 'acc_vowel': 0.996816, 'acc_consonant': 0.997587, 'acc_word': 0.993707, 'loss_grapheme': 0.150758, 'loss_vowel': 0.138641, 'loss_consonant': 0.100767, 'loss_word': 0.084611}\n",
      "   76 | 0.000050 | 160000/160635 | 10.0196 | 8.5150 |\n",
      "val: {'recall': 0.995116, 'recall_grapheme': 0.993478, 'recall_vowel': 0.996276, 'recall_consonant': 0.997233, 'recall_word': 0.993367, 'acc_grapheme': 0.993235, 'acc_vowel': 0.996866, 'acc_consonant': 0.997811, 'acc_word': 0.993409, 'loss_grapheme': 0.143217, 'loss_vowel': 0.131639, 'loss_consonant': 0.094312, 'loss_word': 0.085514}\n",
      "   77 | 0.000046 | 160000/160635 | 9.2807 | 8.8305 |||\n",
      "val: {'recall': 0.995197, 'recall_grapheme': 0.993794, 'recall_vowel': 0.996356, 'recall_consonant': 0.996843, 'recall_word': 0.993848, 'acc_grapheme': 0.993235, 'acc_vowel': 0.996767, 'acc_consonant': 0.997861, 'acc_word': 0.993881, 'loss_grapheme': 0.138773, 'loss_vowel': 0.132944, 'loss_consonant': 0.095816, 'loss_word': 0.07765}\n",
      "   78 | 0.000042 | 160000/160635 | 6.9109 | 8.8698 |||\n",
      "val: {'recall': 0.99606, 'recall_grapheme': 0.995008, 'recall_vowel': 0.996766, 'recall_consonant': 0.997456, 'recall_word': 0.994339, 'acc_grapheme': 0.994205, 'acc_vowel': 0.997289, 'acc_consonant': 0.998085, 'acc_word': 0.994379, 'loss_grapheme': 0.124736, 'loss_vowel': 0.108581, 'loss_consonant': 0.07885, 'loss_word': 0.085773}\n",
      "   79 | 0.000038 | 160000/160635 | 10.4571 | 8.6434 |\n",
      "val: {'recall': 0.995947, 'recall_grapheme': 0.994761, 'recall_vowel': 0.996744, 'recall_consonant': 0.997522, 'recall_word': 0.994136, 'acc_grapheme': 0.993981, 'acc_vowel': 0.997189, 'acc_consonant': 0.99811, 'acc_word': 0.99418, 'loss_grapheme': 0.115953, 'loss_vowel': 0.105364, 'loss_consonant': 0.078267, 'loss_word': 0.074285}\n",
      "   80 | 0.000035 | 160000/160635 | 9.1681 | 8.6297 |||\n",
      "val: {'recall': 0.996066, 'recall_grapheme': 0.994859, 'recall_vowel': 0.997063, 'recall_consonant': 0.997484, 'recall_word': 0.994397, 'acc_grapheme': 0.993956, 'acc_vowel': 0.997438, 'acc_consonant': 0.998035, 'acc_word': 0.994453, 'loss_grapheme': 0.121036, 'loss_vowel': 0.107918, 'loss_consonant': 0.078422, 'loss_word': 0.080264}\n",
      "   81 | 0.000031 | 160000/160635 | 10.0229 | 8.7929 ||\n",
      "val: {'recall': 0.995784, 'recall_grapheme': 0.994452, 'recall_vowel': 0.996715, 'recall_consonant': 0.997518, 'recall_word': 0.994274, 'acc_grapheme': 0.994205, 'acc_vowel': 0.997388, 'acc_consonant': 0.99811, 'acc_word': 0.994354, 'loss_grapheme': 0.113523, 'loss_vowel': 0.10077, 'loss_consonant': 0.073755, 'loss_word': 0.076619}\n",
      "   82 | 0.000028 | 160000/160635 | 9.0315 | 8.7890 |||\n",
      "val: {'recall': 0.995946, 'recall_grapheme': 0.994589, 'recall_vowel': 0.997034, 'recall_consonant': 0.997573, 'recall_word': 0.994428, 'acc_grapheme': 0.994379, 'acc_vowel': 0.997413, 'acc_consonant': 0.998209, 'acc_word': 0.994453, 'loss_grapheme': 0.11191, 'loss_vowel': 0.093878, 'loss_consonant': 0.067602, 'loss_word': 0.078775}\n",
      "   83 | 0.000025 | 160000/160635 | 6.5045 | 8.7278 ||\n",
      "val: {'recall': 0.995657, 'recall_grapheme': 0.994086, 'recall_vowel': 0.996988, 'recall_consonant': 0.99747, 'recall_word': 0.994071, 'acc_grapheme': 0.993906, 'acc_vowel': 0.997289, 'acc_consonant': 0.99811, 'acc_word': 0.994105, 'loss_grapheme': 0.121464, 'loss_vowel': 0.102152, 'loss_consonant': 0.071375, 'loss_word': 0.082476}\n",
      "   84 | 0.000022 | 160000/160635 | 9.5628 | 8.9383 |||\n",
      "val: {'recall': 0.996014, 'recall_grapheme': 0.994748, 'recall_vowel': 0.997117, 'recall_consonant': 0.997441, 'recall_word': 0.994192, 'acc_grapheme': 0.993856, 'acc_vowel': 0.997264, 'acc_consonant': 0.997985, 'acc_word': 0.994254, 'loss_grapheme': 0.134717, 'loss_vowel': 0.120744, 'loss_consonant': 0.086422, 'loss_word': 0.081255}\n",
      "   85 | 0.000019 | 160000/160635 | 7.7256 | 8.4874 |||\n",
      "val: {'recall': 0.9958, 'recall_grapheme': 0.994392, 'recall_vowel': 0.997348, 'recall_consonant': 0.99707, 'recall_word': 0.994095, 'acc_grapheme': 0.993956, 'acc_vowel': 0.997513, 'acc_consonant': 0.998184, 'acc_word': 0.994105, 'loss_grapheme': 0.125981, 'loss_vowel': 0.11243, 'loss_consonant': 0.082825, 'loss_word': 0.085448}\n",
      "   86 | 0.000016 | 160000/160635 | 10.8353 | 9.1103 ||\n",
      "val: {'recall': 0.995688, 'recall_grapheme': 0.994446, 'recall_vowel': 0.996932, 'recall_consonant': 0.996931, 'recall_word': 0.993978, 'acc_grapheme': 0.993906, 'acc_vowel': 0.997189, 'acc_consonant': 0.997985, 'acc_word': 0.994006, 'loss_grapheme': 0.126752, 'loss_vowel': 0.115786, 'loss_consonant': 0.082489, 'loss_word': 0.075208}\n",
      "   87 | 0.000014 | 160000/160635 | 0.7957 | 8.7117 |||\n",
      "val: {'recall': 0.995804, 'recall_grapheme': 0.994457, 'recall_vowel': 0.997127, 'recall_consonant': 0.997175, 'recall_word': 0.994519, 'acc_grapheme': 0.994155, 'acc_vowel': 0.997463, 'acc_consonant': 0.998309, 'acc_word': 0.994578, 'loss_grapheme': 0.123013, 'loss_vowel': 0.108502, 'loss_consonant': 0.074252, 'loss_word': 0.079651}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   88 | 0.000012 | 160000/160635 | 12.0667 | 9.0004 |\n",
      "val: {'recall': 0.995459, 'recall_grapheme': 0.994115, 'recall_vowel': 0.996638, 'recall_consonant': 0.99697, 'recall_word': 0.994097, 'acc_grapheme': 0.993583, 'acc_vowel': 0.997015, 'acc_consonant': 0.998035, 'acc_word': 0.994105, 'loss_grapheme': 0.130576, 'loss_vowel': 0.123989, 'loss_consonant': 0.088514, 'loss_word': 0.075225}\n",
      "   89 | 0.000010 | 160000/160635 | 10.1072 | 8.5567 |\n",
      "val: {'recall': 0.995416, 'recall_grapheme': 0.993983, 'recall_vowel': 0.996734, 'recall_consonant': 0.996963, 'recall_word': 0.993856, 'acc_grapheme': 0.993483, 'acc_vowel': 0.99704, 'acc_consonant': 0.998035, 'acc_word': 0.993906, 'loss_grapheme': 0.123718, 'loss_vowel': 0.118753, 'loss_consonant': 0.087396, 'loss_word': 0.074629}\n",
      "   90 | 0.000008 | 160000/160635 | 10.3343 | 8.5914 ||\n",
      "val: {'recall': 0.995254, 'recall_grapheme': 0.993801, 'recall_vowel': 0.996558, 'recall_consonant': 0.996858, 'recall_word': 0.993775, 'acc_grapheme': 0.993459, 'acc_vowel': 0.99699, 'acc_consonant': 0.997911, 'acc_word': 0.993832, 'loss_grapheme': 0.123426, 'loss_vowel': 0.11615, 'loss_consonant': 0.086557, 'loss_word': 0.074119}\n",
      "   91 | 0.000006 | 160000/160635 | 9.9938 | 9.0324 |||\n",
      "val: {'recall': 0.995658, 'recall_grapheme': 0.994352, 'recall_vowel': 0.997051, 'recall_consonant': 0.996878, 'recall_word': 0.993839, 'acc_grapheme': 0.993807, 'acc_vowel': 0.997289, 'acc_consonant': 0.99796, 'acc_word': 0.993856, 'loss_grapheme': 0.115276, 'loss_vowel': 0.106904, 'loss_consonant': 0.081866, 'loss_word': 0.071252}\n",
      "   92 | 0.000005 | 160000/160635 | 9.1867 | 8.6751 |||\n",
      "val: {'recall': 0.996075, 'recall_grapheme': 0.994886, 'recall_vowel': 0.997411, 'recall_consonant': 0.997116, 'recall_word': 0.9944, 'acc_grapheme': 0.994055, 'acc_vowel': 0.997612, 'acc_consonant': 0.998209, 'acc_word': 0.994429, 'loss_grapheme': 0.119661, 'loss_vowel': 0.108588, 'loss_consonant': 0.078427, 'loss_word': 0.079783}\n",
      "   93 | 0.000004 | 160000/160635 | 12.0193 | 8.9568 ||\n",
      "val: {'recall': 0.994982, 'recall_grapheme': 0.99339, 'recall_vowel': 0.996259, 'recall_consonant': 0.996891, 'recall_word': 0.993768, 'acc_grapheme': 0.993135, 'acc_vowel': 0.996642, 'acc_consonant': 0.997712, 'acc_word': 0.993807, 'loss_grapheme': 0.137682, 'loss_vowel': 0.13077, 'loss_consonant': 0.095233, 'loss_word': 0.073924}\n",
      "   94 | 0.000002 | 160000/160635 | 5.9207 | 8.6724 ||\n",
      "val: {'recall': 0.99592, 'recall_grapheme': 0.994843, 'recall_vowel': 0.996986, 'recall_consonant': 0.997009, 'recall_word': 0.994427, 'acc_grapheme': 0.994155, 'acc_vowel': 0.997339, 'acc_consonant': 0.99806, 'acc_word': 0.994429, 'loss_grapheme': 0.118219, 'loss_vowel': 0.101043, 'loss_consonant': 0.074122, 'loss_word': 0.076749}\n",
      "   95 | 0.000002 | 160000/160635 | 7.7182 | 8.6044 |||\n",
      "val: {'recall': 0.99571, 'recall_grapheme': 0.994418, 'recall_vowel': 0.996987, 'recall_consonant': 0.997017, 'recall_word': 0.993968, 'acc_grapheme': 0.993881, 'acc_vowel': 0.997339, 'acc_consonant': 0.99811, 'acc_word': 0.994006, 'loss_grapheme': 0.114325, 'loss_vowel': 0.102532, 'loss_consonant': 0.077244, 'loss_word': 0.076703}\n",
      "   96 | 0.000001 | 160000/160635 | 10.7481 | 8.9379 ||\n",
      "val: {'recall': 0.995306, 'recall_grapheme': 0.993715, 'recall_vowel': 0.99677, 'recall_consonant': 0.997025, 'recall_word': 0.99372, 'acc_grapheme': 0.99326, 'acc_vowel': 0.997015, 'acc_consonant': 0.997861, 'acc_word': 0.993757, 'loss_grapheme': 0.150241, 'loss_vowel': 0.136066, 'loss_consonant': 0.097864, 'loss_word': 0.087447}\n",
      "   97 | 0.000000 | 160000/160635 | 6.5225 | 8.5620 |||\n",
      "val: {'recall': 0.996049, 'recall_grapheme': 0.995092, 'recall_vowel': 0.996909, 'recall_consonant': 0.997103, 'recall_word': 0.994373, 'acc_grapheme': 0.99413, 'acc_vowel': 0.997413, 'acc_consonant': 0.998135, 'acc_word': 0.994429, 'loss_grapheme': 0.109956, 'loss_vowel': 0.097497, 'loss_consonant': 0.071465, 'loss_word': 0.078135}\n",
      "   98 | 0.000000 | 160000/160635 | 4.1974 | 8.7423 |||\n",
      "val: {'recall': 0.996217, 'recall_grapheme': 0.995056, 'recall_vowel': 0.997569, 'recall_consonant': 0.99719, 'recall_word': 0.994427, 'acc_grapheme': 0.994354, 'acc_vowel': 0.997587, 'acc_consonant': 0.998284, 'acc_word': 0.994478, 'loss_grapheme': 0.114405, 'loss_vowel': 0.097504, 'loss_consonant': 0.070116, 'loss_word': 0.083099}\n",
      "   99 | 0.000000 | 160000/160635 | 4.9675 | 8.7077 ||\n",
      "val: {'recall': 0.996043, 'recall_grapheme': 0.994946, 'recall_vowel': 0.997294, 'recall_consonant': 0.996984, 'recall_word': 0.994449, 'acc_grapheme': 0.994304, 'acc_vowel': 0.997513, 'acc_consonant': 0.998135, 'acc_word': 0.994478, 'loss_grapheme': 0.107846, 'loss_vowel': 0.091977, 'loss_consonant': 0.065798, 'loss_word': 0.078134}\n",
      "CYCLE: 3\n",
      "{'recall': 0.996043, 'recall_grapheme': 0.994946, 'recall_vowel': 0.997294, 'recall_consonant': 0.996984, 'recall_word': 0.994449, 'acc_grapheme': 0.994304, 'acc_vowel': 0.997513, 'acc_consonant': 0.998135, 'acc_word': 0.994478, 'loss_grapheme': 0.107846, 'loss_vowel': 0.091977, 'loss_consonant': 0.065798, 'loss_word': 0.078134}\n",
      "    0 | 0.000040 | 160000/160635 | 5.2537 | 8.8088 ||\n",
      "val: {'recall': 0.995816, 'recall_grapheme': 0.994399, 'recall_vowel': 0.996942, 'recall_consonant': 0.997525, 'recall_word': 0.994093, 'acc_grapheme': 0.993707, 'acc_vowel': 0.997015, 'acc_consonant': 0.998035, 'acc_word': 0.994155, 'loss_grapheme': 0.117571, 'loss_vowel': 0.108525, 'loss_consonant': 0.081299, 'loss_word': 0.06939}\n",
      "    1 | 0.000080 | 160000/160635 | 4.1384 | 8.7574 |||\n",
      "val: {'recall': 0.995582, 'recall_grapheme': 0.994151, 'recall_vowel': 0.996641, 'recall_consonant': 0.997385, 'recall_word': 0.994164, 'acc_grapheme': 0.993906, 'acc_vowel': 0.997115, 'acc_consonant': 0.997985, 'acc_word': 0.99418, 'loss_grapheme': 0.132802, 'loss_vowel': 0.11662, 'loss_consonant': 0.084001, 'loss_word': 0.081808}\n",
      "    2 | 0.000120 | 160000/160635 | 10.1394 | 8.9039 |\n",
      "val: {'recall': 0.994644, 'recall_grapheme': 0.992795, 'recall_vowel': 0.99612, 'recall_consonant': 0.996866, 'recall_word': 0.993965, 'acc_grapheme': 0.992986, 'acc_vowel': 0.996692, 'acc_consonant': 0.997836, 'acc_word': 0.993981, 'loss_grapheme': 0.144045, 'loss_vowel': 0.131934, 'loss_consonant': 0.094651, 'loss_word': 0.076262}\n",
      "    3 | 0.000159 | 160000/160635 | 12.0611 | 8.7509 |\n",
      "val: {'recall': 0.995427, 'recall_grapheme': 0.994062, 'recall_vowel': 0.996688, 'recall_consonant': 0.996894, 'recall_word': 0.994232, 'acc_grapheme': 0.993732, 'acc_vowel': 0.996916, 'acc_consonant': 0.99796, 'acc_word': 0.994279, 'loss_grapheme': 0.122811, 'loss_vowel': 0.109959, 'loss_consonant': 0.079518, 'loss_word': 0.077668}\n",
      "    4 | 0.000199 | 160000/160635 | 8.7217 | 8.4826 |||\n",
      "val: {'recall': 0.995297, 'recall_grapheme': 0.993672, 'recall_vowel': 0.997045, 'recall_consonant': 0.996798, 'recall_word': 0.993828, 'acc_grapheme': 0.993856, 'acc_vowel': 0.997339, 'acc_consonant': 0.99796, 'acc_word': 0.993906, 'loss_grapheme': 0.102769, 'loss_vowel': 0.092245, 'loss_consonant': 0.069838, 'loss_word': 0.069409}\n",
      "    5 | 0.000238 | 160000/160635 | 11.6680 | 8.9083 |\n",
      "val: {'recall': 0.994804, 'recall_grapheme': 0.9931, 'recall_vowel': 0.995816, 'recall_consonant': 0.997198, 'recall_word': 0.993596, 'acc_grapheme': 0.993036, 'acc_vowel': 0.996294, 'acc_consonant': 0.997662, 'acc_word': 0.993633, 'loss_grapheme': 0.15406, 'loss_vowel': 0.149318, 'loss_consonant': 0.103066, 'loss_word': 0.081766}\n",
      "    6 | 0.000276 | 160000/160635 | 10.1522 | 8.7121 ||\n",
      "val: {'recall': 0.995147, 'recall_grapheme': 0.993551, 'recall_vowel': 0.996627, 'recall_consonant': 0.99686, 'recall_word': 0.993896, 'acc_grapheme': 0.993135, 'acc_vowel': 0.997065, 'acc_consonant': 0.99801, 'acc_word': 0.993956, 'loss_grapheme': 0.123541, 'loss_vowel': 0.107119, 'loss_consonant': 0.077773, 'loss_word': 0.076982}\n",
      "    7 | 0.000315 | 160000/160635 | 11.7380 | 8.7042 |\n",
      "val: {'recall': 0.995098, 'recall_grapheme': 0.993338, 'recall_vowel': 0.996321, 'recall_consonant': 0.997394, 'recall_word': 0.993269, 'acc_grapheme': 0.992936, 'acc_vowel': 0.996692, 'acc_consonant': 0.997936, 'acc_word': 0.993309, 'loss_grapheme': 0.116095, 'loss_vowel': 0.106845, 'loss_consonant': 0.076347, 'loss_word': 0.0715}\n",
      "    8 | 0.000353 | 160000/160635 | 9.5329 | 8.9896 |||\n",
      "val: {'recall': 0.995318, 'recall_grapheme': 0.994052, 'recall_vowel': 0.996732, 'recall_consonant': 0.996437, 'recall_word': 0.993685, 'acc_grapheme': 0.99321, 'acc_vowel': 0.996866, 'acc_consonant': 0.997562, 'acc_word': 0.993658, 'loss_grapheme': 0.122079, 'loss_vowel': 0.106749, 'loss_consonant': 0.077299, 'loss_word': 0.07617}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9 | 0.000390 | 160000/160635 | 9.8810 | 8.7716 |||\n",
      "val: {'recall': 0.995799, 'recall_grapheme': 0.994675, 'recall_vowel': 0.996689, 'recall_consonant': 0.997156, 'recall_word': 0.993871, 'acc_grapheme': 0.993931, 'acc_vowel': 0.996916, 'acc_consonant': 0.997911, 'acc_word': 0.993931, 'loss_grapheme': 0.101348, 'loss_vowel': 0.082072, 'loss_consonant': 0.058538, 'loss_word': 0.068742}\n",
      "   10 | 0.000388 | 160000/160635 | 10.3682 | 8.8818 ||\n",
      "val: {'recall': 0.995302, 'recall_grapheme': 0.993943, 'recall_vowel': 0.99607, 'recall_consonant': 0.997253, 'recall_word': 0.993646, 'acc_grapheme': 0.993483, 'acc_vowel': 0.996642, 'acc_consonant': 0.997612, 'acc_word': 0.993633, 'loss_grapheme': 0.152051, 'loss_vowel': 0.122484, 'loss_consonant': 0.077517, 'loss_word': 0.09114}\n",
      "   11 | 0.000386 | 160000/160635 | 8.8178 | 9.1847 |||\n",
      "val: {'recall': 0.995097, 'recall_grapheme': 0.993417, 'recall_vowel': 0.996199, 'recall_consonant': 0.997357, 'recall_word': 0.993878, 'acc_grapheme': 0.993011, 'acc_vowel': 0.996642, 'acc_consonant': 0.997662, 'acc_word': 0.993931, 'loss_grapheme': 0.153229, 'loss_vowel': 0.133636, 'loss_consonant': 0.094743, 'loss_word': 0.083026}\n",
      "   12 | 0.000384 | 110720/160635 | 10.1711 | 9.0569 ||"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-6df1386d1bbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-c4c11a7e93f6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcycle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_cycles\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CYCLE:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtrain_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-c4c11a7e93f6>\u001b[0m in \u001b[0;36mtrain_cycle\u001b[0;34m(args, model, optimizer, lr_scheduler)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mlam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbx2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbbx1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbby2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbby1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;31m# compute output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_aux1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_aux2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0mloss_primary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_a\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_b\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mloss_aux1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_aux1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_a\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_aux1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_b\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0m_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_tup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(args, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CYCLE: 1\n",
      "{'recall': 0.984603, 'recall_grapheme': 0.97713, 'recall_vowel': 0.991312, 'recall_consonant': 0.992841, 'recall_word': 0.97772, 'acc_grapheme': 0.975376, 'acc_vowel': 0.990822, 'acc_consonant': 0.990349, 'acc_word': 0.977764, 'loss_grapheme': 0.377051, 'loss_vowel': 0.251561, 'loss_consonant': 0.166439, 'loss_word': 0.327534}\n",
      "    0 | 0.000080 | 160000/160635 | 25.9052 | 18.1899 |\n",
      "val: {'recall': 0.984696, 'recall_grapheme': 0.977376, 'recall_vowel': 0.991461, 'recall_consonant': 0.992571, 'recall_word': 0.978015, 'acc_grapheme': 0.9756, 'acc_vowel': 0.991195, 'acc_consonant': 0.991245, 'acc_word': 0.978038, 'loss_grapheme': 0.340236, 'loss_vowel': 0.240668, 'loss_consonant': 0.161181, 'loss_word': 0.29083}\n",
      "###>>>>> saved\n",
      "    1 | 0.000160 | 160000/160635 | 2.3221 | 18.9110 ||\n",
      "val: {'recall': 0.984781, 'recall_grapheme': 0.978174, 'recall_vowel': 0.991073, 'recall_consonant': 0.991703, 'recall_word': 0.97805, 'acc_grapheme': 0.976694, 'acc_vowel': 0.991319, 'acc_consonant': 0.99117, 'acc_word': 0.978162, 'loss_grapheme': 0.293794, 'loss_vowel': 0.198103, 'loss_consonant': 0.139734, 'loss_word': 0.244299}\n",
      "###>>>>> saved\n",
      "    2 | 0.000239 | 160000/160635 | 14.3098 | 18.1875 |\n",
      "val: {'recall': 0.984629, 'recall_grapheme': 0.977931, 'recall_vowel': 0.99116, 'recall_consonant': 0.991494, 'recall_word': 0.977285, 'acc_grapheme': 0.976023, 'acc_vowel': 0.991195, 'acc_consonant': 0.991145, 'acc_word': 0.977316, 'loss_grapheme': 0.33478, 'loss_vowel': 0.241768, 'loss_consonant': 0.16355, 'loss_word': 0.277588}\n",
      "    3 | 0.000318 | 160000/160635 | 19.2009 | 18.2638 |\n",
      "val: {'recall': 0.984571, 'recall_grapheme': 0.977557, 'recall_vowel': 0.990713, 'recall_consonant': 0.992457, 'recall_word': 0.977443, 'acc_grapheme': 0.976048, 'acc_vowel': 0.990673, 'acc_consonant': 0.991096, 'acc_word': 0.97749, 'loss_grapheme': 0.333418, 'loss_vowel': 0.23937, 'loss_consonant': 0.163375, 'loss_word': 0.279844}\n",
      "    4 | 0.000397 | 160000/160635 | 19.7396 | 17.9612 |\n",
      "val: {'recall': 0.983972, 'recall_grapheme': 0.976882, 'recall_vowel': 0.99079, 'recall_consonant': 0.991335, 'recall_word': 0.977321, 'acc_grapheme': 0.974456, 'acc_vowel': 0.990847, 'acc_consonant': 0.99122, 'acc_word': 0.977416, 'loss_grapheme': 0.376483, 'loss_vowel': 0.277006, 'loss_consonant': 0.177096, 'loss_word': 0.319178}\n",
      "    5 | 0.000396 | 160000/160635 | 25.1112 | 18.2098 |\n",
      "val: {'recall': 0.984144, 'recall_grapheme': 0.97701, 'recall_vowel': 0.991161, 'recall_consonant': 0.991393, 'recall_word': 0.977308, 'acc_grapheme': 0.975177, 'acc_vowel': 0.990971, 'acc_consonant': 0.991245, 'acc_word': 0.977416, 'loss_grapheme': 0.360193, 'loss_vowel': 0.283771, 'loss_consonant': 0.177305, 'loss_word': 0.326814}\n",
      "    6 | 0.000395 | 160000/160635 | 22.4892 | 18.4964 |\n",
      "val: {'recall': 0.985197, 'recall_grapheme': 0.978447, 'recall_vowel': 0.991417, 'recall_consonant': 0.992477, 'recall_word': 0.978321, 'acc_grapheme': 0.977416, 'acc_vowel': 0.991469, 'acc_consonant': 0.991991, 'acc_word': 0.97851, 'loss_grapheme': 0.298504, 'loss_vowel': 0.201846, 'loss_consonant': 0.141152, 'loss_word': 0.243685}\n",
      "###>>>>> saved\n",
      "    7 | 0.000394 | 160000/160635 | 22.6900 | 18.1613 |\n",
      "val: {'recall': 0.984958, 'recall_grapheme': 0.978691, 'recall_vowel': 0.991695, 'recall_consonant': 0.990753, 'recall_word': 0.978879, 'acc_grapheme': 0.977167, 'acc_vowel': 0.991394, 'acc_consonant': 0.991444, 'acc_word': 0.978958, 'loss_grapheme': 0.315668, 'loss_vowel': 0.217791, 'loss_consonant': 0.149168, 'loss_word': 0.268986}\n",
      "    8 | 0.000392 | 160000/160635 | 18.0319 | 17.6471 |\n",
      "val: {'recall': 0.985911, 'recall_grapheme': 0.980308, 'recall_vowel': 0.991334, 'recall_consonant': 0.991695, 'recall_word': 0.979779, 'acc_grapheme': 0.977938, 'acc_vowel': 0.991693, 'acc_consonant': 0.991916, 'acc_word': 0.979804, 'loss_grapheme': 0.332507, 'loss_vowel': 0.224873, 'loss_consonant': 0.154374, 'loss_word': 0.277954}\n",
      "###>>>>> saved\n",
      "    9 | 0.000390 | 160000/160635 | 7.3551 | 17.6395 ||\n",
      "val: {'recall': 0.98574, 'recall_grapheme': 0.980065, 'recall_vowel': 0.991982, 'recall_consonant': 0.990848, 'recall_word': 0.979325, 'acc_grapheme': 0.978684, 'acc_vowel': 0.991892, 'acc_consonant': 0.992016, 'acc_word': 0.979306, 'loss_grapheme': 0.251469, 'loss_vowel': 0.158123, 'loss_consonant': 0.112822, 'loss_word': 0.201565}\n",
      "   11 | 0.000386 | 160000/160635 | 22.4904 | 17.7227 |\n",
      "val: {'recall': 0.98606, 'recall_grapheme': 0.980194, 'recall_vowel': 0.991895, 'recall_consonant': 0.991959, 'recall_word': 0.980569, 'acc_grapheme': 0.978261, 'acc_vowel': 0.991344, 'acc_consonant': 0.991916, 'acc_word': 0.980351, 'loss_grapheme': 0.363433, 'loss_vowel': 0.268398, 'loss_consonant': 0.169744, 'loss_word': 0.3045}\n",
      "###>>>>> saved\n",
      "   12 | 0.000384 | 160000/160635 | 17.1256 | 17.5252 |\n",
      "val: {'recall': 0.986659, 'recall_grapheme': 0.980851, 'recall_vowel': 0.992543, 'recall_consonant': 0.992391, 'recall_word': 0.981032, 'acc_grapheme': 0.979629, 'acc_vowel': 0.99219, 'acc_consonant': 0.992513, 'acc_word': 0.980948, 'loss_grapheme': 0.323615, 'loss_vowel': 0.230281, 'loss_consonant': 0.14762, 'loss_word': 0.250576}\n",
      "###>>>>> saved\n",
      "   13 | 0.000381 | 160000/160635 | 12.9224 | 17.2048 |\n",
      "val: {'recall': 0.985883, 'recall_grapheme': 0.97967, 'recall_vowel': 0.991736, 'recall_consonant': 0.992456, 'recall_word': 0.980127, 'acc_grapheme': 0.97759, 'acc_vowel': 0.990996, 'acc_consonant': 0.991419, 'acc_word': 0.980077, 'loss_grapheme': 0.384961, 'loss_vowel': 0.283676, 'loss_consonant': 0.183022, 'loss_word': 0.306478}\n",
      "   14 | 0.000378 | 160000/160635 | 12.6175 | 17.8544 |\n",
      "val: {'recall': 0.986564, 'recall_grapheme': 0.9807, 'recall_vowel': 0.992481, 'recall_consonant': 0.992375, 'recall_word': 0.980794, 'acc_grapheme': 0.979157, 'acc_vowel': 0.99224, 'acc_consonant': 0.99224, 'acc_word': 0.980898, 'loss_grapheme': 0.315242, 'loss_vowel': 0.210297, 'loss_consonant': 0.144708, 'loss_word': 0.240964}\n",
      "   15 | 0.000375 | 160000/160635 | 22.7105 | 17.4718 |\n",
      "val: {'recall': 0.987583, 'recall_grapheme': 0.981534, 'recall_vowel': 0.992481, 'recall_consonant': 0.994785, 'recall_word': 0.981849, 'acc_grapheme': 0.979406, 'acc_vowel': 0.991991, 'acc_consonant': 0.992041, 'acc_word': 0.981943, 'loss_grapheme': 0.373212, 'loss_vowel': 0.276412, 'loss_consonant': 0.17435, 'loss_word': 0.29317}\n",
      "###>>>>> saved\n",
      "   16 | 0.000372 | 160000/160635 | 10.8118 | 17.3184 |\n",
      "val: {'recall': 0.987173, 'recall_grapheme': 0.982443, 'recall_vowel': 0.993072, 'recall_consonant': 0.990735, 'recall_word': 0.981047, 'acc_grapheme': 0.980798, 'acc_vowel': 0.993011, 'acc_consonant': 0.992787, 'acc_word': 0.981147, 'loss_grapheme': 0.199111, 'loss_vowel': 0.125772, 'loss_consonant': 0.093307, 'loss_word': 0.162389}\n",
      "   17 | 0.000369 | 160000/160635 | 23.9006 | 16.8475 |\n",
      "val: {'recall': 0.986276, 'recall_grapheme': 0.980408, 'recall_vowel': 0.991932, 'recall_consonant': 0.992358, 'recall_word': 0.979612, 'acc_grapheme': 0.978261, 'acc_vowel': 0.991916, 'acc_consonant': 0.992687, 'acc_word': 0.979828, 'loss_grapheme': 0.3389, 'loss_vowel': 0.244647, 'loss_consonant': 0.159519, 'loss_word': 0.266583}\n",
      "   18 | 0.000365 | 160000/160635 | 20.5498 | 17.3063 |\n",
      "val: {'recall': 0.987655, 'recall_grapheme': 0.9823, 'recall_vowel': 0.992976, 'recall_consonant': 0.993044, 'recall_word': 0.981673, 'acc_grapheme': 0.980276, 'acc_vowel': 0.992812, 'acc_consonant': 0.992862, 'acc_word': 0.981694, 'loss_grapheme': 0.337896, 'loss_vowel': 0.24476, 'loss_consonant': 0.169639, 'loss_word': 0.257741}\n",
      "###>>>>> saved\n",
      "   19 | 0.000362 | 160000/160635 | 2.4900 | 17.0661 ||\n",
      "val: {'recall': 0.987694, 'recall_grapheme': 0.982341, 'recall_vowel': 0.992259, 'recall_consonant': 0.993835, 'recall_word': 0.981039, 'acc_grapheme': 0.980798, 'acc_vowel': 0.99229, 'acc_consonant': 0.992538, 'acc_word': 0.981221, 'loss_grapheme': 0.28009, 'loss_vowel': 0.204381, 'loss_consonant': 0.136422, 'loss_word': 0.213273}\n",
      "###>>>>> saved\n",
      "   20 | 0.000358 | 160000/160635 | 24.8392 | 16.8766 |\n",
      "val: {'recall': 0.988367, 'recall_grapheme': 0.983538, 'recall_vowel': 0.992502, 'recall_consonant': 0.99389, 'recall_word': 0.981545, 'acc_grapheme': 0.9804, 'acc_vowel': 0.992762, 'acc_consonant': 0.992936, 'acc_word': 0.981644, 'loss_grapheme': 0.282382, 'loss_vowel': 0.189099, 'loss_consonant': 0.129681, 'loss_word': 0.211628}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###>>>>> saved\n",
      "   21 | 0.000354 | 160000/160635 | 15.5916 | 16.6565 |\n",
      "val: {'recall': 0.98801, 'recall_grapheme': 0.983662, 'recall_vowel': 0.992418, 'recall_consonant': 0.992298, 'recall_word': 0.982332, 'acc_grapheme': 0.981445, 'acc_vowel': 0.992812, 'acc_consonant': 0.992886, 'acc_word': 0.982365, 'loss_grapheme': 0.324178, 'loss_vowel': 0.234505, 'loss_consonant': 0.151151, 'loss_word': 0.243136}\n",
      "   22 | 0.000350 | 160000/160635 | 23.2937 | 17.4513 |\n",
      "val: {'recall': 0.988243, 'recall_grapheme': 0.98287, 'recall_vowel': 0.993232, 'recall_consonant': 0.993999, 'recall_word': 0.982236, 'acc_grapheme': 0.981022, 'acc_vowel': 0.993011, 'acc_consonant': 0.993235, 'acc_word': 0.98239, 'loss_grapheme': 0.34441, 'loss_vowel': 0.244334, 'loss_consonant': 0.157811, 'loss_word': 0.282093}\n",
      "   23 | 0.000346 | 160000/160635 | 16.0969 | 17.3355 |\n",
      "val: {'recall': 0.987697, 'recall_grapheme': 0.983472, 'recall_vowel': 0.991819, 'recall_consonant': 0.992025, 'recall_word': 0.981513, 'acc_grapheme': 0.981619, 'acc_vowel': 0.992862, 'acc_consonant': 0.992911, 'acc_word': 0.981619, 'loss_grapheme': 0.236622, 'loss_vowel': 0.16262, 'loss_consonant': 0.107342, 'loss_word': 0.180174}\n",
      "   24 | 0.000341 | 160000/160635 | 21.3979 | 17.1300 |\n",
      "val: {'recall': 0.987971, 'recall_grapheme': 0.983033, 'recall_vowel': 0.991145, 'recall_consonant': 0.994674, 'recall_word': 0.982577, 'acc_grapheme': 0.981495, 'acc_vowel': 0.99224, 'acc_consonant': 0.992488, 'acc_word': 0.982639, 'loss_grapheme': 0.350988, 'loss_vowel': 0.253262, 'loss_consonant': 0.168491, 'loss_word': 0.243213}\n",
      "   25 | 0.000337 | 160000/160635 | 20.9972 | 17.1177 |\n",
      "val: {'recall': 0.988682, 'recall_grapheme': 0.983704, 'recall_vowel': 0.993359, 'recall_consonant': 0.993961, 'recall_word': 0.982753, 'acc_grapheme': 0.981719, 'acc_vowel': 0.992812, 'acc_consonant': 0.993359, 'acc_word': 0.982838, 'loss_grapheme': 0.275113, 'loss_vowel': 0.199015, 'loss_consonant': 0.129291, 'loss_word': 0.200743}\n",
      "###>>>>> saved\n",
      "   26 | 0.000332 | 160000/160635 | 23.1804 | 17.8410 |\n",
      "val: {'recall': 0.987352, 'recall_grapheme': 0.982601, 'recall_vowel': 0.991852, 'recall_consonant': 0.992353, 'recall_word': 0.982703, 'acc_grapheme': 0.980724, 'acc_vowel': 0.991643, 'acc_consonant': 0.992538, 'acc_word': 0.982689, 'loss_grapheme': 0.376371, 'loss_vowel': 0.276725, 'loss_consonant': 0.177767, 'loss_word': 0.26244}\n",
      "   27 | 0.000328 | 160000/160635 | 14.4470 | 17.1053 |\n",
      "val: {'recall': 0.987737, 'recall_grapheme': 0.983324, 'recall_vowel': 0.992391, 'recall_consonant': 0.99191, 'recall_word': 0.982666, 'acc_grapheme': 0.981992, 'acc_vowel': 0.992389, 'acc_consonant': 0.99316, 'acc_word': 0.982738, 'loss_grapheme': 0.337984, 'loss_vowel': 0.243071, 'loss_consonant': 0.155784, 'loss_word': 0.241117}\n",
      "   28 | 0.000323 | 160000/160635 | 15.3128 | 16.8768 |\n",
      "val: {'recall': 0.987468, 'recall_grapheme': 0.981863, 'recall_vowel': 0.992131, 'recall_consonant': 0.994016, 'recall_word': 0.982283, 'acc_grapheme': 0.980823, 'acc_vowel': 0.992464, 'acc_consonant': 0.992687, 'acc_word': 0.982365, 'loss_grapheme': 0.353332, 'loss_vowel': 0.259585, 'loss_consonant': 0.166046, 'loss_word': 0.248285}\n",
      "   29 | 0.000318 | 160000/160635 | 18.8869 | 17.0821 |\n",
      "val: {'recall': 0.988192, 'recall_grapheme': 0.98301, 'recall_vowel': 0.992732, 'recall_consonant': 0.994017, 'recall_word': 0.983027, 'acc_grapheme': 0.981171, 'acc_vowel': 0.992439, 'acc_consonant': 0.992663, 'acc_word': 0.983037, 'loss_grapheme': 0.366401, 'loss_vowel': 0.265626, 'loss_consonant': 0.172656, 'loss_word': 0.253463}\n",
      "   30 | 0.000312 | 160000/160635 | 19.8744 | 16.2851 |\n",
      "val: {'recall': 0.988903, 'recall_grapheme': 0.985533, 'recall_vowel': 0.993452, 'recall_consonant': 0.991094, 'recall_word': 0.984282, 'acc_grapheme': 0.983808, 'acc_vowel': 0.993707, 'acc_consonant': 0.993906, 'acc_word': 0.98438, 'loss_grapheme': 0.210391, 'loss_vowel': 0.142074, 'loss_consonant': 0.098753, 'loss_word': 0.147085}\n",
      "###>>>>> saved\n",
      "   31 | 0.000307 | 160000/160635 | 17.6695 | 16.7389 |\n",
      "val: {'recall': 0.988771, 'recall_grapheme': 0.98466, 'recall_vowel': 0.993055, 'recall_consonant': 0.992708, 'recall_word': 0.98416, 'acc_grapheme': 0.983733, 'acc_vowel': 0.993135, 'acc_consonant': 0.993906, 'acc_word': 0.984181, 'loss_grapheme': 0.231817, 'loss_vowel': 0.162561, 'loss_consonant': 0.104803, 'loss_word': 0.171621}\n",
      "   32 | 0.000302 | 160000/160635 | 15.1674 | 16.1244 |\n",
      "val: {'recall': 0.988153, 'recall_grapheme': 0.983805, 'recall_vowel': 0.993202, 'recall_consonant': 0.991799, 'recall_word': 0.983206, 'acc_grapheme': 0.982191, 'acc_vowel': 0.99311, 'acc_consonant': 0.993558, 'acc_word': 0.983261, 'loss_grapheme': 0.287124, 'loss_vowel': 0.214051, 'loss_consonant': 0.135744, 'loss_word': 0.221832}\n",
      "   33 | 0.000296 | 160000/160635 | 3.2225 | 16.6551 ||\n",
      "val: {'recall': 0.989356, 'recall_grapheme': 0.984897, 'recall_vowel': 0.993613, 'recall_consonant': 0.994017, 'recall_word': 0.984893, 'acc_grapheme': 0.984007, 'acc_vowel': 0.993459, 'acc_consonant': 0.994105, 'acc_word': 0.984902, 'loss_grapheme': 0.229957, 'loss_vowel': 0.160528, 'loss_consonant': 0.110729, 'loss_word': 0.160413}\n",
      "###>>>>> saved\n",
      "   34 | 0.000291 | 160000/160635 | 6.7783 | 16.5986 ||\n",
      "val: {'recall': 0.989073, 'recall_grapheme': 0.985332, 'recall_vowel': 0.99323, 'recall_consonant': 0.992399, 'recall_word': 0.984917, 'acc_grapheme': 0.983286, 'acc_vowel': 0.993284, 'acc_consonant': 0.993583, 'acc_word': 0.984927, 'loss_grapheme': 0.307985, 'loss_vowel': 0.228772, 'loss_consonant': 0.14896, 'loss_word': 0.207988}\n",
      "   35 | 0.000285 | 160000/160635 | 16.9726 | 16.7386 |\n",
      "val: {'recall': 0.989106, 'recall_grapheme': 0.98541, 'recall_vowel': 0.993667, 'recall_consonant': 0.991935, 'recall_word': 0.985188, 'acc_grapheme': 0.983932, 'acc_vowel': 0.993832, 'acc_consonant': 0.993658, 'acc_word': 0.985226, 'loss_grapheme': 0.298242, 'loss_vowel': 0.224771, 'loss_consonant': 0.151185, 'loss_word': 0.191105}\n",
      "   36 | 0.000279 | 160000/160635 | 20.4818 | 15.8777 |\n",
      "val: {'recall': 0.989696, 'recall_grapheme': 0.98552, 'recall_vowel': 0.993535, 'recall_consonant': 0.994209, 'recall_word': 0.985104, 'acc_grapheme': 0.984106, 'acc_vowel': 0.993583, 'acc_consonant': 0.994105, 'acc_word': 0.985076, 'loss_grapheme': 0.260517, 'loss_vowel': 0.183752, 'loss_consonant': 0.121131, 'loss_word': 0.181001}\n",
      "###>>>>> saved\n",
      "   37 | 0.000274 | 160000/160635 | 12.8417 | 16.3575 |\n",
      "val: {'recall': 0.989469, 'recall_grapheme': 0.985637, 'recall_vowel': 0.993301, 'recall_consonant': 0.993301, 'recall_word': 0.984352, 'acc_grapheme': 0.983659, 'acc_vowel': 0.993533, 'acc_consonant': 0.99418, 'acc_word': 0.984281, 'loss_grapheme': 0.247068, 'loss_vowel': 0.173272, 'loss_consonant': 0.119867, 'loss_word': 0.183487}\n",
      "   38 | 0.000268 | 160000/160635 | 8.3735 | 16.4500 ||\n",
      "val: {'recall': 0.99035, 'recall_grapheme': 0.986916, 'recall_vowel': 0.993812, 'recall_consonant': 0.993757, 'recall_word': 0.986069, 'acc_grapheme': 0.985698, 'acc_vowel': 0.994006, 'acc_consonant': 0.994702, 'acc_word': 0.986121, 'loss_grapheme': 0.175668, 'loss_vowel': 0.115907, 'loss_consonant': 0.081551, 'loss_word': 0.131107}\n",
      "###>>>>> saved\n",
      "   39 | 0.000262 | 160000/160635 | 18.9201 | 16.1510 |\n",
      "val: {'recall': 0.99007, 'recall_grapheme': 0.98546, 'recall_vowel': 0.994376, 'recall_consonant': 0.994983, 'recall_word': 0.985825, 'acc_grapheme': 0.98443, 'acc_vowel': 0.993881, 'acc_consonant': 0.994254, 'acc_word': 0.985823, 'loss_grapheme': 0.299035, 'loss_vowel': 0.223968, 'loss_consonant': 0.150113, 'loss_word': 0.192397}\n",
      "   40 | 0.000256 | 160000/160635 | 14.1863 | 15.9034 |\n",
      "val: {'recall': 0.990124, 'recall_grapheme': 0.985754, 'recall_vowel': 0.993971, 'recall_consonant': 0.995017, 'recall_word': 0.985857, 'acc_grapheme': 0.984082, 'acc_vowel': 0.993757, 'acc_consonant': 0.994503, 'acc_word': 0.985798, 'loss_grapheme': 0.27831, 'loss_vowel': 0.202457, 'loss_consonant': 0.134884, 'loss_word': 0.196146}\n",
      "   41 | 0.000250 | 160000/160635 | 21.7496 | 16.0394 |\n",
      "val: {'recall': 0.990527, 'recall_grapheme': 0.985961, 'recall_vowel': 0.994229, 'recall_consonant': 0.995955, 'recall_word': 0.986343, 'acc_grapheme': 0.984853, 'acc_vowel': 0.994105, 'acc_consonant': 0.994677, 'acc_word': 0.986395, 'loss_grapheme': 0.262223, 'loss_vowel': 0.201857, 'loss_consonant': 0.125975, 'loss_word': 0.179742}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###>>>>> saved\n",
      "   42 | 0.000244 | 160000/160635 | 21.5886 | 16.1747 |\n",
      "val: {'recall': 0.990179, 'recall_grapheme': 0.986513, 'recall_vowel': 0.993908, 'recall_consonant': 0.993783, 'recall_word': 0.986363, 'acc_grapheme': 0.984853, 'acc_vowel': 0.993583, 'acc_consonant': 0.994429, 'acc_word': 0.98637, 'loss_grapheme': 0.29636, 'loss_vowel': 0.225799, 'loss_consonant': 0.150296, 'loss_word': 0.187128}\n",
      "   43 | 0.000238 | 160000/160635 | 0.9276 | 15.5587 ||\n",
      "val: {'recall': 0.989957, 'recall_grapheme': 0.986283, 'recall_vowel': 0.993358, 'recall_consonant': 0.993906, 'recall_word': 0.985797, 'acc_grapheme': 0.985126, 'acc_vowel': 0.993856, 'acc_consonant': 0.994478, 'acc_word': 0.985897, 'loss_grapheme': 0.200493, 'loss_vowel': 0.146154, 'loss_consonant': 0.099368, 'loss_word': 0.133185}\n",
      "   44 | 0.000231 | 160000/160635 | 0.9186 | 15.4935 ||\n",
      "val: {'recall': 0.989522, 'recall_grapheme': 0.985582, 'recall_vowel': 0.993522, 'recall_consonant': 0.993404, 'recall_word': 0.986566, 'acc_grapheme': 0.985027, 'acc_vowel': 0.993856, 'acc_consonant': 0.994404, 'acc_word': 0.986569, 'loss_grapheme': 0.230975, 'loss_vowel': 0.165821, 'loss_consonant': 0.111825, 'loss_word': 0.14875}\n",
      "   45 | 0.000225 | 160000/160635 | 20.2045 | 15.9692 |\n",
      "val: {'recall': 0.991009, 'recall_grapheme': 0.987102, 'recall_vowel': 0.993937, 'recall_consonant': 0.995895, 'recall_word': 0.986755, 'acc_grapheme': 0.985325, 'acc_vowel': 0.993732, 'acc_consonant': 0.994528, 'acc_word': 0.986718, 'loss_grapheme': 0.326273, 'loss_vowel': 0.242942, 'loss_consonant': 0.159322, 'loss_word': 0.208526}\n",
      "###>>>>> saved\n",
      "   46 | 0.000219 | 160000/160635 | 19.1707 | 15.9692 |\n",
      "val: {'recall': 0.990872, 'recall_grapheme': 0.986588, 'recall_vowel': 0.994551, 'recall_consonant': 0.99576, 'recall_word': 0.987132, 'acc_grapheme': 0.985698, 'acc_vowel': 0.994031, 'acc_consonant': 0.994727, 'acc_word': 0.987216, 'loss_grapheme': 0.255246, 'loss_vowel': 0.190221, 'loss_consonant': 0.121737, 'loss_word': 0.168415}\n",
      "   47 | 0.000213 | 160000/160635 | 13.4044 | 16.2878 |\n",
      "val: {'recall': 0.99105, 'recall_grapheme': 0.987167, 'recall_vowel': 0.994005, 'recall_consonant': 0.995861, 'recall_word': 0.987187, 'acc_grapheme': 0.985425, 'acc_vowel': 0.994006, 'acc_consonant': 0.994677, 'acc_word': 0.987216, 'loss_grapheme': 0.313882, 'loss_vowel': 0.230896, 'loss_consonant': 0.153054, 'loss_word': 0.214174}\n",
      "###>>>>> saved\n",
      "   48 | 0.000206 | 160000/160635 | 17.8309 | 15.7753 |\n",
      "val: {'recall': 0.990196, 'recall_grapheme': 0.986337, 'recall_vowel': 0.993732, 'recall_consonant': 0.994377, 'recall_word': 0.986937, 'acc_grapheme': 0.985251, 'acc_vowel': 0.993608, 'acc_consonant': 0.994578, 'acc_word': 0.986867, 'loss_grapheme': 0.318983, 'loss_vowel': 0.235957, 'loss_consonant': 0.153987, 'loss_word': 0.196371}\n",
      "   49 | 0.000200 | 160000/160635 | 20.2042 | 16.1598 |\n",
      "val: {'recall': 0.990818, 'recall_grapheme': 0.987193, 'recall_vowel': 0.994288, 'recall_consonant': 0.994597, 'recall_word': 0.987778, 'acc_grapheme': 0.985773, 'acc_vowel': 0.994279, 'acc_consonant': 0.994951, 'acc_word': 0.987837, 'loss_grapheme': 0.274768, 'loss_vowel': 0.204044, 'loss_consonant': 0.133328, 'loss_word': 0.171912}\n",
      "   50 | 0.000194 | 160000/160635 | 12.6637 | 15.3295 |\n",
      "val: {'recall': 0.99084, 'recall_grapheme': 0.987309, 'recall_vowel': 0.993815, 'recall_consonant': 0.994925, 'recall_word': 0.987989, 'acc_grapheme': 0.98627, 'acc_vowel': 0.99408, 'acc_consonant': 0.995001, 'acc_word': 0.987987, 'loss_grapheme': 0.2962, 'loss_vowel': 0.219603, 'loss_consonant': 0.139805, 'loss_word': 0.193349}\n",
      "   51 | 0.000187 | 160000/160635 | 15.1974 | 16.3897 |\n",
      "val: {'recall': 0.990961, 'recall_grapheme': 0.987545, 'recall_vowel': 0.994668, 'recall_consonant': 0.994088, 'recall_word': 0.987692, 'acc_grapheme': 0.986121, 'acc_vowel': 0.994379, 'acc_consonant': 0.994951, 'acc_word': 0.987738, 'loss_grapheme': 0.258702, 'loss_vowel': 0.193913, 'loss_consonant': 0.12452, 'loss_word': 0.170629}\n",
      "   52 | 0.000181 | 160000/160635 | 15.5097 | 15.8344 |\n",
      "val: {'recall': 0.990461, 'recall_grapheme': 0.986435, 'recall_vowel': 0.993856, 'recall_consonant': 0.995116, 'recall_word': 0.987904, 'acc_grapheme': 0.985425, 'acc_vowel': 0.993832, 'acc_consonant': 0.994677, 'acc_word': 0.987837, 'loss_grapheme': 0.30679, 'loss_vowel': 0.227848, 'loss_consonant': 0.154195, 'loss_word': 0.18213}\n",
      "   53 | 0.000175 | 160000/160635 | 18.6329 | 15.9160 |\n",
      "val: {'recall': 0.9909, 'recall_grapheme': 0.986836, 'recall_vowel': 0.994525, 'recall_consonant': 0.995404, 'recall_word': 0.988036, 'acc_grapheme': 0.985897, 'acc_vowel': 0.99423, 'acc_consonant': 0.994926, 'acc_word': 0.988061, 'loss_grapheme': 0.275607, 'loss_vowel': 0.211016, 'loss_consonant': 0.141818, 'loss_word': 0.162223}\n",
      "   54 | 0.000169 | 160000/160635 | 18.2592 | 15.5298 |\n",
      "val: {'recall': 0.990996, 'recall_grapheme': 0.986984, 'recall_vowel': 0.994301, 'recall_consonant': 0.995716, 'recall_word': 0.988333, 'acc_grapheme': 0.985798, 'acc_vowel': 0.994354, 'acc_consonant': 0.994901, 'acc_word': 0.98831, 'loss_grapheme': 0.30841, 'loss_vowel': 0.237427, 'loss_consonant': 0.153178, 'loss_word': 0.182059}\n",
      "   55 | 0.000163 | 160000/160635 | 20.9746 | 15.5560 |\n",
      "val: {'recall': 0.990341, 'recall_grapheme': 0.98616, 'recall_vowel': 0.993766, 'recall_consonant': 0.995278, 'recall_word': 0.987536, 'acc_grapheme': 0.984927, 'acc_vowel': 0.993931, 'acc_consonant': 0.994478, 'acc_word': 0.987415, 'loss_grapheme': 0.329232, 'loss_vowel': 0.257921, 'loss_consonant': 0.168136, 'loss_word': 0.186577}\n",
      "   56 | 0.000156 | 160000/160635 | 10.1844 | 15.5165 |\n",
      "val: {'recall': 0.990997, 'recall_grapheme': 0.987578, 'recall_vowel': 0.99452, 'recall_consonant': 0.994314, 'recall_word': 0.988144, 'acc_grapheme': 0.986121, 'acc_vowel': 0.994304, 'acc_consonant': 0.994976, 'acc_word': 0.988136, 'loss_grapheme': 0.273579, 'loss_vowel': 0.212448, 'loss_consonant': 0.141618, 'loss_word': 0.163656}\n",
      "   57 | 0.000150 | 160000/160635 | 7.2618 | 16.3662 ||\n",
      "val: {'recall': 0.99126, 'recall_grapheme': 0.987626, 'recall_vowel': 0.994343, 'recall_consonant': 0.995444, 'recall_word': 0.988735, 'acc_grapheme': 0.986867, 'acc_vowel': 0.994404, 'acc_consonant': 0.995249, 'acc_word': 0.988758, 'loss_grapheme': 0.244243, 'loss_vowel': 0.184031, 'loss_consonant': 0.1213, 'loss_word': 0.147677}\n",
      "###>>>>> saved\n",
      "   58 | 0.000144 | 160000/160635 | 15.2638 | 15.3465 |\n",
      "val: {'recall': 0.99164, 'recall_grapheme': 0.988478, 'recall_vowel': 0.99497, 'recall_consonant': 0.994634, 'recall_word': 0.988096, 'acc_grapheme': 0.987315, 'acc_vowel': 0.994827, 'acc_consonant': 0.995647, 'acc_word': 0.988111, 'loss_grapheme': 0.217732, 'loss_vowel': 0.160714, 'loss_consonant': 0.108939, 'loss_word': 0.147056}\n",
      "###>>>>> saved\n",
      "   59 | 0.000138 | 160000/160635 | 10.6169 | 15.6226 |\n",
      "val: {'recall': 0.991297, 'recall_grapheme': 0.987611, 'recall_vowel': 0.994734, 'recall_consonant': 0.99523, 'recall_word': 0.988544, 'acc_grapheme': 0.98724, 'acc_vowel': 0.994578, 'acc_consonant': 0.995473, 'acc_word': 0.988534, 'loss_grapheme': 0.251801, 'loss_vowel': 0.186986, 'loss_consonant': 0.12689, 'loss_word': 0.15499}\n",
      "   60 | 0.000132 | 160000/160635 | 17.1215 | 15.3304 |\n",
      "val: {'recall': 0.991613, 'recall_grapheme': 0.988125, 'recall_vowel': 0.994797, 'recall_consonant': 0.995405, 'recall_word': 0.988305, 'acc_grapheme': 0.986768, 'acc_vowel': 0.994578, 'acc_consonant': 0.995075, 'acc_word': 0.988335, 'loss_grapheme': 0.301748, 'loss_vowel': 0.247315, 'loss_consonant': 0.15799, 'loss_word': 0.178363}\n",
      "   61 | 0.000126 | 160000/160635 | 20.7184 | 15.2986 |\n",
      "val: {'recall': 0.991165, 'recall_grapheme': 0.988065, 'recall_vowel': 0.994295, 'recall_consonant': 0.994237, 'recall_word': 0.988379, 'acc_grapheme': 0.98739, 'acc_vowel': 0.994379, 'acc_consonant': 0.995324, 'acc_word': 0.988434, 'loss_grapheme': 0.24155, 'loss_vowel': 0.190274, 'loss_consonant': 0.122931, 'loss_word': 0.143479}\n",
      "   62 | 0.000121 | 160000/160635 | 5.6094 | 15.2014 ||\n",
      "val: {'recall': 0.991934, 'recall_grapheme': 0.988712, 'recall_vowel': 0.99493, 'recall_consonant': 0.995383, 'recall_word': 0.98844, 'acc_grapheme': 0.987116, 'acc_vowel': 0.994528, 'acc_consonant': 0.995647, 'acc_word': 0.988509, 'loss_grapheme': 0.244574, 'loss_vowel': 0.189367, 'loss_consonant': 0.122491, 'loss_word': 0.155123}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###>>>>> saved\n",
      "   63 | 0.000115 | 160000/160635 | 21.0869 | 15.4909 |\n",
      "val: {'recall': 0.991303, 'recall_grapheme': 0.987797, 'recall_vowel': 0.994674, 'recall_consonant': 0.994944, 'recall_word': 0.988234, 'acc_grapheme': 0.987191, 'acc_vowel': 0.994652, 'acc_consonant': 0.99515, 'acc_word': 0.988285, 'loss_grapheme': 0.248291, 'loss_vowel': 0.195017, 'loss_consonant': 0.12395, 'loss_word': 0.15407}\n",
      "   64 | 0.000109 | 160000/160635 | 17.6079 | 15.3241 |\n",
      "val: {'recall': 0.991486, 'recall_grapheme': 0.987871, 'recall_vowel': 0.994552, 'recall_consonant': 0.995651, 'recall_word': 0.989058, 'acc_grapheme': 0.986818, 'acc_vowel': 0.994354, 'acc_consonant': 0.99505, 'acc_word': 0.989006, 'loss_grapheme': 0.301503, 'loss_vowel': 0.234748, 'loss_consonant': 0.153272, 'loss_word': 0.16906}\n",
      "   65 | 0.000104 | 160000/160635 | 11.3319 | 15.2315 |\n",
      "val: {'recall': 0.991995, 'recall_grapheme': 0.988611, 'recall_vowel': 0.995208, 'recall_consonant': 0.99555, 'recall_word': 0.988736, 'acc_grapheme': 0.987837, 'acc_vowel': 0.995075, 'acc_consonant': 0.995523, 'acc_word': 0.988807, 'loss_grapheme': 0.232567, 'loss_vowel': 0.180341, 'loss_consonant': 0.113221, 'loss_word': 0.153469}\n",
      "###>>>>> saved\n",
      "   66 | 0.000098 | 160000/160635 | 20.4229 | 15.2405 |\n",
      "val: {'recall': 0.991734, 'recall_grapheme': 0.988498, 'recall_vowel': 0.994864, 'recall_consonant': 0.995076, 'recall_word': 0.988369, 'acc_grapheme': 0.987489, 'acc_vowel': 0.994702, 'acc_consonant': 0.995448, 'acc_word': 0.988409, 'loss_grapheme': 0.229002, 'loss_vowel': 0.175597, 'loss_consonant': 0.115152, 'loss_word': 0.145149}\n",
      "   67 | 0.000093 | 160000/160635 | 11.9867 | 15.1450 |\n",
      "val: {'recall': 0.991328, 'recall_grapheme': 0.987367, 'recall_vowel': 0.99494, 'recall_consonant': 0.995638, 'recall_word': 0.988767, 'acc_grapheme': 0.986643, 'acc_vowel': 0.994652, 'acc_consonant': 0.995423, 'acc_word': 0.988807, 'loss_grapheme': 0.28292, 'loss_vowel': 0.219256, 'loss_consonant': 0.142424, 'loss_word': 0.168302}\n",
      "   68 | 0.000088 | 160000/160635 | 11.6757 | 15.0957 |\n",
      "val: {'recall': 0.991239, 'recall_grapheme': 0.987707, 'recall_vowel': 0.993935, 'recall_consonant': 0.995607, 'recall_word': 0.989133, 'acc_grapheme': 0.986544, 'acc_vowel': 0.994155, 'acc_consonant': 0.994951, 'acc_word': 0.98918, 'loss_grapheme': 0.322736, 'loss_vowel': 0.264851, 'loss_consonant': 0.164949, 'loss_word': 0.174399}\n",
      "   69 | 0.000082 | 160000/160635 | 11.5914 | 15.4230 |\n",
      "val: {'recall': 0.992347, 'recall_grapheme': 0.989214, 'recall_vowel': 0.995092, 'recall_consonant': 0.995866, 'recall_word': 0.989102, 'acc_grapheme': 0.988186, 'acc_vowel': 0.994851, 'acc_consonant': 0.995871, 'acc_word': 0.989156, 'loss_grapheme': 0.229701, 'loss_vowel': 0.165878, 'loss_consonant': 0.110527, 'loss_word': 0.156143}\n",
      "###>>>>> saved\n",
      "   70 | 0.000077 | 160000/160635 | 17.5724 | 15.6579 |\n",
      "val: {'recall': 0.991741, 'recall_grapheme': 0.988223, 'recall_vowel': 0.995016, 'recall_consonant': 0.995504, 'recall_word': 0.989427, 'acc_grapheme': 0.987415, 'acc_vowel': 0.994851, 'acc_consonant': 0.995672, 'acc_word': 0.989454, 'loss_grapheme': 0.240913, 'loss_vowel': 0.18645, 'loss_consonant': 0.125173, 'loss_word': 0.142162}\n",
      "   71 | 0.000073 | 160000/160635 | 13.8454 | 15.9541 |\n",
      "val: {'recall': 0.991832, 'recall_grapheme': 0.988204, 'recall_vowel': 0.994876, 'recall_consonant': 0.996043, 'recall_word': 0.988445, 'acc_grapheme': 0.98734, 'acc_vowel': 0.994677, 'acc_consonant': 0.995797, 'acc_word': 0.988459, 'loss_grapheme': 0.252039, 'loss_vowel': 0.193635, 'loss_consonant': 0.125448, 'loss_word': 0.161492}\n",
      "   72 | 0.000068 | 160000/160635 | 5.1014 | 15.0514 ||\n",
      "val: {'recall': 0.992212, 'recall_grapheme': 0.989558, 'recall_vowel': 0.995012, 'recall_consonant': 0.99472, 'recall_word': 0.989107, 'acc_grapheme': 0.988559, 'acc_vowel': 0.994976, 'acc_consonant': 0.995946, 'acc_word': 0.98918, 'loss_grapheme': 0.206889, 'loss_vowel': 0.146962, 'loss_consonant': 0.101167, 'loss_word': 0.133665}\n",
      "   73 | 0.000063 | 160000/160635 | 8.4648 | 15.2737 ||\n",
      "val: {'recall': 0.992082, 'recall_grapheme': 0.988763, 'recall_vowel': 0.994924, 'recall_consonant': 0.995877, 'recall_word': 0.98944, 'acc_grapheme': 0.988086, 'acc_vowel': 0.994951, 'acc_consonant': 0.995896, 'acc_word': 0.989429, 'loss_grapheme': 0.226571, 'loss_vowel': 0.16748, 'loss_consonant': 0.110369, 'loss_word': 0.14597}\n",
      "   74 | 0.000059 | 160000/160635 | 13.8177 | 14.9581 |\n",
      "val: {'recall': 0.991834, 'recall_grapheme': 0.988681, 'recall_vowel': 0.995155, 'recall_consonant': 0.99482, 'recall_word': 0.989176, 'acc_grapheme': 0.987887, 'acc_vowel': 0.994926, 'acc_consonant': 0.995821, 'acc_word': 0.989255, 'loss_grapheme': 0.241058, 'loss_vowel': 0.187461, 'loss_consonant': 0.119594, 'loss_word': 0.153814}\n",
      "   75 | 0.000054 | 160000/160635 | 15.0491 | 15.3840 |\n",
      "val: {'recall': 0.991671, 'recall_grapheme': 0.988075, 'recall_vowel': 0.994825, 'recall_consonant': 0.99571, 'recall_word': 0.989163, 'acc_grapheme': 0.98734, 'acc_vowel': 0.994652, 'acc_consonant': 0.995672, 'acc_word': 0.989205, 'loss_grapheme': 0.269059, 'loss_vowel': 0.218702, 'loss_consonant': 0.141698, 'loss_word': 0.162969}\n",
      "   76 | 0.000050 | 160000/160635 | 13.6407 | 14.8288 |\n",
      "val: {'recall': 0.992383, 'recall_grapheme': 0.989255, 'recall_vowel': 0.995139, 'recall_consonant': 0.995884, 'recall_word': 0.989408, 'acc_grapheme': 0.989081, 'acc_vowel': 0.995175, 'acc_consonant': 0.996145, 'acc_word': 0.989454, 'loss_grapheme': 0.167321, 'loss_vowel': 0.120949, 'loss_consonant': 0.078694, 'loss_word': 0.113641}\n",
      "###>>>>> saved\n",
      "   77 | 0.000046 | 160000/160635 | 18.1327 | 15.3466 |\n",
      "val: {'recall': 0.991606, 'recall_grapheme': 0.987872, 'recall_vowel': 0.994912, 'recall_consonant': 0.99577, 'recall_word': 0.988761, 'acc_grapheme': 0.987489, 'acc_vowel': 0.994727, 'acc_consonant': 0.995672, 'acc_word': 0.988807, 'loss_grapheme': 0.276516, 'loss_vowel': 0.218427, 'loss_consonant': 0.140244, 'loss_word': 0.16081}\n",
      "   78 | 0.000042 | 160000/160635 | 16.5099 | 14.8519 |\n",
      "val: {'recall': 0.99127, 'recall_grapheme': 0.988098, 'recall_vowel': 0.994802, 'recall_consonant': 0.994084, 'recall_word': 0.989157, 'acc_grapheme': 0.987589, 'acc_vowel': 0.994802, 'acc_consonant': 0.995598, 'acc_word': 0.98918, 'loss_grapheme': 0.299609, 'loss_vowel': 0.232466, 'loss_consonant': 0.151835, 'loss_word': 0.176826}\n",
      "   79 | 0.000038 | 160000/160635 | 15.8550 | 14.9167 |\n",
      "val: {'recall': 0.991789, 'recall_grapheme': 0.988314, 'recall_vowel': 0.994754, 'recall_consonant': 0.995773, 'recall_word': 0.989347, 'acc_grapheme': 0.987887, 'acc_vowel': 0.994727, 'acc_consonant': 0.995821, 'acc_word': 0.989379, 'loss_grapheme': 0.256434, 'loss_vowel': 0.206098, 'loss_consonant': 0.13201, 'loss_word': 0.146109}\n",
      "   80 | 0.000035 | 160000/160635 | 11.9438 | 14.9235 |\n",
      "val: {'recall': 0.992275, 'recall_grapheme': 0.988983, 'recall_vowel': 0.995245, 'recall_consonant': 0.995892, 'recall_word': 0.989615, 'acc_grapheme': 0.988409, 'acc_vowel': 0.99505, 'acc_consonant': 0.99607, 'acc_word': 0.989653, 'loss_grapheme': 0.212035, 'loss_vowel': 0.156516, 'loss_consonant': 0.101814, 'loss_word': 0.13834}\n",
      "   81 | 0.000031 | 160000/160635 | 7.6754 | 14.8821 ||\n",
      "val: {'recall': 0.992515, 'recall_grapheme': 0.990199, 'recall_vowel': 0.995289, 'recall_consonant': 0.994371, 'recall_word': 0.989434, 'acc_grapheme': 0.989379, 'acc_vowel': 0.995249, 'acc_consonant': 0.996219, 'acc_word': 0.989504, 'loss_grapheme': 0.180598, 'loss_vowel': 0.129995, 'loss_consonant': 0.083757, 'loss_word': 0.13174}\n",
      "###>>>>> saved\n",
      "   82 | 0.000028 | 160000/160635 | 14.7509 | 15.4956 |\n",
      "val: {'recall': 0.991362, 'recall_grapheme': 0.987695, 'recall_vowel': 0.994347, 'recall_consonant': 0.995711, 'recall_word': 0.989126, 'acc_grapheme': 0.987017, 'acc_vowel': 0.99423, 'acc_consonant': 0.995399, 'acc_word': 0.989131, 'loss_grapheme': 0.320093, 'loss_vowel': 0.268967, 'loss_consonant': 0.172667, 'loss_word': 0.176597}\n",
      "   83 | 0.000025 | 160000/160635 | 21.1181 | 15.9073 |\n",
      "val: {'recall': 0.991696, 'recall_grapheme': 0.98814, 'recall_vowel': 0.994602, 'recall_consonant': 0.995901, 'recall_word': 0.989227, 'acc_grapheme': 0.987713, 'acc_vowel': 0.994677, 'acc_consonant': 0.995772, 'acc_word': 0.98928, 'loss_grapheme': 0.25926, 'loss_vowel': 0.198457, 'loss_consonant': 0.125538, 'loss_word': 0.150843}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   84 | 0.000022 | 160000/160635 | 19.3180 | 14.6824 |\n",
      "val: {'recall': 0.991856, 'recall_grapheme': 0.9882, 'recall_vowel': 0.99511, 'recall_consonant': 0.995914, 'recall_word': 0.989211, 'acc_grapheme': 0.987663, 'acc_vowel': 0.995001, 'acc_consonant': 0.995722, 'acc_word': 0.98923, 'loss_grapheme': 0.262447, 'loss_vowel': 0.205642, 'loss_consonant': 0.128688, 'loss_word': 0.160564}\n",
      "   85 | 0.000019 | 160000/160635 | 16.4641 | 15.8382 |\n",
      "val: {'recall': 0.99163, 'recall_grapheme': 0.988013, 'recall_vowel': 0.994678, 'recall_consonant': 0.995817, 'recall_word': 0.989342, 'acc_grapheme': 0.987191, 'acc_vowel': 0.994503, 'acc_consonant': 0.995399, 'acc_word': 0.989379, 'loss_grapheme': 0.314966, 'loss_vowel': 0.257951, 'loss_consonant': 0.16325, 'loss_word': 0.172734}\n",
      "   86 | 0.000016 | 160000/160635 | 13.6084 | 15.3989 |\n",
      "val: {'recall': 0.991518, 'recall_grapheme': 0.988468, 'recall_vowel': 0.994388, 'recall_consonant': 0.994748, 'recall_word': 0.989141, 'acc_grapheme': 0.987862, 'acc_vowel': 0.994603, 'acc_consonant': 0.995647, 'acc_word': 0.989156, 'loss_grapheme': 0.240468, 'loss_vowel': 0.190033, 'loss_consonant': 0.124846, 'loss_word': 0.131344}\n",
      "   87 | 0.000014 | 160000/160635 | 20.0424 | 14.9108 |\n",
      "val: {'recall': 0.991971, 'recall_grapheme': 0.988647, 'recall_vowel': 0.994802, 'recall_consonant': 0.995789, 'recall_word': 0.989524, 'acc_grapheme': 0.988186, 'acc_vowel': 0.994827, 'acc_consonant': 0.995846, 'acc_word': 0.989529, 'loss_grapheme': 0.229895, 'loss_vowel': 0.181676, 'loss_consonant': 0.115813, 'loss_word': 0.134058}\n",
      "   88 | 0.000012 | 160000/160635 | 13.9081 | 14.5555 |\n",
      "val: {'recall': 0.992253, 'recall_grapheme': 0.988847, 'recall_vowel': 0.994874, 'recall_consonant': 0.996445, 'recall_word': 0.989735, 'acc_grapheme': 0.988509, 'acc_vowel': 0.994802, 'acc_consonant': 0.996045, 'acc_word': 0.989753, 'loss_grapheme': 0.225989, 'loss_vowel': 0.179977, 'loss_consonant': 0.118972, 'loss_word': 0.126985}\n",
      "   89 | 0.000010 | 160000/160635 | 5.6774 | 15.4479 ||\n",
      "val: {'recall': 0.992362, 'recall_grapheme': 0.989112, 'recall_vowel': 0.995245, 'recall_consonant': 0.99598, 'recall_word': 0.989539, 'acc_grapheme': 0.988658, 'acc_vowel': 0.995001, 'acc_consonant': 0.996095, 'acc_word': 0.989578, 'loss_grapheme': 0.239795, 'loss_vowel': 0.180544, 'loss_consonant': 0.11578, 'loss_word': 0.150779}\n",
      "   90 | 0.000008 | 160000/160635 | 9.2286 | 15.0841 ||\n",
      "val: {'recall': 0.992018, 'recall_grapheme': 0.988729, 'recall_vowel': 0.994807, 'recall_consonant': 0.995808, 'recall_word': 0.989698, 'acc_grapheme': 0.988036, 'acc_vowel': 0.994976, 'acc_consonant': 0.996045, 'acc_word': 0.989728, 'loss_grapheme': 0.240017, 'loss_vowel': 0.185679, 'loss_consonant': 0.123295, 'loss_word': 0.142788}\n",
      "   91 | 0.000006 | 160000/160635 | 6.3401 | 14.7057 ||\n",
      "val: {'recall': 0.991902, 'recall_grapheme': 0.988716, 'recall_vowel': 0.995259, 'recall_consonant': 0.994916, 'recall_word': 0.989348, 'acc_grapheme': 0.988111, 'acc_vowel': 0.995025, 'acc_consonant': 0.996095, 'acc_word': 0.98933, 'loss_grapheme': 0.229332, 'loss_vowel': 0.173922, 'loss_consonant': 0.110303, 'loss_word': 0.146055}\n",
      "   92 | 0.000005 | 160000/160635 | 15.2676 | 15.0994 |\n",
      "val: {'recall': 0.99244, 'recall_grapheme': 0.989274, 'recall_vowel': 0.9952, 'recall_consonant': 0.996014, 'recall_word': 0.98951, 'acc_grapheme': 0.988658, 'acc_vowel': 0.995125, 'acc_consonant': 0.996045, 'acc_word': 0.989554, 'loss_grapheme': 0.220525, 'loss_vowel': 0.168363, 'loss_consonant': 0.109643, 'loss_word': 0.13928}\n",
      "   93 | 0.000004 | 160000/160635 | 14.1091 | 15.6024 |\n",
      "val: {'recall': 0.991921, 'recall_grapheme': 0.988499, 'recall_vowel': 0.994738, 'recall_consonant': 0.995949, 'recall_word': 0.989582, 'acc_grapheme': 0.988061, 'acc_vowel': 0.994827, 'acc_consonant': 0.995871, 'acc_word': 0.989628, 'loss_grapheme': 0.247576, 'loss_vowel': 0.192146, 'loss_consonant': 0.123482, 'loss_word': 0.144189}\n",
      "   94 | 0.000002 | 160000/160635 | 19.0936 | 15.3838 |\n",
      "val: {'recall': 0.991857, 'recall_grapheme': 0.988404, 'recall_vowel': 0.994806, 'recall_consonant': 0.995815, 'recall_word': 0.989413, 'acc_grapheme': 0.987688, 'acc_vowel': 0.994777, 'acc_consonant': 0.995598, 'acc_word': 0.989404, 'loss_grapheme': 0.284055, 'loss_vowel': 0.232263, 'loss_consonant': 0.146886, 'loss_word': 0.158985}\n",
      "   95 | 0.000002 | 088960/160635 | 22.1310 | 15.2445 |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc7c8eb40d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chec/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/chec/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/chec/anaconda3/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/home/chec/anaconda3/lib/python3.7/multiprocessing/popen_fork.py\", line 48, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/home/chec/anaconda3/lib/python3.7/multiprocessing/popen_fork.py\", line 28, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-6df1386d1bbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-f52fdf61a139>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcycle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_cycles\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CYCLE:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtrain_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-f52fdf61a139>\u001b[0m in \u001b[0;36mtrain_cycle\u001b[0;34m(args, model, optimizer, lr_scheduler)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0;31m#loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(args, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_model(model, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
