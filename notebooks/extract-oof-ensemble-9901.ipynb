{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time, gc\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pretrainedmodels\n",
    "from argparse import Namespace\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "#from efficientnet_pytorch import EfficientNet\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from apex import amp\n",
    "BATCH_SIZE = 512\n",
    "ENABLE_APEX = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_DIR = '/kaggle/input/bengaliai-cv19'\n",
    "#MODEL_DIR = '/kaggle/input/model3-weights'\n",
    "\n",
    "DATA_DIR = '/mnt/chicm/data/bengali'\n",
    "#MODEL_DIR = './models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "#test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "#class_map_df = pd.read_csv(f'{DATA_DIR}/class_map.csv')\n",
    "#sample_sub_df = pd.read_csv(f'{DATA_DIR}/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvcore.data.auto_augment import RandAugment\n",
    "def get_train_augs():\n",
    "    return RandAugment(n=2, m=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class BengaliDataset(Dataset):\n",
    "    def __init__(self, df, img_df, train_mode=True, test_mode=False):\n",
    "        self.df = df\n",
    "        self.img_df = img_df\n",
    "        self.train_mode = train_mode\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = self.get_img(row.image_id)\n",
    "        orig_img = img.copy()\n",
    "        #print(img.shape)\n",
    "        if self.train_mode:\n",
    "            augs = get_train_augs()\n",
    "            #img = augs(image=img)['image']\n",
    "            img = np.asarray(augs(Image.fromarray(img)))\n",
    "        \n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "        orig_img = np.expand_dims(orig_img, axis=-1)\n",
    "        \n",
    "        #print('###', img.shape)\n",
    "        #img = np.concatenate([img, img, img], 2)\n",
    "        #print('>>>', img.shape)\n",
    "        \n",
    "        # taken from https://www.kaggle.com/iafoss/image-preprocessing-128x128\n",
    "        #MEAN = [ 0.06922848809290576,  0.06922848809290576,  0.06922848809290576]\n",
    "        #STD = [ 0.20515700083327537,  0.20515700083327537,  0.20515700083327537]\n",
    "        \n",
    "        img = transforms.functional.to_tensor(img)\n",
    "        orig_img = transforms.functional.to_tensor(orig_img)\n",
    "        \n",
    "        #img = transforms.functional.normalize(img, mean=MEAN, std=STD)\n",
    "        \n",
    "        if self.test_mode:\n",
    "            return img\n",
    "        elif self.train_mode:\n",
    "            return img, orig_img, torch.tensor([row.grapheme_root, row.vowel_diacritic, row.consonant_diacritic, row.word_label])\n",
    "        else:\n",
    "            return img, torch.tensor([row.grapheme_root, row.vowel_diacritic, row.consonant_diacritic, row.word_label])\n",
    "                    \n",
    "    def get_img(self, img_id):\n",
    "        return 255 - self.img_df.loc[img_id].values.reshape(HEIGHT, WIDTH).astype(np.uint8)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "def get_train_val_loaders(batch_size=4, val_batch_size=4, ifold=0, dev_mode=False):\n",
    "    train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "\n",
    "    train_df = shuffle(train_df, random_state=1234)\n",
    "\n",
    "    grapheme_words = np.unique(train_df.grapheme.values)\n",
    "    grapheme_words_dict = {grapheme: i for i, grapheme in enumerate(grapheme_words)}\n",
    "    train_df['word_label'] = train_df['grapheme'].map(lambda x: grapheme_words_dict[x])\n",
    "\n",
    "    print(train_df.shape)\n",
    "\n",
    "    if dev_mode:\n",
    "        img_df = pd.read_parquet(f'{DATA_DIR}/train_image_data_0.parquet').set_index('image_id')\n",
    "        train_df = train_df.iloc[:1000]\n",
    "    else:\n",
    "        img_dfs = [pd.read_parquet(f'{DATA_DIR}/train_image_data_{i}.parquet') for i in range(4)]\n",
    "        img_df = pd.concat(img_dfs, axis=0).set_index('image_id')\n",
    "    print(img_df.shape)\n",
    "    #split_index = int(len(train_df) * 0.9)\n",
    "    \n",
    "    #train = train_df.iloc[:split_index]\n",
    "    #val = train_df.iloc[split_index:]\n",
    "    \n",
    "    kf = StratifiedKFold(5, random_state=1234, shuffle=True)\n",
    "    for i, (train_idx, val_idx) in enumerate(kf.split(train_df, train_df['grapheme_root'].values)):\n",
    "        if i == ifold:\n",
    "            #print(val_idx)\n",
    "            train = train_df.iloc[train_idx]\n",
    "            val = train_df.iloc[val_idx]\n",
    "            break\n",
    "    assert i == ifold\n",
    "    print(train.shape, val.shape)\n",
    "    \n",
    "    train_ds = BengaliDataset(train, img_df, True, False)\n",
    "    val_ds = BengaliDataset(val, img_df, False, False)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "    train_loader.num = len(train_ds)\n",
    "\n",
    "    val_loader = DataLoader(val_ds, batch_size=val_batch_size, shuffle=False, num_workers=8, drop_last=False)\n",
    "    val_loader.num = len(val_ds)\n",
    "    val_loader.df = val\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_loader(batch_size=4, idx=0):\n",
    "    img_df = pd.read_parquet(f'{DATA_DIR}/test_image_data_{idx}.parquet').set_index('image_id')\n",
    "\n",
    "    ds = BengaliDataset(img_df)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=False)\n",
    "    loader.num = len(ds)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from timm.models.activations import Swish, Mish\n",
    "from timm.models.adaptive_avgmax_pool import SelectAdaptivePool2d\n",
    "MEAN = [ 0.06922848809290576 ]\n",
    "STD = [ 0.20515700083327537 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliNet3(nn.Module):\n",
    "    def __init__(self, backbone_name):\n",
    "        super(BengaliNet3, self).__init__()\n",
    "        self.n_grapheme = 168\n",
    "        self.n_vowel = 11\n",
    "        self.n_consonant = 7\n",
    "        self.backbone_name = backbone_name\n",
    "        \n",
    "        self.num_classes = self.n_grapheme + self.n_vowel + self.n_consonant\n",
    "        \n",
    "        #self.conv0 = nn.Conv2d(1, 3, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        if self.backbone_name.startswith('efficient'):\n",
    "            self.backbone = EfficientNet.from_name(self.backbone_name, override_params={'num_classes': 1000})\n",
    "            self.fc = nn.Linear(self.backbone._fc.in_features, self.num_classes)\n",
    "        else:\n",
    "            self.backbone = pretrainedmodels.__dict__[self.backbone_name](num_classes=1000, pretrained=None)\n",
    "            self.fc = nn.Linear(self.backbone.last_linear.in_features, self.num_classes)\n",
    "        \n",
    "        #self.fix_input_layer()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "    def logits(self, x):\n",
    "        x = self.avg_pool(x)\n",
    "        #x = F.dropout2d(x, 0.2, self.training)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(x.size())\n",
    "        return self.fc(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, size=(224,224), mode='bilinear', align_corners=False)\n",
    "        for i in range(len(x)):\n",
    "            transforms.functional.normalize(x[i], mean=MEAN, std=STD, inplace=True)\n",
    "        x = torch.cat([x,x,x], 1)\n",
    "        #x = self.conv0(x)\n",
    "        #print(x.size())\n",
    "        if self.backbone_name.startswith('efficient'):\n",
    "            x = self.backbone.extract_features(x)\n",
    "        else:\n",
    "            x = self.backbone.features(x)\n",
    "        x = self.logits(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliResNet(nn.Module):\n",
    "    def __init__(self, backbone_name='se_resnext50_32x4d'):\n",
    "        super(BengaliResNet, self).__init__()\n",
    "        self.n_grapheme = 168\n",
    "        self.n_vowel = 11\n",
    "        self.n_consonant = 7\n",
    "        self.n_word = 1295\n",
    "        self.backbone_name = backbone_name\n",
    "        \n",
    "        self.num_classes = self.n_grapheme + self.n_vowel + self.n_consonant + self.n_word\n",
    "        \n",
    "        self.backbone = pretrainedmodels.__dict__[self.backbone_name](num_classes=1000, pretrained=None)\n",
    "        self.fc = nn.Linear(self.backbone.last_linear.in_features, self.num_classes)\n",
    "        \n",
    "        self.num_p2_features = self.backbone.layer2[-1].se_module.fc2.out_channels\n",
    "        self.num_p3_features = self.backbone.layer3[-1].se_module.fc2.out_channels\n",
    "        self.p2_head = nn.Conv2d(self.num_p2_features, self.num_p2_features * 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.p3_head = nn.Conv2d(self.num_p3_features, self.num_p3_features * 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.num_p2_features * 4)\n",
    "        self.bn3 = nn.BatchNorm2d(self.num_p3_features * 4)\n",
    "        self.act2 = Swish()\n",
    "        self.act3 = Swish()\n",
    "        \n",
    "        self.fc_aux1 = nn.Linear(self.num_p3_features * 4, self.num_classes)\n",
    "        self.fc_aux2 = nn.Linear(self.num_p2_features * 4, self.num_classes)\n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        for fc in [self.fc, self.fc_aux1, self.fc_aux2]:\n",
    "            nn.init.zeros_(fc.bias.data)\n",
    "\n",
    "        print('init model4')\n",
    "        \n",
    "    def features(self, x):\n",
    "        x = self.backbone.layer0(x); #print(x.size())\n",
    "        x = self.backbone.layer1(x); #print(x.size())\n",
    "        x = self.backbone.layer2(x); p2 = x; p2 = self.p2_head(p2); p2 = self.bn2(p2); p2 = self.act2(p2) #print(x.size())\n",
    "        x = self.backbone.layer3(x); p3 = x; p3 = self.p3_head(p3); p3 = self.bn3(p3); p3 = self.act3(p3) #print(x.size())\n",
    "        x = self.backbone.layer4(x); #print(x.size())\n",
    "        return x, p2, p3\n",
    "        \n",
    "    def logits(self, x, p2, p3):\n",
    "        x = self.avg_pool(x)\n",
    "        #x = F.dropout2d(x, 0.2, self.training)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        p2 = self.avg_pool(p2)\n",
    "        p2 = torch.flatten(p2, 1)\n",
    "        \n",
    "        p3 = self.avg_pool(p3)\n",
    "        p3 = torch.flatten(p3, 1)\n",
    "        return self.fc(x), self.fc_aux1(p3), self.fc_aux2(p2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, size=(224,224), mode='bilinear', align_corners=False)\n",
    "        for i in range(len(x)):\n",
    "            transforms.functional.normalize(x[i], mean=MEAN, std=STD, inplace=True)\n",
    "        x = torch.cat([x,x,x], 1)\n",
    "        #x = self.conv0(x)\n",
    "        #print(x.size())\n",
    "        x, p2, p3 = self.features(x)\n",
    "        x, logits_aux1, logits_aux2 = self.logits(x, p2, p3)\n",
    "\n",
    "        return x #, logits_aux1, logits_aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cfg = Namespace()\n",
    "cfg.MODEL_NAME = 'tf_efficientnet_b4'\n",
    "cfg.PRETRAINED = True\n",
    "cfg.IN_CHANNELS = 1\n",
    "cfg.POOL_TYPE = 'avg'\n",
    "cfg.CLS_HEAD = 'linear'\n",
    "cfg.MODEL_ACTIVATION = 'swish'\n",
    "cfg.DROP_CONNECT = 0.2\n",
    "cfg.DROPOUT= 0.\n",
    "cfg.NUM_WORD_CLASSES = 1295\n",
    "cfg.NUM_GRAPHEME_CLASSES = 168\n",
    "cfg.NUM_VOWEL_CLASSES = 11\n",
    "cfg.NUM_CONSONANT_CLASSES = 7\n",
    "cfg.CKP_NAME = 'model4_eb4_fold1.pth'\n",
    "'''\n",
    "class BengaliEfficientNet(nn.Module):\n",
    "    \"\"\"\n",
    "    EfficientNet B0-B8.\n",
    "    Args:\n",
    "        cfg (CfgNode): configs\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone_name, img_resize=None):\n",
    "        super(BengaliEfficientNet, self).__init__()\n",
    "        #model_name = cfg.MODEL_NAME\n",
    "        self.img_resize = img_resize\n",
    "        pretrained = False\n",
    "        input_channels = 1\n",
    "        pool_type = 'avg'\n",
    "        drop_connect_rate = 0.2\n",
    "        self.drop_rate = 0.\n",
    "        cls_head = 'linear'\n",
    "        num_total_classes = 168+11+7+1295\n",
    "\n",
    "        backbone = timm.create_model(\n",
    "            model_name=backbone_name,\n",
    "            pretrained=pretrained,\n",
    "            in_chans=input_channels,\n",
    "            drop_connect_rate=drop_connect_rate,\n",
    "        )\n",
    "        self.conv_stem = backbone.conv_stem\n",
    "        self.bn1 = backbone.bn1\n",
    "        self.act1 = backbone.act1\n",
    "        ### Original blocks ###\n",
    "        for i in range(len((backbone.blocks))):\n",
    "            setattr(self, \"block{}\".format(str(i)), backbone.blocks[i])\n",
    "        self.conv_head = backbone.conv_head\n",
    "        self.bn2 = backbone.bn2\n",
    "        self.act2 = backbone.act2\n",
    "        self.aux_block5 = backbone.blocks[5]\n",
    "        self.aux_num_features = self.block5[-1].bn3.num_features\n",
    "        self.aux_head4 = nn.Conv2d(self.aux_num_features, self.aux_num_features * 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(self.aux_num_features * 4)\n",
    "        self.act4 = Swish()\n",
    "        self.aux_head5 = nn.Conv2d(self.aux_num_features, self.aux_num_features * 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(self.aux_num_features * 4)\n",
    "        self.act5 = Swish()\n",
    "        self.global_pool = SelectAdaptivePool2d(pool_type=pool_type)\n",
    "        self.num_features = backbone.num_features * self.global_pool.feat_mult()\n",
    "        assert cls_head == 'linear'\n",
    "        if cls_head == \"linear\":\n",
    "            ### Baseline head ###\n",
    "            self.fc = nn.Linear(self.num_features, num_total_classes)            \n",
    "            self.aux_fc1 = nn.Linear(self.aux_num_features*4, num_total_classes)\n",
    "            self.aux_fc2 = nn.Linear(self.aux_num_features*4, num_total_classes)\n",
    "            \n",
    "            for fc in [self.fc, self.aux_fc1, self.aux_fc2]:\n",
    "                nn.init.zeros_(fc.bias.data)\n",
    "        #elif cls_head == \"norm_softmax\":\n",
    "            ### NormSoftmax ###\n",
    "            #self.grapheme_fc = NormSoftmax(self.num_features, num_grapheme_classes)\n",
    "            #self.consonant_fc = NormSoftmax(self.num_features, num_consonant_classes)\n",
    "            #self.vowel_fc = NormSoftmax(self.num_features, num_vowel_classes)\n",
    "        # Replace with Mish activation\n",
    "        #if cfg.MODEL_ACTIVATION == \"mish\":\n",
    "        #    convert_swish_to_mish(self)\n",
    "        del backbone\n",
    "\n",
    "    def _features(self, x):\n",
    "        x = self.conv_stem(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.block0(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x); b4 = x\n",
    "        x = self.block5(x); b4 = self.aux_block5(b4); b5 = x\n",
    "        x = self.block6(x)\n",
    "        x = self.conv_head(x); b4 = self.aux_head4(b4); b5 = self.aux_head5(b5)\n",
    "        x = self.bn2(x); b4 = self.bn4(b4); b5 = self.bn5(b5)\n",
    "        x = self.act2(x); b4 = self.act4(b4); b5 = self.act5(b5)\n",
    "        return b4, b5, x\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.img_resize is None:\n",
    "            x = x.clone()\n",
    "        else:\n",
    "            x = F.interpolate(x, size=(self.img_resize, self.img_resize), mode='bilinear', align_corners=False)\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            transforms.functional.normalize(x[i], mean=MEAN, std=STD, inplace=True)\n",
    "\n",
    "        # _, _, x = self._features(x)\n",
    "        b4, b5, x = self._features(x)\n",
    "        x = self.global_pool(x); b4 = self.global_pool(b4); b5 = self.global_pool(b5)\n",
    "        x = torch.flatten(x, 1); b4 = torch.flatten(b4, 1); b5 = torch.flatten(b5, 1)\n",
    "        if self.drop_rate > 0.:\n",
    "            x = F.dropout(x, p=self.drop_rate, training=self.training)\n",
    "        logits = self.fc(x)\n",
    "        \n",
    "        aux_logits1 = self.aux_fc1(b4)\n",
    "        aux_logits2 = self.aux_fc2(b5)\n",
    "        \n",
    "        return logits #, aux_logits1, aux_logits2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(backbone, model_file, model_type, img_resize=None):\n",
    "    print(backbone, model_file, model_type, img_resize)\n",
    "    if model_type == 'BengaliNet3':\n",
    "        model = BengaliNet3(backbone_name=backbone)\n",
    "    elif model_type == 'BengaliEfficientNet':\n",
    "        model = BengaliEfficientNet(backbone_name=backbone, img_resize=img_resize)\n",
    "    elif model_type == 'BengaliResNet':\n",
    "        model = BengaliResNet(backbone_name=backbone)\n",
    "    else:\n",
    "        raise ValueError('wrong model type')\n",
    "    #model_file = os.path.join(MODEL_DIR, ckp_name)\n",
    "\n",
    "    assert os.path.exists(model_file)\n",
    "    print('loading {}...'.format(model_file))\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models():\n",
    "    models = []\n",
    "    for backbone, model_file, model_type, img_resize in ckp_list:\n",
    "        model = create_model(backbone, model_file, model_type, img_resize).cuda()\n",
    "        #if ENABLE_APEX:\n",
    "        #    model = amp.initialize(model, None, opt_level=\"O1\",verbosity=0)\n",
    "        model.eval()\n",
    "        model = nn.DataParallel(model)\n",
    "        models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model4_eb4_fold1_256_512.pth\t       model4_eb4_fold2_380_cv998333.pth\r\n",
      "model4_eb4_fold1_256_512.pth_swa       model4_eb4_fold2_380_swa_cv998293.pth\r\n",
      "model4_eb4_fold1_380.pth\t       model4_eb4_fold2_380_swa_cv99858.pth\r\n",
      "model4_eb4_fold1_380.pth_swa\t       model4_eb4_fold2_380_swa_cv99863.pth\r\n",
      "model4_eb4_fold1_380_swa_cv998361.pth  model4_eb4_fold2_380_swa_cv998675.pth\r\n",
      "model4_eb4_fold1_380_swa_cv998658.pth  model4_eb4_fold2_380_swa_cv998693.pth\r\n",
      "model4_eb4_fold1_380_swa_cv998744.pth  model4_eb4_fold2_380_swa_cv99877.pth\r\n",
      "model4_eb4_fold1_cv9964.pth\t       model4_eb4_fold2_380_swa_cv998835.pth\r\n",
      "model4_eb4_fold1_cv9969.pth\t       model4_eb4_fold2_380_swa_cv998894.pth\r\n",
      "model4_eb4_fold1_cv9971.pth\t       model4_eb4_fold2_380_swa_cv998946.pth\r\n",
      "model4_eb4_fold1_cv9976.pth\t       model4_eb4_fold2_cv9976.pth\r\n",
      "model4_eb4_fold1_cv997705.pth\t       model4_eb4_fold3_380_swa_cv998499.pth\r\n",
      "model4_eb4_fold1_cv9977.pth\t       model4_eb4_fold3_380_swa_cv998671.pth\r\n",
      "model4_eb4_fold1_cv997864.pth\t       model4_eb4_fold3_cv998185_swa.pth\r\n",
      "model4_eb4_fold1_cv998144.pth\t       model4_eb4_fold3_cv998307_swa.pth\r\n",
      "model4_eb4_fold2_380_cv998268.pth\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./model4-ckps/tf_efficientnet_b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model4_se_resnext50_fold0_224_cv9976.pth\r\n",
      "model4_se_resnext50_fold0_224_cv9977.pth\r\n",
      "model4_se_resnext50_fold0_224_cv9978.pth\r\n",
      "model4_se_resnext50_fold0_224_cv998106.pth\r\n",
      "model4_se_resnext50_fold0_224_swa_cv998273.pth\r\n",
      "model4_se_resnext50_fold4_224_cv997979.pth\r\n",
      "model4_se_resnext50_fold4_224_swa_cv998285.pth\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./model4-ckps/se_resnext50_32x4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckp_list = [\n",
    "    ('se_resnext50_32x4d', './model4-ckps/se_resnext50_32x4d/model4_se_resnext50_fold0_224_swa_cv998273.pth', 'BengaliResNet', None), # lb\n",
    "    ('tf_efficientnet_b4', './model4-ckps/tf_efficientnet_b4/model4_eb4_fold1_380_swa_cv998658.pth', 'BengaliEfficientNet', 380),  # lb9893\n",
    "    ('tf_efficientnet_b4', './model4-ckps/tf_efficientnet_b4/model4_eb4_fold2_380_swa_cv998894.pth', 'BengaliEfficientNet', 380),  # lb\n",
    "    ('tf_efficientnet_b4', './model4-ckps/tf_efficientnet_b4/model4_eb4_fold3_cv998307_swa.pth', 'BengaliEfficientNet', None),  # lb9892\n",
    "    ('se_resnext50_32x4d', './model4-ckps/se_resnext50_32x4d/model4_se_resnext50_fold4_224_cv997979.pth', 'BengaliResNet', None) # lb 9882\n",
    "    ]\n",
    "model_weights = [0.15, 0.2, 0.3, 0.2, 0.15]\n",
    "#model_weights = [0.7, 0.3]\n",
    "#model_weights = [1.]\n",
    "#model_weights = [0.2, 0.15, 0.25, 0.25, 0.15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "\n",
    "\n",
    "def calc_metrics(preds0, preds1, preds2, y):\n",
    "    assert len(y) == len(preds0) == len(preds1) == len(preds2)\n",
    "\n",
    "    recall_grapheme = sklearn.metrics.recall_score(y[:, 0], preds0, average='macro')\n",
    "    recall_vowel = sklearn.metrics.recall_score(y[:, 1], preds1, average='macro')\n",
    "    recall_consonant = sklearn.metrics.recall_score(y[:, 2], preds2, average='macro')\n",
    "    \n",
    "    scores = [recall_grapheme, recall_vowel, recall_consonant]\n",
    "    final_recall_score = np.average(scores, weights=[2, 1, 1])\n",
    "    \n",
    "    old_recall_grapheme = sklearn.metrics.recall_score(preds0, y[:, 0], average='macro')\n",
    "    old_recall_vowel = sklearn.metrics.recall_score(preds1, y[:, 1], average='macro')\n",
    "    old_recall_consonant = sklearn.metrics.recall_score(preds2, y[:, 2], average='macro')\n",
    "    \n",
    "    old_scores = [old_recall_grapheme, old_recall_vowel, old_recall_consonant]\n",
    "    old_final_recall_score = np.average(old_scores, weights=[2, 1, 1])\n",
    "\n",
    "    \n",
    "    metrics = {}\n",
    "    metrics['recall'] = round(final_recall_score, 6)\n",
    "    metrics['recall_grapheme'] = round(recall_grapheme, 6)\n",
    "    metrics['recall_vowel'] = round(recall_vowel, 6)\n",
    "    metrics['recall_consonant'] = round(recall_consonant, 6)\n",
    "    \n",
    "    metrics['acc_grapheme'] = round((preds0 == y[:, 0]).sum() / len(y), 6)\n",
    "    metrics['acc_vowel'] = round((preds1 == y[:, 1]).sum() / len(y), 6)\n",
    "    metrics['acc_consonant'] = round((preds2 == y[:, 2]).sum() / len(y), 6)\n",
    "    \n",
    "    metrics['old_recall'] = round(old_final_recall_score, 6)\n",
    "    metrics['old_recall_grapheme'] = round(old_recall_grapheme, 6)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, val_loader, ifold):\n",
    "    print('fold:', ifold)\n",
    "    preds0, preds1,preds2 = [], [], []\n",
    "    y_true, soft_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            y_true.append(y)\n",
    "            \n",
    "            x = x.cuda()\n",
    "            outputs = model(x)\n",
    "            soft_labels.append(outputs[:, :186].cpu())\n",
    "            \n",
    "            outputs = torch.split(outputs[:, :186], [168, 11, 7], dim=1)\n",
    "            #print('image:', x)\n",
    "            #print(outputs[0][:, :10])\n",
    "            \n",
    "            preds0.append(torch.max(outputs[0], dim=1)[1])\n",
    "            preds1.append(torch.max(outputs[1], dim=1)[1])\n",
    "            preds2.append(torch.max(outputs[2], dim=1)[1])\n",
    "            \n",
    "    preds0 = torch.cat(preds0, 0).cpu().numpy()\n",
    "    preds1 = torch.cat(preds1, 0).cpu().numpy()\n",
    "    preds2 = torch.cat(preds2, 0).cpu().numpy()\n",
    "    \n",
    "    y_true = torch.cat(y_true, 0).numpy()\n",
    "    soft_labels = torch.cat(soft_labels, 0).numpy()\n",
    "    val_df = val_loader.df\n",
    "    \n",
    "    metrics = calc_metrics(preds0, preds1, preds2, y_true)\n",
    "    print(metrics)\n",
    "    \n",
    "    print(soft_labels.shape, val_df.shape)\n",
    "    val_df.to_csv(f'./gary/fold{ifold}_true_labels.csv', index=False)\n",
    "    np.save(f'./gary/fold{ifold}_logits.npy', soft_labels)\n",
    "    \n",
    "    \n",
    "    return preds0, preds1, preds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se_resnext50_32x4d ./model4-ckps/se_resnext50_32x4d/model4_se_resnext50_fold0_224_swa_cv998273.pth BengaliResNet None\n",
      "init model4\n",
      "loading ./model4-ckps/se_resnext50_32x4d/model4_se_resnext50_fold0_224_swa_cv998273.pth...\n",
      "tf_efficientnet_b4 ./model4-ckps/tf_efficientnet_b4/model4_eb4_fold1_380_swa_cv998658.pth BengaliEfficientNet 380\n",
      "loading ./model4-ckps/tf_efficientnet_b4/model4_eb4_fold1_380_swa_cv998658.pth...\n",
      "tf_efficientnet_b4 ./model4-ckps/tf_efficientnet_b4/model4_eb4_fold2_380_swa_cv998894.pth BengaliEfficientNet 380\n",
      "loading ./model4-ckps/tf_efficientnet_b4/model4_eb4_fold2_380_swa_cv998894.pth...\n",
      "tf_efficientnet_b4 ./model4-ckps/tf_efficientnet_b4/model4_eb4_fold3_cv998307_swa.pth BengaliEfficientNet None\n",
      "loading ./model4-ckps/tf_efficientnet_b4/model4_eb4_fold3_cv998307_swa.pth...\n",
      "se_resnext50_32x4d ./model4-ckps/se_resnext50_32x4d/model4_se_resnext50_fold4_224_cv997979.pth BengaliResNet None\n",
      "init model4\n",
      "loading ./model4-ckps/se_resnext50_32x4d/model4_se_resnext50_fold4_224_cv997979.pth...\n"
     ]
    }
   ],
   "source": [
    "models = create_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200840, 6)\n",
      "(200840, 32332)\n",
      "(160596, 6) (40244, 6)\n"
     ]
    }
   ],
   "source": [
    "_, val_loader0 = get_train_val_loaders(batch_size=256, val_batch_size=512, ifold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "      <th>word_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>45330</td>\n",
       "      <td>Train_45330</td>\n",
       "      <td>62</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>ণ্ডে</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177100</td>\n",
       "      <td>Train_177100</td>\n",
       "      <td>113</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>র্ভূ</td>\n",
       "      <td>964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194002</td>\n",
       "      <td>Train_194002</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ক্টা</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177170</td>\n",
       "      <td>Train_177170</td>\n",
       "      <td>72</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>দ্যে</td>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41279</td>\n",
       "      <td>Train_41279</td>\n",
       "      <td>124</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>ল্যা</td>\n",
       "      <td>1043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image_id  grapheme_root  vowel_diacritic  consonant_diacritic  \\\n",
       "45330    Train_45330             62                7                    0   \n",
       "177100  Train_177100            113                5                    2   \n",
       "194002  Train_194002             15                1                    0   \n",
       "177170  Train_177170             72                7                    4   \n",
       "41279    Train_41279            124                1                    4   \n",
       "\n",
       "       grapheme  word_label  \n",
       "45330      ণ্ডে         364  \n",
       "177100     র্ভূ         964  \n",
       "194002     ক্টা          39  \n",
       "177170     দ্যে         487  \n",
       "41279      ল্যা        1043  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loader0.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'recall': 0.997514, 'recall_grapheme': 0.997074, 'recall_vowel': 0.998411, 'recall_consonant': 0.997497, 'acc_grapheme': 0.997391, 'acc_vowel': 0.998907, 'acc_consonant': 0.998782, 'old_recall': 0.998273, 'old_recall_grapheme': 0.99803}\n",
      "(40244, 186) (40244, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 62, 113,  15, ..., 113, 107,  69]),\n",
       " array([7, 5, 1, ..., 4, 0, 1]),\n",
       " array([0, 2, 0, ..., 2, 4, 0]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(models[0], val_loader0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200840, 6)\n",
      "(200840, 32332)\n",
      "(160716, 6) (40124, 6)\n",
      "fold: 3\n",
      "{'recall': 0.997782, 'recall_grapheme': 0.996724, 'recall_vowel': 0.998748, 'recall_consonant': 0.998934, 'acc_grapheme': 0.997657, 'acc_vowel': 0.998804, 'acc_consonant': 0.999302, 'old_recall': 0.998307, 'old_recall_grapheme': 0.99744}\n",
      "(40124, 186) (40124, 6)\n",
      "(200840, 6)\n",
      "(200840, 32332)\n",
      "(160735, 6) (40105, 6)\n",
      "fold: 4\n",
      "{'recall': 0.997096, 'recall_grapheme': 0.996461, 'recall_vowel': 0.998323, 'recall_consonant': 0.997139, 'acc_grapheme': 0.996883, 'acc_vowel': 0.998678, 'acc_consonant': 0.998703, 'old_recall': 0.997979, 'old_recall_grapheme': 0.997128}\n",
      "(40105, 186) (40105, 6)\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 5):\n",
    "    _, val_loader = get_train_val_loaders(batch_size=256, val_batch_size=512, ifold=i)\n",
    "    predict(models[i], val_loader, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError('/mnt/chicm/anaconda3/lib/python3.7/site-packages/amp_C.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZN3c1011CPUTensorIdEv')\n"
     ]
    }
   ],
   "source": [
    "model1 = BengaliEfficientNet(backbone_name='tf_efficientnet_b4', img_resize=380).cuda()\n",
    "model1 = amp.initialize(model1, opt_level=\"O1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.load_state_dict(torch.load('./model4-ckps/tf_efficientnet_b4/model4_eb4_fold1_380_swa_cv998658.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
