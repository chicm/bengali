{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time, gc\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pretrainedmodels\n",
    "from argparse import Namespace\n",
    "from sklearn.utils import shuffle\n",
    "from apex import amp\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from cvcore.data.auto_augment import RandAugment\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_map.csv\t\t       train.csv\r\n",
      "sample_submission.csv\t       train.csv.zip\r\n",
      "test.csv\t\t       train_image_data_0.parquet\r\n",
      "test_image_data_0.parquet      train_image_data_0.parquet.zip\r\n",
      "test_image_data_0.parquet.zip  train_image_data_1.parquet\r\n",
      "test_image_data_1.parquet      train_image_data_1.parquet.zip\r\n",
      "test_image_data_1.parquet.zip  train_image_data_2.parquet\r\n",
      "test_image_data_2.parquet      train_image_data_2.parquet.zip\r\n",
      "test_image_data_2.parquet.zip  train_image_data_3.parquet\r\n",
      "test_image_data_3.parquet      train_image_data_3.parquet.zip\r\n",
      "test_image_data_3.parquet.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/chec/data/bengali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /home/chec/data/bengali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/chec/data/bengali'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "class_map_df = pd.read_csv(f'{DATA_DIR}/class_map.csv')\n",
    "sample_sub_df = pd.read_csv(f'{DATA_DIR}/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>ক্ট্রো</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_1</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>হ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>খ্রী</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_3</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>র্টি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_4</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>থ্রো</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  grapheme_root  vowel_diacritic  consonant_diacritic grapheme\n",
       "0  Train_0             15                9                    5   ক্ট্রো\n",
       "1  Train_1            159                0                    0        হ\n",
       "2  Train_2             22                3                    5     খ্রী\n",
       "3  Train_3             53                2                    2     র্টি\n",
       "4  Train_4             71                9                    5     থ্রো"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 137\n",
    "WIDTH = 236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as albu\n",
    "'''\n",
    "def get_train_augs(p=1.):\n",
    "    return albu.Compose([\n",
    "        #albu.HorizontalFlip(.5),\n",
    "        albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.5 ),\n",
    "        albu.Blur(blur_limit=3, p=0.3),\n",
    "        albu.OpticalDistortion(p=0.3),\n",
    "        albu.GaussNoise(p=0.3)\n",
    "        #albu.GridDistortion(p=.33),\n",
    "        #albu.HueSaturationValue(p=.33) # not for grey scale\n",
    "    ], p=p)\n",
    "'''\n",
    "def get_train_augs():\n",
    "    return RandAugment(n=2, m=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class BengaliDataset(Dataset):\n",
    "    def __init__(self, df, img_df, train_mode=True, test_mode=False):\n",
    "        self.df = df\n",
    "        self.img_df = img_df\n",
    "        self.train_mode = train_mode\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = self.get_img(row.image_id)\n",
    "        #print(img.shape)\n",
    "        if self.train_mode:\n",
    "            augs = get_train_augs()\n",
    "            #img = augs(image=img)['image']\n",
    "            img = np.asarray(augs(Image.fromarray(img)))\n",
    "        \n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "        #print('###', img.shape)\n",
    "        img = np.concatenate([img, img, img], 2)\n",
    "        #print('>>>', img.shape)\n",
    "        \n",
    "        # taken from https://www.kaggle.com/iafoss/image-preprocessing-128x128\n",
    "        MEAN = [ 0.06922848809290576,  0.06922848809290576,  0.06922848809290576]\n",
    "        STD = [ 0.20515700083327537,  0.20515700083327537,  0.20515700083327537]\n",
    "        \n",
    "        img = transforms.functional.to_tensor(img)\n",
    "        img = transforms.functional.normalize(img, mean=MEAN, std=STD)\n",
    "        \n",
    "        if self.test_mode:\n",
    "            return img\n",
    "        else:\n",
    "            return img, torch.tensor([row.grapheme_root, row.vowel_diacritic, row.consonant_diacritic])\n",
    "\n",
    "    def get_img(self, img_id):\n",
    "        return 255 - self.img_df.loc[img_id].values.reshape(HEIGHT, WIDTH).astype(np.uint8)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "def get_train_val_loaders(batch_size=4, val_batch_size=4, ifold=0, dev_mode=False):\n",
    "    train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "    train_df = shuffle(train_df, random_state=1234)\n",
    "    print(train_df.shape)\n",
    "\n",
    "    if dev_mode:\n",
    "        img_df = pd.read_parquet(f'{DATA_DIR}/train_image_data_0.parquet').set_index('image_id')\n",
    "        train_df = train_df.iloc[:1000]\n",
    "    else:\n",
    "        img_dfs = [pd.read_parquet(f'{DATA_DIR}/train_image_data_{i}.parquet') for i in range(4)]\n",
    "        img_df = pd.concat(img_dfs, axis=0).set_index('image_id')\n",
    "    print(img_df.shape)\n",
    "    #split_index = int(len(train_df) * 0.9)\n",
    "    \n",
    "    #train = train_df.iloc[:split_index]\n",
    "    #val = train_df.iloc[split_index:]\n",
    "    \n",
    "    kf = StratifiedKFold(5, random_state=1234, shuffle=True)\n",
    "    for i, (train_idx, val_idx) in enumerate(kf.split(train_df, train_df['grapheme_root'].values)):\n",
    "        if i == ifold:\n",
    "            #print(val_idx)\n",
    "            train = train_df.iloc[train_idx]\n",
    "            val = train_df.iloc[val_idx]\n",
    "            break\n",
    "    assert i == ifold\n",
    "    print(train.shape, val.shape)\n",
    "    \n",
    "    train_ds = BengaliDataset(train, img_df, True, False)\n",
    "    val_ds = BengaliDataset(val, img_df, False, False)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "    train_loader.num = len(train_ds)\n",
    "\n",
    "    val_loader = DataLoader(val_ds, batch_size=val_batch_size, shuffle=False, num_workers=8, drop_last=False)\n",
    "    val_loader.num = len(val_ds)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader, val_loader = get_train_val_loaders(dev_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fbresnet152', 'bninception', 'resnext101_32x4d', 'resnext101_64x4d', 'inceptionv4', 'inceptionresnetv2', 'alexnet', 'densenet121', 'densenet169', 'densenet201', 'densenet161', 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'inceptionv3', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19_bn', 'vgg19', 'nasnetamobile', 'nasnetalarge', 'dpn68', 'dpn68b', 'dpn92', 'dpn98', 'dpn131', 'dpn107', 'xception', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152', 'se_resnext50_32x4d', 'se_resnext101_32x4d', 'cafferesnet101', 'pnasnet5large', 'polynet']\n"
     ]
    }
   ],
   "source": [
    "print(pretrainedmodels.model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'resnet50' # could be fbresnet152 or inceptionresnetv2\n",
    "#model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet').cuda()\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained=False).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.features(torch.randn((2, 3, 137, 236)).cuda()).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.last_linear.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliNet(nn.Module):\n",
    "    def __init__(self, backbone_name):\n",
    "        super(BengaliNet, self).__init__()\n",
    "        self.n_grapheme = 168\n",
    "        self.n_vowel = 11\n",
    "        self.n_consonant = 7\n",
    "        self.backbone_name = backbone_name\n",
    "        \n",
    "        self.num_classes = self.n_grapheme + self.n_vowel + self.n_consonant\n",
    "        \n",
    "        #self.conv0 = nn.Conv2d(1, 3, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        if self.backbone_name.startswith('efficient'):\n",
    "            self.backbone = EfficientNet.from_pretrained(self.backbone_name)\n",
    "            self.fc = nn.Linear(self.backbone._fc.in_features, self.num_classes)\n",
    "        else:\n",
    "            self.backbone = pretrainedmodels.__dict__[self.backbone_name](num_classes=1000, pretrained='imagenet')\n",
    "            self.fc = nn.Linear(self.backbone.last_linear.in_features, self.num_classes)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        #self.fix_input_layer()\n",
    "        \n",
    "    def fix_input_layer(self):\n",
    "        if self.backbone_name in ['se_resnext50_32x4d', 'se_resnext101_32x4d', 'se_resnet50', 'senet154', 'se_resnet152', 'nasnetmobile', 'mobilenet', 'nasnetalarge']:\n",
    "            #self.backbone = eval(backbone_name)()\n",
    "            #print(self.backbone.layer0.conv1)\n",
    "            w = self.backbone.layer0.conv1.weight.data\n",
    "            self.backbone.layer0.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "            #self.backbone.layer0.conv1.weight = torch.nn.Parameter(torch.cat((w, w[:, 2, :, :].unsqueeze(1)), dim=1))\n",
    "            self.backbone.layer0.conv1.weight = torch.nn.Parameter(w[:, 0, :, :].unsqueeze(1))\n",
    "        \n",
    "    def logits(self, x):\n",
    "        x = self.avg_pool(x)\n",
    "        #x = F.dropout2d(x, 0.2, self.training)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x = self.conv0(x)\n",
    "        #print(x.size())\n",
    "        if self.backbone_name.startswith('efficient'):\n",
    "            x = self.backbone.extract_features(x)\n",
    "        else:\n",
    "            x = self.backbone.features(x)\n",
    "        x = self.logits(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './models'\n",
    "def create_model(args):\n",
    "    model = BengaliNet(backbone_name=args.backbone)\n",
    "    model_file = os.path.join(MODEL_DIR, args.backbone, args.ckp_name)\n",
    "\n",
    "    parent_dir = os.path.dirname(model_file)\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "\n",
    "    print('model file: {}, exist: {}'.format(model_file, os.path.exists(model_file)))\n",
    "\n",
    "    if args.predict and (not os.path.exists(model_file)):\n",
    "        raise AttributeError('model file does not exist: {}'.format(model_file))\n",
    "\n",
    "    if os.path.exists(model_file):\n",
    "        print('loading {}...'.format(model_file))\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "    \n",
    "    return model, model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bnet = BengaliNet('se_resnext50_32x4d').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bnet(torch.randn((2, 1, 137, 236)).cuda()).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.111111"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(1/9, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "\n",
    "\n",
    "def macro_recall(pred_y, y, n_grapheme=168, n_vowel=11, n_consonant=7):\n",
    "    pred_y = torch.split(pred_y, [n_grapheme, n_vowel, n_consonant], dim=1)\n",
    "    pred_labels = [torch.argmax(py, dim=1).cpu().numpy() for py in pred_y]\n",
    "\n",
    "    #y = y.cpu().numpy()\n",
    "    # pred_y = [p.cpu().numpy() for p in pred_y]\n",
    "\n",
    "    recall_grapheme = sklearn.metrics.recall_score(pred_labels[0], y_grapheme, average='macro')\n",
    "    recall_vowel = sklearn.metrics.recall_score(pred_labels[1], y_vowel, average='macro')\n",
    "    recall_consonant = sklearn.metrics.recall_score(pred_labels[2], y_consonant, average='macro')\n",
    "    scores = [recall_grapheme, recall_vowel, recall_consonant]\n",
    "    final_score = np.average(scores, weights=[2, 1, 1])\n",
    "    # print(f'recall: grapheme {recall_grapheme}, vowel {recall_vowel}, consonant {recall_consonant}, '\n",
    "    #       f'total {final_score}, y {y.shape}')\n",
    "    return final_score\n",
    "\n",
    "def calc_metrics(preds0, preds1, preds2, y):\n",
    "    assert len(y) == len(preds0) == len(preds1) == len(preds2)\n",
    "\n",
    "    recall_grapheme = sklearn.metrics.recall_score(preds0, y[:, 0], average='macro')\n",
    "    recall_vowel = sklearn.metrics.recall_score(preds1, y[:, 1], average='macro')\n",
    "    recall_consonant = sklearn.metrics.recall_score(preds2, y[:, 2], average='macro')\n",
    "    scores = [recall_grapheme, recall_vowel, recall_consonant]\n",
    "    final_recall_score = np.average(scores, weights=[2, 1, 1])\n",
    "    \n",
    "    metrics = {}\n",
    "    metrics['recall'] = round(final_recall_score, 6)\n",
    "    metrics['recall_grapheme'] = round(recall_grapheme, 6)\n",
    "    metrics['recall_vowel'] = round(recall_vowel, 6)\n",
    "    metrics['recall_consonant'] = round(recall_consonant, 6)\n",
    "    \n",
    "    metrics['acc_grapheme'] = round((preds0 == y[:, 0]).sum() / len(y), 6)\n",
    "    metrics['acc_vowel'] = round((preds1 == y[:, 1]).sum() / len(y), 6)\n",
    "    metrics['acc_consonant'] = round((preds2 == y[:, 2]).sum() / len(y), 6)\n",
    "    \n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(outputs, y_true):\n",
    "    # outputs: (N, 182)\n",
    "    # y_true: (N, 3)\n",
    "    \n",
    "    outputs = torch.split(outputs, [168, 11, 7], dim=1)\n",
    "    loss0 = F.cross_entropy(outputs[0], y_true[:, 0], reduction='mean')\n",
    "    loss1 = F.cross_entropy(outputs[1], y_true[:, 1], reduction='mean')\n",
    "    loss2 = F.cross_entropy(outputs[2], y_true[:, 2], reduction='mean')\n",
    "    \n",
    "    return loss0 + loss1 + loss2 #, loss0.item(), loss1.item(), loss2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    loss0, loss1, loss2 = 0., 0., 0.\n",
    "    preds0, preds1,preds2 = [], [], []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            y_true.append(y)\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            outputs = model(x)\n",
    "            outputs = torch.split(outputs, [168, 11, 7], dim=1)\n",
    "            \n",
    "            preds0.append(torch.max(outputs[0], dim=1)[1])\n",
    "            preds1.append(torch.max(outputs[1], dim=1)[1])\n",
    "            preds2.append(torch.max(outputs[2], dim=1)[1])\n",
    "            loss0 += F.cross_entropy(outputs[0], y[:, 0], reduction='sum').item()\n",
    "            loss1 += F.cross_entropy(outputs[1], y[:, 1], reduction='sum').item()\n",
    "            loss2 += F.cross_entropy(outputs[2], y[:, 2], reduction='sum').item()\n",
    "            \n",
    "            # for debug\n",
    "            #metrics = {}\n",
    "            #metrics['loss_grapheme'] =  F.cross_entropy(outputs[0], y[:, 0], reduction='mean').item()\n",
    "            #metrics['loss_vowel'] =  F.cross_entropy(outputs[1], y[:, 1], reduction='mean').item()\n",
    "            #metrics['loss_consonant'] =  F.cross_entropy(outputs[2], y[:, 2], reduction='mean').item()\n",
    "            #return metrics\n",
    "    \n",
    "    preds0 = torch.cat(preds0, 0).cpu().numpy()\n",
    "    preds1 = torch.cat(preds1, 0).cpu().numpy()\n",
    "    preds2 = torch.cat(preds2, 0).cpu().numpy()\n",
    "    y_true = torch.cat(y_true, 0).numpy()\n",
    "    \n",
    "    #print('y_true:', y_true.shape)\n",
    "    #print('preds0:', preds0.shape)\n",
    "    \n",
    "    metrics = calc_metrics(preds0, preds1, preds2, y_true)\n",
    "    metrics['loss_grapheme'] = round(loss0 / val_loader.num, 6)\n",
    "    metrics['loss_vowel'] = round(loss1 / val_loader.num, 6)\n",
    "    metrics['loss_consonant'] = round(loss2 / val_loader.num, 6)\n",
    "    \n",
    "    return metrics\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lrs(optimizer):\n",
    "    lrs = []\n",
    "    for pgs in optimizer.state_dict()['param_groups']:\n",
    "        lrs.append(pgs['lr'])\n",
    "    lrs = ['{:.6f}'.format(x) for x in lrs]\n",
    "    return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_file):\n",
    "    parent_dir = os.path.dirname(model_file)\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        torch.save(model.module.state_dict(), model_file)\n",
    "    else:\n",
    "        torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(data, targets, alpha=1):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets = targets[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    data = data * lam + shuffled_data * (1 - lam)\n",
    "    targets = (targets, shuffled_targets, lam)\n",
    "\n",
    "    return data, targets\n",
    "\n",
    "\n",
    "def mixup_criterion(outputs, targets):\n",
    "    targets1, targets2, lam = targets\n",
    "    #criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    return lam * criterion(outputs, targets1) + (1 - lam) * criterion(outputs, targets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9381385494239725"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from over9000.over9000 import Over9000\n",
    "from over9000.radam import RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvcore.solver import WarmupCyclicalLR\n",
    "def make_optimizer(model, base_lr=4e-4, weight_decay=0., weight_decay_bias=0., epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    Create optimizer with per-layer learning rate and weight decay.\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    for key, value in model.named_parameters():\n",
    "        if not value.requires_grad:\n",
    "            continue\n",
    "        lr = base_lr\n",
    "        params += [{\"params\": [value], \"lr\": lr, \"weight_decay\": weight_decay_bias if 'bias' in key else weight_decay}]\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(params, lr, eps=epsilon)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model):\n",
    "    optimizer = make_optimizer(model)\n",
    "    scheduler = WarmupCyclicalLR(\n",
    "        \"cos\", args.base_lr, args.num_epochs, iters_per_epoch=len(train_loader), warmup_epochs=args.warmup_epochs)\n",
    "    \n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    for cycle in range(1, args.num_cycles+1):\n",
    "        print('CYCLE:', cycle)\n",
    "        train_cycle(args, model, optimizer, scheduler)\n",
    "        scheduler = WarmupCyclicalLR(\n",
    "            \"cos\", args.base_lr, args.num_epochs, iters_per_epoch=len(train_loader), warmup_epochs=args.warmup_epochs)\n",
    "\n",
    "def train_cycle(args, model, optimizer, lr_scheduler):\n",
    "    best_metrics = 0.\n",
    "    best_key = 'recall'\n",
    "    \n",
    "    val_metrics = validate(model, val_loader)\n",
    "    print(val_metrics)\n",
    "    best_metrics = val_metrics[best_key]\n",
    "    \n",
    "    model.train()\n",
    "    train_iter = 0\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        train_loss = 0\n",
    "\n",
    "        bg = time.time()\n",
    "        for batch_idx, (img, targets) in enumerate(train_loader):\n",
    "            train_iter += 1\n",
    "            img, targets  = img.cuda(), targets.cuda()\n",
    "            batch_size = img.size(0)\n",
    "            r = np.random.rand()\n",
    "\n",
    "            if True:\n",
    "                # generate mixed sample\n",
    "                lam = np.random.beta(args.beta, args.beta)\n",
    "                rand_index = torch.randperm(img.size()[0]).cuda()\n",
    "                target_a = targets\n",
    "                target_b = targets[rand_index]\n",
    "                bbx1, bby1, bbx2, bby2 = rand_bbox(img.size(), lam)\n",
    "                img[:, :, bbx1:bbx2, bby1:bby2] = img[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "                # adjust lambda to exactly match pixel ratio\n",
    "                lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (img.size()[-1] * img.size()[-2]))\n",
    "                # compute output\n",
    "                outputs = model(img)\n",
    "                loss = criterion(outputs, target_a) * lam + criterion(outputs, target_b) * (1. - lam)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "            #loss.backward()\n",
    "            lr_scheduler(optimizer, batch_idx, epoch)\n",
    "            optimizer.step()            \n",
    "            \n",
    "            current_lr = get_lrs(optimizer)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            print('\\r {:4d} | {:.6f} | {:06d}/{} | {:.4f} | {:.4f} |'.format(\n",
    "                epoch, float(current_lr[0]), batch_size*(batch_idx+1), train_loader.num, \n",
    "                loss.item(), train_loss/(batch_idx+1)), end='')\n",
    "\n",
    "        if True:#train_iter > 0 and train_iter % args.iter_val == 0:\n",
    "            val_metrics = validate(model, val_loader)\n",
    "            print('\\nval:', val_metrics)\n",
    "                \n",
    "            if val_metrics[best_key] > best_metrics:\n",
    "                best_metrics = val_metrics[best_key]\n",
    "                save_model(model, model_file)\n",
    "                print('###>>>>> saved')\n",
    "                \n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.backbone = 'efficientnet-b4'\n",
    "args.ckp_name = 'model3_efficientnet-b4_fold1_randaugment.pth'\n",
    "args.predict =False\n",
    "\n",
    "args.base_lr = 4e-4\n",
    "args.num_epochs = 150\n",
    "args.warmup_epochs = 10\n",
    "args.num_cycles = 100\n",
    "args.batch_size = 768\n",
    "args.val_batch_size = 1024\n",
    "args.st_epochs = 10\n",
    "\n",
    "args.beta = 1.5\n",
    "args.cutmix_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200840, 5)\n",
      "(200840, 32332)\n",
      "(160635, 5) (40205, 5)\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_train_val_loaders(batch_size=args.batch_size, val_batch_size=args.val_batch_size, ifold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n",
      "model file: ./models/efficientnet-b4/model3_efficientnet-b4_fold1_randaugment.pth, exist: False\n"
     ]
    }
   ],
   "source": [
    "model, model_file = create_model(args)\n",
    "#if torch.cuda.device_count() > 1:\n",
    "#    model = nn.DataParallel(model)\n",
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CYCLE: 1\n",
      "{'recall': 0.061073, 'recall_grapheme': 0.004175, 'recall_vowel': 0.087298, 'recall_consonant': 0.148645, 'acc_grapheme': 0.005497, 'acc_vowel': 0.173337, 'acc_consonant': 0.127049, 'loss_grapheme': 5.142862, 'loss_vowel': 2.358716, 'loss_consonant': 1.958236}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chec/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 0.000040 | 160512/160635 | 9.0782 | 9.3719 |\n",
      "val: {'recall': 0.081958, 'recall_grapheme': 0.005502, 'recall_vowel': 0.123567, 'recall_consonant': 0.193261, 'acc_grapheme': 0.008855, 'acc_vowel': 0.159781, 'acc_consonant': 0.328317, 'loss_grapheme': 5.112659, 'loss_vowel': 2.339818, 'loss_consonant': 1.852748}\n",
      "###>>>>> saved\n",
      "    1 | 0.000080 | 160512/160635 | 8.1126 | 8.5154 |\n",
      "val: {'recall': 0.082445, 'recall_grapheme': 0.003051, 'recall_vowel': 0.234511, 'recall_consonant': 0.089168, 'acc_grapheme': 0.036911, 'acc_vowel': 0.324636, 'acc_consonant': 0.624176, 'loss_grapheme': 4.724176, 'loss_vowel': 1.94818, 'loss_consonant': 1.196133}\n",
      "###>>>>> saved\n",
      "    2 | 0.000120 | 160512/160635 | 7.7359 | 7.8308 |\n",
      "val: {'recall': 0.198782, 'recall_grapheme': 0.010327, 'recall_vowel': 0.437957, 'recall_consonant': 0.336515, 'acc_grapheme': 0.077453, 'acc_vowel': 0.660341, 'acc_consonant': 0.662256, 'loss_grapheme': 4.501469, 'loss_vowel': 1.289796, 'loss_consonant': 0.948831}\n",
      "###>>>>> saved\n",
      "    3 | 0.000160 | 160512/160635 | 7.4726 | 7.3844 |\n",
      "val: {'recall': 0.348974, 'recall_grapheme': 0.055719, 'recall_vowel': 0.656801, 'recall_consonant': 0.627656, 'acc_grapheme': 0.142694, 'acc_vowel': 0.779082, 'acc_consonant': 0.789976, 'loss_grapheme': 3.94382, 'loss_vowel': 0.928149, 'loss_consonant': 0.675413}\n",
      "###>>>>> saved\n",
      "    4 | 0.000199 | 160512/160635 | 6.5868 | 6.9192 |\n",
      "val: {'recall': 0.499566, 'recall_grapheme': 0.252653, 'recall_vowel': 0.845271, 'recall_consonant': 0.647687, 'acc_grapheme': 0.353787, 'acc_vowel': 0.839771, 'acc_consonant': 0.863077, 'loss_grapheme': 3.011795, 'loss_vowel': 0.775099, 'loss_consonant': 0.514427}\n",
      "###>>>>> saved\n",
      "    5 | 0.000239 | 160512/160635 | 6.8265 | 6.4508 |\n",
      "val: {'recall': 0.712831, 'recall_grapheme': 0.504356, 'recall_vowel': 0.899655, 'recall_consonant': 0.942957, 'acc_grapheme': 0.589653, 'acc_vowel': 0.890934, 'acc_consonant': 0.896605, 'loss_grapheme': 2.129826, 'loss_vowel': 0.679969, 'loss_consonant': 0.454036}\n",
      "###>>>>> saved\n",
      "    6 | 0.000278 | 160512/160635 | 6.9298 | 5.9817 |\n",
      "val: {'recall': 0.806264, 'recall_grapheme': 0.672773, 'recall_vowel': 0.932807, 'recall_consonant': 0.946704, 'acc_grapheme': 0.7318, 'acc_vowel': 0.923766, 'acc_consonant': 0.926352, 'loss_grapheme': 1.481698, 'loss_vowel': 0.54753, 'loss_consonant': 0.376883}\n",
      "###>>>>> saved\n",
      "    7 | 0.000318 | 160512/160635 | 6.5037 | 5.7062 |\n",
      "val: {'recall': 0.885954, 'recall_grapheme': 0.821291, 'recall_vowel': 0.947406, 'recall_consonant': 0.95383, 'acc_grapheme': 0.82251, 'acc_vowel': 0.940007, 'acc_consonant': 0.935331, 'loss_grapheme': 1.025389, 'loss_vowel': 0.473739, 'loss_consonant': 0.329011}\n",
      "###>>>>> saved\n",
      "    8 | 0.000357 | 160512/160635 | 4.3370 | 5.5068 |\n",
      "val: {'recall': 0.910689, 'recall_grapheme': 0.865789, 'recall_vowel': 0.954163, 'recall_consonant': 0.957014, 'acc_grapheme': 0.862455, 'acc_vowel': 0.948713, 'acc_consonant': 0.951051, 'loss_grapheme': 0.926886, 'loss_vowel': 0.416967, 'loss_consonant': 0.313455}\n",
      "###>>>>> saved\n",
      "    9 | 0.000395 | 160512/160635 | 6.1722 | 5.1874 |\n",
      "val: {'recall': 0.929131, 'recall_grapheme': 0.895232, 'recall_vowel': 0.964179, 'recall_consonant': 0.961881, 'acc_grapheme': 0.890287, 'acc_vowel': 0.959284, 'acc_consonant': 0.960826, 'loss_grapheme': 0.71957, 'loss_vowel': 0.372189, 'loss_consonant': 0.272137}\n",
      "###>>>>> saved\n",
      "   10 | 0.000395 | 160512/160635 | 6.4684 | 5.1194 |\n",
      "val: {'recall': 0.937866, 'recall_grapheme': 0.908764, 'recall_vowel': 0.96854, 'recall_consonant': 0.965397, 'acc_grapheme': 0.902674, 'acc_vowel': 0.965228, 'acc_consonant': 0.95911, 'loss_grapheme': 0.665506, 'loss_vowel': 0.384458, 'loss_consonant': 0.283282}\n",
      "###>>>>> saved\n",
      "   11 | 0.000394 | 160512/160635 | 5.7706 | 5.1066 |\n",
      "val: {'recall': 0.940488, 'recall_grapheme': 0.913835, 'recall_vowel': 0.971991, 'recall_consonant': 0.962293, 'acc_grapheme': 0.913643, 'acc_vowel': 0.966944, 'acc_consonant': 0.970501, 'loss_grapheme': 0.597641, 'loss_vowel': 0.372051, 'loss_consonant': 0.264518}\n",
      "###>>>>> saved\n",
      "   12 | 0.000393 | 160512/160635 | 3.8045 | 4.8744 |\n",
      "val: {'recall': 0.947176, 'recall_grapheme': 0.920622, 'recall_vowel': 0.973363, 'recall_consonant': 0.974097, 'acc_grapheme': 0.919164, 'acc_vowel': 0.971372, 'acc_consonant': 0.97075, 'loss_grapheme': 0.69017, 'loss_vowel': 0.36214, 'loss_consonant': 0.25526}\n",
      "###>>>>> saved\n",
      "   13 | 0.000391 | 160512/160635 | 3.9409 | 4.7884 |\n",
      "val: {'recall': 0.952177, 'recall_grapheme': 0.92869, 'recall_vowel': 0.977568, 'recall_consonant': 0.973761, 'acc_grapheme': 0.925134, 'acc_vowel': 0.975103, 'acc_consonant': 0.974257, 'loss_grapheme': 0.577191, 'loss_vowel': 0.316685, 'loss_consonant': 0.258276}\n",
      "###>>>>> saved\n",
      "   14 | 0.000390 | 160512/160635 | 5.4253 | 4.7041 |\n",
      "val: {'recall': 0.954667, 'recall_grapheme': 0.931468, 'recall_vowel': 0.9779, 'recall_consonant': 0.977834, 'acc_grapheme': 0.932123, 'acc_vowel': 0.977291, 'acc_consonant': 0.970327, 'loss_grapheme': 0.584263, 'loss_vowel': 0.33479, 'loss_consonant': 0.247896}\n",
      "###>>>>> saved\n",
      "   15 | 0.000389 | 160512/160635 | 3.0833 | 4.9043 |\n",
      "val: {'recall': 0.956132, 'recall_grapheme': 0.93601, 'recall_vowel': 0.977027, 'recall_consonant': 0.97548, 'acc_grapheme': 0.93175, 'acc_vowel': 0.974356, 'acc_consonant': 0.971272, 'loss_grapheme': 0.651707, 'loss_vowel': 0.3867, 'loss_consonant': 0.264236}\n",
      "###>>>>> saved\n",
      "   16 | 0.000387 | 160512/160635 | 5.7075 | 4.7069 |\n",
      "val: {'recall': 0.959129, 'recall_grapheme': 0.938876, 'recall_vowel': 0.978952, 'recall_consonant': 0.979813, 'acc_grapheme': 0.937371, 'acc_vowel': 0.978908, 'acc_consonant': 0.97764, 'loss_grapheme': 0.605474, 'loss_vowel': 0.334548, 'loss_consonant': 0.242613}\n",
      "###>>>>> saved\n",
      "   17 | 0.000386 | 160512/160635 | 5.9144 | 4.4966 |\n",
      "val: {'recall': 0.959144, 'recall_grapheme': 0.9389, 'recall_vowel': 0.979933, 'recall_consonant': 0.978845, 'acc_grapheme': 0.93752, 'acc_vowel': 0.977565, 'acc_consonant': 0.976446, 'loss_grapheme': 0.588912, 'loss_vowel': 0.359852, 'loss_consonant': 0.258017}\n",
      "###>>>>> saved\n",
      "   18 | 0.000384 | 160512/160635 | 5.1002 | 4.6893 |\n",
      "val: {'recall': 0.962026, 'recall_grapheme': 0.943125, 'recall_vowel': 0.980476, 'recall_consonant': 0.981379, 'acc_grapheme': 0.939286, 'acc_vowel': 0.977839, 'acc_consonant': 0.977739, 'loss_grapheme': 0.633565, 'loss_vowel': 0.352685, 'loss_consonant': 0.259605}\n",
      "###>>>>> saved\n",
      "   19 | 0.000383 | 160512/160635 | 3.4032 | 4.3544 |\n",
      "val: {'recall': 0.964277, 'recall_grapheme': 0.945569, 'recall_vowel': 0.981892, 'recall_consonant': 0.984076, 'acc_grapheme': 0.942171, 'acc_vowel': 0.980997, 'acc_consonant': 0.97948, 'loss_grapheme': 0.559343, 'loss_vowel': 0.308568, 'loss_consonant': 0.21781}\n",
      "###>>>>> saved\n",
      "   20 | 0.000381 | 160512/160635 | 4.6023 | 4.4464 |\n",
      "val: {'recall': 0.963716, 'recall_grapheme': 0.944914, 'recall_vowel': 0.981263, 'recall_consonant': 0.983773, 'acc_grapheme': 0.941052, 'acc_vowel': 0.977291, 'acc_consonant': 0.972516, 'loss_grapheme': 0.662341, 'loss_vowel': 0.366076, 'loss_consonant': 0.249236}\n",
      "   21 | 0.000379 | 160512/160635 | 4.0569 | 4.3908 |\n",
      "val: {'recall': 0.964821, 'recall_grapheme': 0.946912, 'recall_vowel': 0.983041, 'recall_consonant': 0.98242, 'acc_grapheme': 0.946648, 'acc_vowel': 0.982738, 'acc_consonant': 0.981569, 'loss_grapheme': 0.474483, 'loss_vowel': 0.273229, 'loss_consonant': 0.197037}\n",
      "###>>>>> saved\n",
      "   22 | 0.000377 | 160512/160635 | 4.4716 | 4.4707 |\n",
      "val: {'recall': 0.965601, 'recall_grapheme': 0.948352, 'recall_vowel': 0.982695, 'recall_consonant': 0.983004, 'acc_grapheme': 0.947096, 'acc_vowel': 0.982266, 'acc_consonant': 0.982738, 'loss_grapheme': 0.538429, 'loss_vowel': 0.313765, 'loss_consonant': 0.216985}\n",
      "###>>>>> saved\n",
      "   23 | 0.000375 | 160512/160635 | 3.3581 | 4.3565 |\n",
      "val: {'recall': 0.967567, 'recall_grapheme': 0.950884, 'recall_vowel': 0.983639, 'recall_consonant': 0.984861, 'acc_grapheme': 0.946325, 'acc_vowel': 0.982863, 'acc_consonant': 0.982962, 'loss_grapheme': 0.554465, 'loss_vowel': 0.296187, 'loss_consonant': 0.203099}\n",
      "###>>>>> saved\n",
      "   24 | 0.000373 | 160512/160635 | 2.6118 | 4.3526 |\n",
      "val: {'recall': 0.966827, 'recall_grapheme': 0.949182, 'recall_vowel': 0.984051, 'recall_consonant': 0.984893, 'acc_grapheme': 0.949658, 'acc_vowel': 0.983161, 'acc_consonant': 0.982962, 'loss_grapheme': 0.526061, 'loss_vowel': 0.285804, 'loss_consonant': 0.199959}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   25 | 0.000371 | 160512/160635 | 1.1343 | 4.2199 |\n",
      "val: {'recall': 0.967237, 'recall_grapheme': 0.950462, 'recall_vowel': 0.983726, 'recall_consonant': 0.984298, 'acc_grapheme': 0.949832, 'acc_vowel': 0.984455, 'acc_consonant': 0.983932, 'loss_grapheme': 0.539282, 'loss_vowel': 0.31078, 'loss_consonant': 0.206629}\n",
      "   26 | 0.000369 | 160512/160635 | 4.1102 | 4.1880 |\n",
      "val: {'recall': 0.969215, 'recall_grapheme': 0.953893, 'recall_vowel': 0.985138, 'recall_consonant': 0.983936, 'acc_grapheme': 0.951698, 'acc_vowel': 0.984927, 'acc_consonant': 0.984703, 'loss_grapheme': 0.511045, 'loss_vowel': 0.275869, 'loss_consonant': 0.189064}\n",
      "###>>>>> saved\n",
      "   27 | 0.000367 | 160512/160635 | 5.4644 | 4.3561 |\n",
      "val: {'recall': 0.968969, 'recall_grapheme': 0.953176, 'recall_vowel': 0.984163, 'recall_consonant': 0.985362, 'acc_grapheme': 0.952643, 'acc_vowel': 0.984256, 'acc_consonant': 0.983186, 'loss_grapheme': 0.554379, 'loss_vowel': 0.304919, 'loss_consonant': 0.198365}\n",
      "   28 | 0.000364 | 160512/160635 | 3.9652 | 4.3194 |\n",
      "val: {'recall': 0.968717, 'recall_grapheme': 0.951828, 'recall_vowel': 0.984792, 'recall_consonant': 0.986421, 'acc_grapheme': 0.951424, 'acc_vowel': 0.983062, 'acc_consonant': 0.981296, 'loss_grapheme': 0.582802, 'loss_vowel': 0.334145, 'loss_consonant': 0.230811}\n",
      "   29 | 0.000362 | 160512/160635 | 2.2347 | 4.1114 |\n",
      "val: {'recall': 0.971028, 'recall_grapheme': 0.956712, 'recall_vowel': 0.985122, 'recall_consonant': 0.985567, 'acc_grapheme': 0.953538, 'acc_vowel': 0.984703, 'acc_consonant': 0.985002, 'loss_grapheme': 0.460097, 'loss_vowel': 0.271461, 'loss_consonant': 0.18606}\n",
      "###>>>>> saved\n",
      "   30 | 0.000359 | 160512/160635 | 5.1026 | 4.2224 |\n",
      "val: {'recall': 0.972177, 'recall_grapheme': 0.957746, 'recall_vowel': 0.986116, 'recall_consonant': 0.9871, 'acc_grapheme': 0.954831, 'acc_vowel': 0.984952, 'acc_consonant': 0.984131, 'loss_grapheme': 0.569915, 'loss_vowel': 0.311642, 'loss_consonant': 0.218654}\n",
      "###>>>>> saved\n",
      "   31 | 0.000357 | 160512/160635 | 4.7561 | 4.3095 |\n",
      "val: {'recall': 0.970186, 'recall_grapheme': 0.954183, 'recall_vowel': 0.985251, 'recall_consonant': 0.987126, 'acc_grapheme': 0.954757, 'acc_vowel': 0.984629, 'acc_consonant': 0.984878, 'loss_grapheme': 0.560466, 'loss_vowel': 0.318554, 'loss_consonant': 0.204796}\n",
      "   32 | 0.000354 | 160512/160635 | 2.6139 | 4.1693 |\n",
      "val: {'recall': 0.971334, 'recall_grapheme': 0.955973, 'recall_vowel': 0.986233, 'recall_consonant': 0.987157, 'acc_grapheme': 0.954434, 'acc_vowel': 0.984703, 'acc_consonant': 0.983634, 'loss_grapheme': 0.479067, 'loss_vowel': 0.278026, 'loss_consonant': 0.194547}\n",
      "   33 | 0.000351 | 160512/160635 | 1.6056 | 3.9367 |\n",
      "val: {'recall': 0.971631, 'recall_grapheme': 0.958356, 'recall_vowel': 0.984885, 'recall_consonant': 0.984928, 'acc_grapheme': 0.955702, 'acc_vowel': 0.985151, 'acc_consonant': 0.984927, 'loss_grapheme': 0.500004, 'loss_vowel': 0.281978, 'loss_consonant': 0.180084}\n",
      "   34 | 0.000349 | 160512/160635 | 5.4229 | 4.1890 |\n",
      "val: {'recall': 0.971689, 'recall_grapheme': 0.957167, 'recall_vowel': 0.985626, 'recall_consonant': 0.986797, 'acc_grapheme': 0.95702, 'acc_vowel': 0.985425, 'acc_consonant': 0.983286, 'loss_grapheme': 0.538703, 'loss_vowel': 0.324029, 'loss_consonant': 0.215771}\n",
      "   35 | 0.000346 | 160512/160635 | 4.7329 | 4.0948 |\n",
      "val: {'recall': 0.973362, 'recall_grapheme': 0.960232, 'recall_vowel': 0.986121, 'recall_consonant': 0.986861, 'acc_grapheme': 0.95814, 'acc_vowel': 0.986718, 'acc_consonant': 0.985649, 'loss_grapheme': 0.450835, 'loss_vowel': 0.256743, 'loss_consonant': 0.176643}\n",
      "###>>>>> saved\n",
      "   36 | 0.000343 | 160512/160635 | 3.8539 | 4.2385 |\n",
      "val: {'recall': 0.973487, 'recall_grapheme': 0.959536, 'recall_vowel': 0.985904, 'recall_consonant': 0.988971, 'acc_grapheme': 0.957393, 'acc_vowel': 0.985947, 'acc_consonant': 0.985798, 'loss_grapheme': 0.50659, 'loss_vowel': 0.315135, 'loss_consonant': 0.213082}\n",
      "###>>>>> saved\n",
      "   37 | 0.000340 | 160512/160635 | 2.2365 | 3.9609 |\n",
      "val: {'recall': 0.973547, 'recall_grapheme': 0.960561, 'recall_vowel': 0.986617, 'recall_consonant': 0.986449, 'acc_grapheme': 0.959085, 'acc_vowel': 0.986643, 'acc_consonant': 0.986643, 'loss_grapheme': 0.471767, 'loss_vowel': 0.2809, 'loss_consonant': 0.189854}\n",
      "###>>>>> saved\n",
      "   38 | 0.000337 | 160512/160635 | 2.5586 | 4.0378 |\n",
      "val: {'recall': 0.97409, 'recall_grapheme': 0.9616, 'recall_vowel': 0.986985, 'recall_consonant': 0.986177, 'acc_grapheme': 0.959682, 'acc_vowel': 0.986942, 'acc_consonant': 0.986942, 'loss_grapheme': 0.465342, 'loss_vowel': 0.272083, 'loss_consonant': 0.186975}\n",
      "###>>>>> saved\n",
      "   39 | 0.000334 | 160512/160635 | 4.7532 | 4.0895 |\n",
      "val: {'recall': 0.972799, 'recall_grapheme': 0.959633, 'recall_vowel': 0.985772, 'recall_consonant': 0.986159, 'acc_grapheme': 0.958239, 'acc_vowel': 0.98642, 'acc_consonant': 0.986171, 'loss_grapheme': 0.499567, 'loss_vowel': 0.304171, 'loss_consonant': 0.200027}\n",
      "   40 | 0.000331 | 160512/160635 | 4.6324 | 4.1387 |\n",
      "val: {'recall': 0.973439, 'recall_grapheme': 0.960301, 'recall_vowel': 0.986886, 'recall_consonant': 0.98627, 'acc_grapheme': 0.959508, 'acc_vowel': 0.986494, 'acc_consonant': 0.986494, 'loss_grapheme': 0.481272, 'loss_vowel': 0.295206, 'loss_consonant': 0.196241}\n",
      "   41 | 0.000328 | 160512/160635 | 2.3523 | 3.9194 |\n",
      "val: {'recall': 0.974965, 'recall_grapheme': 0.962196, 'recall_vowel': 0.987602, 'recall_consonant': 0.987868, 'acc_grapheme': 0.960179, 'acc_vowel': 0.987166, 'acc_consonant': 0.986022, 'loss_grapheme': 0.484044, 'loss_vowel': 0.312845, 'loss_consonant': 0.201347}\n",
      "###>>>>> saved\n",
      "   42 | 0.000324 | 160512/160635 | 4.3156 | 4.1426 |\n",
      "val: {'recall': 0.974152, 'recall_grapheme': 0.961401, 'recall_vowel': 0.986229, 'recall_consonant': 0.987577, 'acc_grapheme': 0.960005, 'acc_vowel': 0.986171, 'acc_consonant': 0.985499, 'loss_grapheme': 0.50483, 'loss_vowel': 0.320341, 'loss_consonant': 0.213984}\n",
      "   43 | 0.000321 | 160512/160635 | 5.0374 | 3.9711 |\n",
      "val: {'recall': 0.974238, 'recall_grapheme': 0.961162, 'recall_vowel': 0.986263, 'recall_consonant': 0.988365, 'acc_grapheme': 0.960801, 'acc_vowel': 0.987564, 'acc_consonant': 0.986047, 'loss_grapheme': 0.49775, 'loss_vowel': 0.315626, 'loss_consonant': 0.205682}\n",
      "   44 | 0.000318 | 160512/160635 | 4.0491 | 3.9778 |\n",
      "val: {'recall': 0.973905, 'recall_grapheme': 0.960712, 'recall_vowel': 0.987397, 'recall_consonant': 0.986801, 'acc_grapheme': 0.961025, 'acc_vowel': 0.98724, 'acc_consonant': 0.987141, 'loss_grapheme': 0.478062, 'loss_vowel': 0.287801, 'loss_consonant': 0.18892}\n",
      "   45 | 0.000314 | 160512/160635 | 3.5792 | 4.1523 |\n",
      "val: {'recall': 0.975292, 'recall_grapheme': 0.964483, 'recall_vowel': 0.986552, 'recall_consonant': 0.985649, 'acc_grapheme': 0.961423, 'acc_vowel': 0.986867, 'acc_consonant': 0.986768, 'loss_grapheme': 0.524, 'loss_vowel': 0.313533, 'loss_consonant': 0.198673}\n",
      "###>>>>> saved\n",
      "   46 | 0.000311 | 160512/160635 | 5.1232 | 3.9886 |\n",
      "val: {'recall': 0.974541, 'recall_grapheme': 0.963355, 'recall_vowel': 0.985856, 'recall_consonant': 0.985601, 'acc_grapheme': 0.961224, 'acc_vowel': 0.987415, 'acc_consonant': 0.987439, 'loss_grapheme': 0.428669, 'loss_vowel': 0.260177, 'loss_consonant': 0.178306}\n",
      "   47 | 0.000307 | 160512/160635 | 3.9407 | 3.9094 |\n",
      "val: {'recall': 0.975585, 'recall_grapheme': 0.963671, 'recall_vowel': 0.986013, 'recall_consonant': 0.988984, 'acc_grapheme': 0.960502, 'acc_vowel': 0.987017, 'acc_consonant': 0.984554, 'loss_grapheme': 0.478197, 'loss_vowel': 0.307401, 'loss_consonant': 0.204836}\n",
      "###>>>>> saved\n",
      "   48 | 0.000304 | 160512/160635 | 4.5063 | 3.8749 |\n",
      "val: {'recall': 0.97593, 'recall_grapheme': 0.964788, 'recall_vowel': 0.987287, 'recall_consonant': 0.986854, 'acc_grapheme': 0.963263, 'acc_vowel': 0.987613, 'acc_consonant': 0.987738, 'loss_grapheme': 0.440839, 'loss_vowel': 0.285149, 'loss_consonant': 0.182773}\n",
      "###>>>>> saved\n",
      "   49 | 0.000300 | 160512/160635 | 5.0685 | 4.0200 |\n",
      "val: {'recall': 0.975981, 'recall_grapheme': 0.964343, 'recall_vowel': 0.988297, 'recall_consonant': 0.98694, 'acc_grapheme': 0.963238, 'acc_vowel': 0.98836, 'acc_consonant': 0.987415, 'loss_grapheme': 0.454165, 'loss_vowel': 0.292156, 'loss_consonant': 0.187202}\n",
      "###>>>>> saved\n",
      "   50 | 0.000296 | 160512/160635 | 4.9300 | 3.9122 |\n",
      "val: {'recall': 0.976239, 'recall_grapheme': 0.96496, 'recall_vowel': 0.987419, 'recall_consonant': 0.987617, 'acc_grapheme': 0.963413, 'acc_vowel': 0.988036, 'acc_consonant': 0.987663, 'loss_grapheme': 0.446295, 'loss_vowel': 0.280751, 'loss_consonant': 0.184412}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###>>>>> saved\n",
      "   51 | 0.000293 | 160512/160635 | 5.0057 | 3.7229 |\n",
      "val: {'recall': 0.975462, 'recall_grapheme': 0.963635, 'recall_vowel': 0.988466, 'recall_consonant': 0.986113, 'acc_grapheme': 0.963214, 'acc_vowel': 0.988459, 'acc_consonant': 0.988136, 'loss_grapheme': 0.400704, 'loss_vowel': 0.240818, 'loss_consonant': 0.159125}\n",
      "   52 | 0.000289 | 160512/160635 | 4.2268 | 3.8135 |\n",
      "val: {'recall': 0.975484, 'recall_grapheme': 0.963644, 'recall_vowel': 0.98739, 'recall_consonant': 0.987256, 'acc_grapheme': 0.963313, 'acc_vowel': 0.987788, 'acc_consonant': 0.988061, 'loss_grapheme': 0.406244, 'loss_vowel': 0.261317, 'loss_consonant': 0.174894}\n",
      "   53 | 0.000285 | 160512/160635 | 3.5911 | 3.8886 |\n",
      "val: {'recall': 0.976018, 'recall_grapheme': 0.965553, 'recall_vowel': 0.987949, 'recall_consonant': 0.985018, 'acc_grapheme': 0.963711, 'acc_vowel': 0.988708, 'acc_consonant': 0.98826, 'loss_grapheme': 0.415905, 'loss_vowel': 0.255179, 'loss_consonant': 0.173964}\n",
      "   54 | 0.000281 | 160512/160635 | 5.0396 | 3.7572 |\n",
      "val: {'recall': 0.976822, 'recall_grapheme': 0.96525, 'recall_vowel': 0.987775, 'recall_consonant': 0.98901, 'acc_grapheme': 0.965104, 'acc_vowel': 0.988857, 'acc_consonant': 0.98826, 'loss_grapheme': 0.418356, 'loss_vowel': 0.269228, 'loss_consonant': 0.170076}\n",
      "###>>>>> saved\n",
      "   55 | 0.000278 | 160512/160635 | 4.8065 | 4.0082 |\n",
      "val: {'recall': 0.97691, 'recall_grapheme': 0.965354, 'recall_vowel': 0.98856, 'recall_consonant': 0.988373, 'acc_grapheme': 0.965104, 'acc_vowel': 0.989006, 'acc_consonant': 0.988534, 'loss_grapheme': 0.461608, 'loss_vowel': 0.297128, 'loss_consonant': 0.196484}\n",
      "###>>>>> saved\n",
      "   56 | 0.000274 | 160512/160635 | 4.3423 | 3.7452 |\n",
      "val: {'recall': 0.977023, 'recall_grapheme': 0.965531, 'recall_vowel': 0.989016, 'recall_consonant': 0.988013, 'acc_grapheme': 0.964383, 'acc_vowel': 0.988534, 'acc_consonant': 0.987439, 'loss_grapheme': 0.446907, 'loss_vowel': 0.280531, 'loss_consonant': 0.179106}\n",
      "###>>>>> saved\n",
      "   57 | 0.000270 | 160512/160635 | 4.0619 | 3.9242 |\n",
      "val: {'recall': 0.97766, 'recall_grapheme': 0.966904, 'recall_vowel': 0.988725, 'recall_consonant': 0.988105, 'acc_grapheme': 0.965079, 'acc_vowel': 0.98918, 'acc_consonant': 0.988459, 'loss_grapheme': 0.415031, 'loss_vowel': 0.268991, 'loss_consonant': 0.178317}\n",
      "###>>>>> saved\n",
      "   58 | 0.000266 | 160512/160635 | 5.2378 | 3.9649 |\n",
      "val: {'recall': 0.976638, 'recall_grapheme': 0.965982, 'recall_vowel': 0.987655, 'recall_consonant': 0.986934, 'acc_grapheme': 0.964283, 'acc_vowel': 0.988335, 'acc_consonant': 0.988086, 'loss_grapheme': 0.432872, 'loss_vowel': 0.282468, 'loss_consonant': 0.185944}\n",
      "   59 | 0.000262 | 160512/160635 | 2.8453 | 3.8319 |\n",
      "val: {'recall': 0.977831, 'recall_grapheme': 0.967629, 'recall_vowel': 0.987893, 'recall_consonant': 0.988173, 'acc_grapheme': 0.965054, 'acc_vowel': 0.988509, 'acc_consonant': 0.989156, 'loss_grapheme': 0.443938, 'loss_vowel': 0.290236, 'loss_consonant': 0.186288}\n",
      "###>>>>> saved\n",
      "   60 | 0.000258 | 160512/160635 | 4.7559 | 3.9513 |\n",
      "val: {'recall': 0.978169, 'recall_grapheme': 0.967257, 'recall_vowel': 0.988714, 'recall_consonant': 0.989449, 'acc_grapheme': 0.965278, 'acc_vowel': 0.988932, 'acc_consonant': 0.988758, 'loss_grapheme': 0.457564, 'loss_vowel': 0.306227, 'loss_consonant': 0.199151}\n",
      "###>>>>> saved\n",
      "   61 | 0.000254 | 160512/160635 | 3.8374 | 3.8939 |\n",
      "val: {'recall': 0.977465, 'recall_grapheme': 0.967366, 'recall_vowel': 0.987575, 'recall_consonant': 0.987552, 'acc_grapheme': 0.965377, 'acc_vowel': 0.988882, 'acc_consonant': 0.988608, 'loss_grapheme': 0.433422, 'loss_vowel': 0.284769, 'loss_consonant': 0.18622}\n",
      "   62 | 0.000250 | 160512/160635 | 4.8309 | 3.8098 |\n",
      "val: {'recall': 0.97762, 'recall_grapheme': 0.966774, 'recall_vowel': 0.988147, 'recall_consonant': 0.988784, 'acc_grapheme': 0.966124, 'acc_vowel': 0.989255, 'acc_consonant': 0.988633, 'loss_grapheme': 0.395869, 'loss_vowel': 0.256803, 'loss_consonant': 0.168183}\n",
      "   63 | 0.000246 | 160512/160635 | 4.3976 | 3.6795 |\n",
      "val: {'recall': 0.978034, 'recall_grapheme': 0.967553, 'recall_vowel': 0.988891, 'recall_consonant': 0.988141, 'acc_grapheme': 0.966148, 'acc_vowel': 0.98933, 'acc_consonant': 0.988832, 'loss_grapheme': 0.371963, 'loss_vowel': 0.249697, 'loss_consonant': 0.178144}\n",
      "   64 | 0.000242 | 160512/160635 | 3.0879 | 3.7819 |\n",
      "val: {'recall': 0.97803, 'recall_grapheme': 0.968005, 'recall_vowel': 0.98801, 'recall_consonant': 0.988098, 'acc_grapheme': 0.966571, 'acc_vowel': 0.988981, 'acc_consonant': 0.988857, 'loss_grapheme': 0.420659, 'loss_vowel': 0.281514, 'loss_consonant': 0.183094}\n",
      "   65 | 0.000237 | 160512/160635 | 3.3095 | 3.8613 |\n",
      "val: {'recall': 0.978712, 'recall_grapheme': 0.968503, 'recall_vowel': 0.989012, 'recall_consonant': 0.988832, 'acc_grapheme': 0.966049, 'acc_vowel': 0.989355, 'acc_consonant': 0.988882, 'loss_grapheme': 0.400265, 'loss_vowel': 0.268942, 'loss_consonant': 0.17505}\n",
      "###>>>>> saved\n",
      "   66 | 0.000233 | 160512/160635 | 3.5531 | 3.7460 |\n",
      "val: {'recall': 0.978145, 'recall_grapheme': 0.968484, 'recall_vowel': 0.988267, 'recall_consonant': 0.987346, 'acc_grapheme': 0.96692, 'acc_vowel': 0.989554, 'acc_consonant': 0.989081, 'loss_grapheme': 0.406648, 'loss_vowel': 0.259461, 'loss_consonant': 0.177347}\n",
      "   67 | 0.000229 | 160512/160635 | 4.3934 | 3.8159 |\n",
      "val: {'recall': 0.97862, 'recall_grapheme': 0.968952, 'recall_vowel': 0.988435, 'recall_consonant': 0.988142, 'acc_grapheme': 0.967392, 'acc_vowel': 0.988932, 'acc_consonant': 0.989031, 'loss_grapheme': 0.401407, 'loss_vowel': 0.262327, 'loss_consonant': 0.176764}\n",
      "   68 | 0.000225 | 160512/160635 | 1.7918 | 3.6820 |\n",
      "val: {'recall': 0.97903, 'recall_grapheme': 0.969164, 'recall_vowel': 0.989164, 'recall_consonant': 0.988628, 'acc_grapheme': 0.967765, 'acc_vowel': 0.989628, 'acc_consonant': 0.989205, 'loss_grapheme': 0.371773, 'loss_vowel': 0.245184, 'loss_consonant': 0.16236}\n",
      "###>>>>> saved\n",
      "   69 | 0.000221 | 160512/160635 | 4.2870 | 3.7631 |\n",
      "val: {'recall': 0.978464, 'recall_grapheme': 0.968443, 'recall_vowel': 0.988651, 'recall_consonant': 0.988317, 'acc_grapheme': 0.967591, 'acc_vowel': 0.989379, 'acc_consonant': 0.989653, 'loss_grapheme': 0.405066, 'loss_vowel': 0.272722, 'loss_consonant': 0.183058}\n",
      "   70 | 0.000217 | 160512/160635 | 4.1923 | 3.9351 |\n",
      "val: {'recall': 0.978868, 'recall_grapheme': 0.969455, 'recall_vowel': 0.98906, 'recall_consonant': 0.987503, 'acc_grapheme': 0.968213, 'acc_vowel': 0.989777, 'acc_consonant': 0.989205, 'loss_grapheme': 0.429233, 'loss_vowel': 0.280613, 'loss_consonant': 0.190361}\n",
      "   71 | 0.000213 | 160512/160635 | 4.2362 | 3.7655 |\n",
      "val: {'recall': 0.979406, 'recall_grapheme': 0.969729, 'recall_vowel': 0.988723, 'recall_consonant': 0.989444, 'acc_grapheme': 0.968263, 'acc_vowel': 0.989578, 'acc_consonant': 0.989404, 'loss_grapheme': 0.398762, 'loss_vowel': 0.269195, 'loss_consonant': 0.181979}\n",
      "###>>>>> saved\n",
      "   72 | 0.000208 | 160512/160635 | 1.7730 | 3.8055 |\n",
      "val: {'recall': 0.979108, 'recall_grapheme': 0.969696, 'recall_vowel': 0.989775, 'recall_consonant': 0.987264, 'acc_grapheme': 0.968014, 'acc_vowel': 0.989827, 'acc_consonant': 0.989703, 'loss_grapheme': 0.435666, 'loss_vowel': 0.290135, 'loss_consonant': 0.194158}\n",
      "   73 | 0.000204 | 160512/160635 | 2.9654 | 3.7814 |\n",
      "val: {'recall': 0.979339, 'recall_grapheme': 0.970304, 'recall_vowel': 0.988585, 'recall_consonant': 0.988164, 'acc_grapheme': 0.968163, 'acc_vowel': 0.989504, 'acc_consonant': 0.989802, 'loss_grapheme': 0.403899, 'loss_vowel': 0.271704, 'loss_consonant': 0.180754}\n",
      "   74 | 0.000200 | 160512/160635 | 1.9233 | 3.7406 |\n",
      "val: {'recall': 0.97914, 'recall_grapheme': 0.969655, 'recall_vowel': 0.989005, 'recall_consonant': 0.988246, 'acc_grapheme': 0.968934, 'acc_vowel': 0.989454, 'acc_consonant': 0.989429, 'loss_grapheme': 0.384573, 'loss_vowel': 0.250546, 'loss_consonant': 0.168061}\n",
      "   75 | 0.000196 | 160512/160635 | 3.1976 | 3.6124 |\n",
      "val: {'recall': 0.979603, 'recall_grapheme': 0.97048, 'recall_vowel': 0.989305, 'recall_consonant': 0.988149, 'acc_grapheme': 0.968362, 'acc_vowel': 0.989753, 'acc_consonant': 0.989504, 'loss_grapheme': 0.362335, 'loss_vowel': 0.24047, 'loss_consonant': 0.164977}\n",
      "###>>>>> saved\n",
      "   76 | 0.000192 | 160512/160635 | 1.0315 | 3.8257 |\n",
      "val: {'recall': 0.979823, 'recall_grapheme': 0.97025, 'recall_vowel': 0.989228, 'recall_consonant': 0.989566, 'acc_grapheme': 0.969631, 'acc_vowel': 0.990001, 'acc_consonant': 0.989603, 'loss_grapheme': 0.385627, 'loss_vowel': 0.254051, 'loss_consonant': 0.171209}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###>>>>> saved\n",
      "   77 | 0.000187 | 160512/160635 | 1.4795 | 3.6708 |\n",
      "val: {'recall': 0.980193, 'recall_grapheme': 0.969962, 'recall_vowel': 0.989296, 'recall_consonant': 0.991555, 'acc_grapheme': 0.968984, 'acc_vowel': 0.990001, 'acc_consonant': 0.990051, 'loss_grapheme': 0.390127, 'loss_vowel': 0.265139, 'loss_consonant': 0.18018}\n",
      "###>>>>> saved\n",
      "   78 | 0.000183 | 160512/160635 | 2.8721 | 3.8874 |\n",
      "val: {'recall': 0.980134, 'recall_grapheme': 0.971135, 'recall_vowel': 0.988264, 'recall_consonant': 0.990003, 'acc_grapheme': 0.968685, 'acc_vowel': 0.989454, 'acc_consonant': 0.98933, 'loss_grapheme': 0.398032, 'loss_vowel': 0.273116, 'loss_consonant': 0.182404}\n",
      "   79 | 0.000179 | 160512/160635 | 1.6144 | 3.6570 |\n",
      "val: {'recall': 0.980405, 'recall_grapheme': 0.971718, 'recall_vowel': 0.988641, 'recall_consonant': 0.989542, 'acc_grapheme': 0.969631, 'acc_vowel': 0.989703, 'acc_consonant': 0.989852, 'loss_grapheme': 0.377308, 'loss_vowel': 0.250882, 'loss_consonant': 0.167135}\n",
      "###>>>>> saved\n",
      "   80 | 0.000175 | 160512/160635 | 4.4680 | 3.7084 |\n",
      "val: {'recall': 0.980109, 'recall_grapheme': 0.971381, 'recall_vowel': 0.988603, 'recall_consonant': 0.989072, 'acc_grapheme': 0.970078, 'acc_vowel': 0.989603, 'acc_consonant': 0.990026, 'loss_grapheme': 0.386277, 'loss_vowel': 0.259429, 'loss_consonant': 0.174552}\n",
      "   81 | 0.000171 | 160512/160635 | 2.4507 | 3.6317 |\n",
      "val: {'recall': 0.980426, 'recall_grapheme': 0.97154, 'recall_vowel': 0.98934, 'recall_consonant': 0.989282, 'acc_grapheme': 0.970004, 'acc_vowel': 0.990374, 'acc_consonant': 0.990026, 'loss_grapheme': 0.351548, 'loss_vowel': 0.235738, 'loss_consonant': 0.160014}\n",
      "###>>>>> saved\n",
      "   82 | 0.000167 | 160512/160635 | 4.0031 | 3.6948 |\n",
      "val: {'recall': 0.980738, 'recall_grapheme': 0.971584, 'recall_vowel': 0.9891, 'recall_consonant': 0.990683, 'acc_grapheme': 0.96983, 'acc_vowel': 0.990076, 'acc_consonant': 0.989951, 'loss_grapheme': 0.379584, 'loss_vowel': 0.253543, 'loss_consonant': 0.167869}\n",
      "###>>>>> saved\n",
      "   83 | 0.000163 | 160512/160635 | 4.6825 | 3.7383 |\n",
      "val: {'recall': 0.981055, 'recall_grapheme': 0.972224, 'recall_vowel': 0.989939, 'recall_consonant': 0.989833, 'acc_grapheme': 0.970053, 'acc_vowel': 0.990275, 'acc_consonant': 0.990374, 'loss_grapheme': 0.406282, 'loss_vowel': 0.282007, 'loss_consonant': 0.187687}\n",
      "###>>>>> saved\n",
      "   84 | 0.000158 | 160512/160635 | 2.6107 | 3.6309 |\n",
      "val: {'recall': 0.981006, 'recall_grapheme': 0.972114, 'recall_vowel': 0.989915, 'recall_consonant': 0.98988, 'acc_grapheme': 0.970526, 'acc_vowel': 0.990374, 'acc_consonant': 0.990225, 'loss_grapheme': 0.373243, 'loss_vowel': 0.258181, 'loss_consonant': 0.173759}\n",
      "   85 | 0.000154 | 160512/160635 | 4.0613 | 3.7754 |\n",
      "val: {'recall': 0.980967, 'recall_grapheme': 0.972086, 'recall_vowel': 0.989177, 'recall_consonant': 0.990518, 'acc_grapheme': 0.970501, 'acc_vowel': 0.989976, 'acc_consonant': 0.990001, 'loss_grapheme': 0.386862, 'loss_vowel': 0.263433, 'loss_consonant': 0.178047}\n",
      "   86 | 0.000150 | 160512/160635 | 1.6254 | 3.8117 |\n",
      "val: {'recall': 0.980998, 'recall_grapheme': 0.972321, 'recall_vowel': 0.989602, 'recall_consonant': 0.989746, 'acc_grapheme': 0.97075, 'acc_vowel': 0.990349, 'acc_consonant': 0.989976, 'loss_grapheme': 0.389537, 'loss_vowel': 0.27019, 'loss_consonant': 0.180224}\n",
      "   87 | 0.000146 | 160512/160635 | 4.2132 | 3.7812 |\n",
      "val: {'recall': 0.980398, 'recall_grapheme': 0.971873, 'recall_vowel': 0.989366, 'recall_consonant': 0.98848, 'acc_grapheme': 0.970128, 'acc_vowel': 0.990051, 'acc_consonant': 0.9902, 'loss_grapheme': 0.393821, 'loss_vowel': 0.265466, 'loss_consonant': 0.180891}\n",
      "   88 | 0.000142 | 160512/160635 | 4.3250 | 3.6881 |\n",
      "val: {'recall': 0.981059, 'recall_grapheme': 0.972645, 'recall_vowel': 0.989451, 'recall_consonant': 0.989497, 'acc_grapheme': 0.970601, 'acc_vowel': 0.990275, 'acc_consonant': 0.990126, 'loss_grapheme': 0.383717, 'loss_vowel': 0.257067, 'loss_consonant': 0.175405}\n",
      "###>>>>> saved\n",
      "   89 | 0.000138 | 160512/160635 | 2.3212 | 3.6962 |\n",
      "val: {'recall': 0.981484, 'recall_grapheme': 0.972809, 'recall_vowel': 0.98973, 'recall_consonant': 0.990589, 'acc_grapheme': 0.970675, 'acc_vowel': 0.990524, 'acc_consonant': 0.990275, 'loss_grapheme': 0.379166, 'loss_vowel': 0.259033, 'loss_consonant': 0.172544}\n",
      "###>>>>> saved\n",
      "   90 | 0.000134 | 160512/160635 | 4.5094 | 3.6293 |\n",
      "val: {'recall': 0.980856, 'recall_grapheme': 0.972445, 'recall_vowel': 0.98913, 'recall_consonant': 0.989404, 'acc_grapheme': 0.97065, 'acc_vowel': 0.989703, 'acc_consonant': 0.989902, 'loss_grapheme': 0.389601, 'loss_vowel': 0.270949, 'loss_consonant': 0.182046}\n",
      "   91 | 0.000130 | 160512/160635 | 3.2322 | 3.8226 |\n",
      "val: {'recall': 0.981327, 'recall_grapheme': 0.972788, 'recall_vowel': 0.990221, 'recall_consonant': 0.98951, 'acc_grapheme': 0.9708, 'acc_vowel': 0.990349, 'acc_consonant': 0.990076, 'loss_grapheme': 0.392468, 'loss_vowel': 0.274802, 'loss_consonant': 0.186335}\n",
      "   92 | 0.000126 | 160512/160635 | 3.8546 | 3.8470 |\n",
      "val: {'recall': 0.98098, 'recall_grapheme': 0.972268, 'recall_vowel': 0.989453, 'recall_consonant': 0.989931, 'acc_grapheme': 0.970427, 'acc_vowel': 0.989877, 'acc_consonant': 0.989703, 'loss_grapheme': 0.411716, 'loss_vowel': 0.282096, 'loss_consonant': 0.192882}\n",
      "   93 | 0.000123 | 160512/160635 | 4.3557 | 3.7591 |\n",
      "val: {'recall': 0.981477, 'recall_grapheme': 0.972696, 'recall_vowel': 0.989552, 'recall_consonant': 0.990963, 'acc_grapheme': 0.971073, 'acc_vowel': 0.990126, 'acc_consonant': 0.990126, 'loss_grapheme': 0.390611, 'loss_vowel': 0.275883, 'loss_consonant': 0.185372}\n",
      "   94 | 0.000119 | 160512/160635 | 4.1187 | 3.6788 |\n",
      "val: {'recall': 0.981392, 'recall_grapheme': 0.972723, 'recall_vowel': 0.990028, 'recall_consonant': 0.990095, 'acc_grapheme': 0.971073, 'acc_vowel': 0.990399, 'acc_consonant': 0.990374, 'loss_grapheme': 0.393074, 'loss_vowel': 0.268447, 'loss_consonant': 0.176861}\n",
      "   95 | 0.000115 | 160512/160635 | 4.8208 | 3.6853 |\n",
      "val: {'recall': 0.981562, 'recall_grapheme': 0.973149, 'recall_vowel': 0.99011, 'recall_consonant': 0.98984, 'acc_grapheme': 0.971695, 'acc_vowel': 0.990922, 'acc_consonant': 0.990922, 'loss_grapheme': 0.367729, 'loss_vowel': 0.258759, 'loss_consonant': 0.173058}\n",
      "###>>>>> saved\n",
      "   96 | 0.000111 | 160512/160635 | 3.0402 | 3.6600 |\n",
      "val: {'recall': 0.981651, 'recall_grapheme': 0.972704, 'recall_vowel': 0.989935, 'recall_consonant': 0.991262, 'acc_grapheme': 0.971073, 'acc_vowel': 0.990698, 'acc_consonant': 0.990499, 'loss_grapheme': 0.376903, 'loss_vowel': 0.258425, 'loss_consonant': 0.174969}\n",
      "###>>>>> saved\n",
      "   97 | 0.000107 | 160512/160635 | 4.0042 | 3.5580 |\n",
      "val: {'recall': 0.981818, 'recall_grapheme': 0.973512, 'recall_vowel': 0.989902, 'recall_consonant': 0.990345, 'acc_grapheme': 0.972018, 'acc_vowel': 0.990548, 'acc_consonant': 0.990573, 'loss_grapheme': 0.363815, 'loss_vowel': 0.247316, 'loss_consonant': 0.168944}\n",
      "###>>>>> saved\n",
      "   98 | 0.000104 | 160512/160635 | 4.7093 | 3.7931 |\n",
      "val: {'recall': 0.981742, 'recall_grapheme': 0.973324, 'recall_vowel': 0.990078, 'recall_consonant': 0.990242, 'acc_grapheme': 0.971571, 'acc_vowel': 0.990922, 'acc_consonant': 0.990648, 'loss_grapheme': 0.379619, 'loss_vowel': 0.257303, 'loss_consonant': 0.176345}\n",
      "   99 | 0.000100 | 160512/160635 | 3.8402 | 3.7210 |\n",
      "val: {'recall': 0.981616, 'recall_grapheme': 0.973491, 'recall_vowel': 0.989788, 'recall_consonant': 0.989693, 'acc_grapheme': 0.971795, 'acc_vowel': 0.990275, 'acc_consonant': 0.990548, 'loss_grapheme': 0.362736, 'loss_vowel': 0.2515, 'loss_consonant': 0.174195}\n",
      "  100 | 0.000096 | 160512/160635 | 2.0762 | 3.9063 |\n",
      "val: {'recall': 0.98184, 'recall_grapheme': 0.973603, 'recall_vowel': 0.990169, 'recall_consonant': 0.989983, 'acc_grapheme': 0.971372, 'acc_vowel': 0.990698, 'acc_consonant': 0.990499, 'loss_grapheme': 0.408357, 'loss_vowel': 0.287545, 'loss_consonant': 0.195712}\n",
      "###>>>>> saved\n",
      "  101 | 0.000093 | 160512/160635 | 4.3450 | 3.7281 |\n",
      "val: {'recall': 0.981733, 'recall_grapheme': 0.972985, 'recall_vowel': 0.990166, 'recall_consonant': 0.990797, 'acc_grapheme': 0.971695, 'acc_vowel': 0.990922, 'acc_consonant': 0.990872, 'loss_grapheme': 0.378167, 'loss_vowel': 0.269434, 'loss_consonant': 0.18359}\n",
      "  102 | 0.000089 | 160512/160635 | 0.9820 | 3.5958 |\n",
      "val: {'recall': 0.981646, 'recall_grapheme': 0.973464, 'recall_vowel': 0.989332, 'recall_consonant': 0.990325, 'acc_grapheme': 0.971994, 'acc_vowel': 0.990449, 'acc_consonant': 0.990598, 'loss_grapheme': 0.345224, 'loss_vowel': 0.241364, 'loss_consonant': 0.167317}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  103 | 0.000086 | 160512/160635 | 2.8523 | 3.6201 |\n",
      "val: {'recall': 0.981634, 'recall_grapheme': 0.973065, 'recall_vowel': 0.989673, 'recall_consonant': 0.990732, 'acc_grapheme': 0.971844, 'acc_vowel': 0.990698, 'acc_consonant': 0.990573, 'loss_grapheme': 0.372433, 'loss_vowel': 0.255555, 'loss_consonant': 0.174102}\n",
      "  104 | 0.000082 | 160512/160635 | 3.7834 | 3.5074 |\n",
      "val: {'recall': 0.982068, 'recall_grapheme': 0.973663, 'recall_vowel': 0.990113, 'recall_consonant': 0.990834, 'acc_grapheme': 0.971844, 'acc_vowel': 0.990797, 'acc_consonant': 0.990822, 'loss_grapheme': 0.355891, 'loss_vowel': 0.244377, 'loss_consonant': 0.166154}\n",
      "###>>>>> saved\n",
      "  105 | 0.000079 | 160512/160635 | 2.6721 | 3.6968 |\n",
      "val: {'recall': 0.982141, 'recall_grapheme': 0.973786, 'recall_vowel': 0.990325, 'recall_consonant': 0.990667, 'acc_grapheme': 0.971745, 'acc_vowel': 0.990872, 'acc_consonant': 0.990598, 'loss_grapheme': 0.36518, 'loss_vowel': 0.251141, 'loss_consonant': 0.169694}\n",
      "###>>>>> saved\n",
      "  106 | 0.000076 | 160512/160635 | 4.3688 | 3.6421 |\n",
      "val: {'recall': 0.982129, 'recall_grapheme': 0.973419, 'recall_vowel': 0.990376, 'recall_consonant': 0.991302, 'acc_grapheme': 0.972217, 'acc_vowel': 0.990747, 'acc_consonant': 0.990698, 'loss_grapheme': 0.377361, 'loss_vowel': 0.264411, 'loss_consonant': 0.179723}\n",
      "  107 | 0.000073 | 160512/160635 | 2.8583 | 3.5470 |\n",
      "val: {'recall': 0.981757, 'recall_grapheme': 0.973256, 'recall_vowel': 0.990617, 'recall_consonant': 0.989902, 'acc_grapheme': 0.971869, 'acc_vowel': 0.991121, 'acc_consonant': 0.990648, 'loss_grapheme': 0.347506, 'loss_vowel': 0.239171, 'loss_consonant': 0.162357}\n",
      "  108 | 0.000069 | 160512/160635 | 4.7552 | 3.6719 |\n",
      "val: {'recall': 0.982306, 'recall_grapheme': 0.974306, 'recall_vowel': 0.990068, 'recall_consonant': 0.990545, 'acc_grapheme': 0.972018, 'acc_vowel': 0.990548, 'acc_consonant': 0.990524, 'loss_grapheme': 0.370858, 'loss_vowel': 0.258675, 'loss_consonant': 0.173773}\n",
      "###>>>>> saved\n",
      "  109 | 0.000066 | 160512/160635 | 3.6082 | 3.6683 |\n",
      "val: {'recall': 0.982155, 'recall_grapheme': 0.973855, 'recall_vowel': 0.990474, 'recall_consonant': 0.990437, 'acc_grapheme': 0.971944, 'acc_vowel': 0.991096, 'acc_consonant': 0.991021, 'loss_grapheme': 0.375686, 'loss_vowel': 0.259588, 'loss_consonant': 0.175819}\n",
      "  110 | 0.000063 | 160512/160635 | 1.3075 | 3.5317 |\n",
      "val: {'recall': 0.98234, 'recall_grapheme': 0.973974, 'recall_vowel': 0.990776, 'recall_consonant': 0.990638, 'acc_grapheme': 0.972814, 'acc_vowel': 0.99122, 'acc_consonant': 0.991096, 'loss_grapheme': 0.347599, 'loss_vowel': 0.243724, 'loss_consonant': 0.16603}\n",
      "###>>>>> saved\n",
      "  111 | 0.000060 | 160512/160635 | 4.3718 | 3.6056 |\n",
      "val: {'recall': 0.982212, 'recall_grapheme': 0.973655, 'recall_vowel': 0.990088, 'recall_consonant': 0.99145, 'acc_grapheme': 0.972566, 'acc_vowel': 0.990946, 'acc_consonant': 0.990772, 'loss_grapheme': 0.357686, 'loss_vowel': 0.248701, 'loss_consonant': 0.168705}\n",
      "  112 | 0.000057 | 160512/160635 | 3.0323 | 3.6193 |\n",
      "val: {'recall': 0.98222, 'recall_grapheme': 0.974069, 'recall_vowel': 0.989939, 'recall_consonant': 0.990803, 'acc_grapheme': 0.972217, 'acc_vowel': 0.990872, 'acc_consonant': 0.991021, 'loss_grapheme': 0.35404, 'loss_vowel': 0.243539, 'loss_consonant': 0.167564}\n",
      "  113 | 0.000054 | 160512/160635 | 3.5754 | 3.5713 |\n",
      "val: {'recall': 0.9821, 'recall_grapheme': 0.973935, 'recall_vowel': 0.989915, 'recall_consonant': 0.990613, 'acc_grapheme': 0.97259, 'acc_vowel': 0.990922, 'acc_consonant': 0.99117, 'loss_grapheme': 0.338511, 'loss_vowel': 0.237164, 'loss_consonant': 0.162276}\n",
      "  114 | 0.000051 | 160512/160635 | 4.5816 | 3.5875 |\n",
      "val: {'recall': 0.982165, 'recall_grapheme': 0.973714, 'recall_vowel': 0.990223, 'recall_consonant': 0.991008, 'acc_grapheme': 0.972416, 'acc_vowel': 0.990723, 'acc_consonant': 0.991096, 'loss_grapheme': 0.35838, 'loss_vowel': 0.249509, 'loss_consonant': 0.169649}\n",
      "  115 | 0.000049 | 160512/160635 | 1.7240 | 3.6368 |\n",
      "val: {'recall': 0.982471, 'recall_grapheme': 0.973887, 'recall_vowel': 0.990935, 'recall_consonant': 0.991176, 'acc_grapheme': 0.972665, 'acc_vowel': 0.991419, 'acc_consonant': 0.991195, 'loss_grapheme': 0.363103, 'loss_vowel': 0.253117, 'loss_consonant': 0.170661}\n",
      "###>>>>> saved\n",
      "  116 | 0.000046 | 160512/160635 | 4.6379 | 3.7335 |\n",
      "val: {'recall': 0.98239, 'recall_grapheme': 0.974285, 'recall_vowel': 0.990649, 'recall_consonant': 0.990343, 'acc_grapheme': 0.972889, 'acc_vowel': 0.99122, 'acc_consonant': 0.990996, 'loss_grapheme': 0.371271, 'loss_vowel': 0.256375, 'loss_consonant': 0.171761}\n",
      "  117 | 0.000043 | 160512/160635 | 4.5638 | 3.7590 |\n",
      "val: {'recall': 0.982627, 'recall_grapheme': 0.974249, 'recall_vowel': 0.990826, 'recall_consonant': 0.991183, 'acc_grapheme': 0.972914, 'acc_vowel': 0.991245, 'acc_consonant': 0.991195, 'loss_grapheme': 0.388436, 'loss_vowel': 0.267832, 'loss_consonant': 0.181281}\n",
      "###>>>>> saved\n",
      "  118 | 0.000041 | 160512/160635 | 4.9056 | 3.6699 |\n",
      "val: {'recall': 0.982567, 'recall_grapheme': 0.974556, 'recall_vowel': 0.990594, 'recall_consonant': 0.990563, 'acc_grapheme': 0.972964, 'acc_vowel': 0.991344, 'acc_consonant': 0.99122, 'loss_grapheme': 0.37021, 'loss_vowel': 0.258603, 'loss_consonant': 0.175735}\n",
      "  119 | 0.000038 | 160512/160635 | 4.3247 | 3.7098 |\n",
      "val: {'recall': 0.982559, 'recall_grapheme': 0.974292, 'recall_vowel': 0.990822, 'recall_consonant': 0.99083, 'acc_grapheme': 0.973013, 'acc_vowel': 0.991444, 'acc_consonant': 0.991121, 'loss_grapheme': 0.371685, 'loss_vowel': 0.257961, 'loss_consonant': 0.175747}\n",
      "  120 | 0.000036 | 160512/160635 | 3.7141 | 3.6776 |\n",
      "val: {'recall': 0.982276, 'recall_grapheme': 0.974328, 'recall_vowel': 0.990149, 'recall_consonant': 0.990301, 'acc_grapheme': 0.97274, 'acc_vowel': 0.990747, 'acc_consonant': 0.990946, 'loss_grapheme': 0.365718, 'loss_vowel': 0.255811, 'loss_consonant': 0.172725}\n",
      "  121 | 0.000033 | 160512/160635 | 5.0585 | 3.7346 |\n",
      "val: {'recall': 0.982517, 'recall_grapheme': 0.974206, 'recall_vowel': 0.990756, 'recall_consonant': 0.990898, 'acc_grapheme': 0.972939, 'acc_vowel': 0.99127, 'acc_consonant': 0.99122, 'loss_grapheme': 0.367451, 'loss_vowel': 0.257671, 'loss_consonant': 0.174149}\n",
      "  122 | 0.000031 | 160512/160635 | 2.8927 | 3.6502 |\n",
      "val: {'recall': 0.98276, 'recall_grapheme': 0.974698, 'recall_vowel': 0.990481, 'recall_consonant': 0.991161, 'acc_grapheme': 0.973063, 'acc_vowel': 0.991295, 'acc_consonant': 0.991319, 'loss_grapheme': 0.371428, 'loss_vowel': 0.260092, 'loss_consonant': 0.175442}\n",
      "###>>>>> saved\n",
      "  123 | 0.000029 | 160512/160635 | 1.9381 | 3.6838 |\n",
      "val: {'recall': 0.982278, 'recall_grapheme': 0.973874, 'recall_vowel': 0.990305, 'recall_consonant': 0.991058, 'acc_grapheme': 0.972789, 'acc_vowel': 0.991046, 'acc_consonant': 0.991145, 'loss_grapheme': 0.375393, 'loss_vowel': 0.261878, 'loss_consonant': 0.176187}\n",
      "  124 | 0.000027 | 160512/160635 | 4.0465 | 3.6021 |\n",
      "val: {'recall': 0.982627, 'recall_grapheme': 0.974576, 'recall_vowel': 0.990231, 'recall_consonant': 0.991125, 'acc_grapheme': 0.973088, 'acc_vowel': 0.990897, 'acc_consonant': 0.991319, 'loss_grapheme': 0.36331, 'loss_vowel': 0.249005, 'loss_consonant': 0.170128}\n",
      "  125 | 0.000025 | 160512/160635 | 2.0352 | 3.5913 |\n",
      "val: {'recall': 0.982423, 'recall_grapheme': 0.974017, 'recall_vowel': 0.990704, 'recall_consonant': 0.990955, 'acc_grapheme': 0.972988, 'acc_vowel': 0.991344, 'acc_consonant': 0.991295, 'loss_grapheme': 0.348343, 'loss_vowel': 0.242414, 'loss_consonant': 0.165241}\n",
      "  126 | 0.000023 | 160512/160635 | 4.7486 | 3.6465 |\n",
      "val: {'recall': 0.982557, 'recall_grapheme': 0.974405, 'recall_vowel': 0.990497, 'recall_consonant': 0.990922, 'acc_grapheme': 0.973337, 'acc_vowel': 0.99127, 'acc_consonant': 0.991071, 'loss_grapheme': 0.365323, 'loss_vowel': 0.254836, 'loss_consonant': 0.173975}\n",
      "  127 | 0.000021 | 160512/160635 | 4.4836 | 3.6748 |\n",
      "val: {'recall': 0.982571, 'recall_grapheme': 0.974241, 'recall_vowel': 0.990681, 'recall_consonant': 0.991122, 'acc_grapheme': 0.972864, 'acc_vowel': 0.991245, 'acc_consonant': 0.991394, 'loss_grapheme': 0.368974, 'loss_vowel': 0.255548, 'loss_consonant': 0.173381}\n",
      "  128 | 0.000019 | 160512/160635 | 2.5028 | 3.6719 |\n",
      "val: {'recall': 0.982676, 'recall_grapheme': 0.974554, 'recall_vowel': 0.990542, 'recall_consonant': 0.991053, 'acc_grapheme': 0.973088, 'acc_vowel': 0.991245, 'acc_consonant': 0.991394, 'loss_grapheme': 0.366347, 'loss_vowel': 0.253044, 'loss_consonant': 0.173309}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  129 | 0.000017 | 160512/160635 | 3.5663 | 3.7424 |\n",
      "val: {'recall': 0.982811, 'recall_grapheme': 0.974709, 'recall_vowel': 0.990576, 'recall_consonant': 0.991251, 'acc_grapheme': 0.973063, 'acc_vowel': 0.991145, 'acc_consonant': 0.991444, 'loss_grapheme': 0.379844, 'loss_vowel': 0.263531, 'loss_consonant': 0.179277}\n",
      "###>>>>> saved\n",
      "  130 | 0.000016 | 160512/160635 | 4.0012 | 3.7222 |\n",
      "val: {'recall': 0.982696, 'recall_grapheme': 0.974535, 'recall_vowel': 0.9907, 'recall_consonant': 0.991012, 'acc_grapheme': 0.972839, 'acc_vowel': 0.991319, 'acc_consonant': 0.991344, 'loss_grapheme': 0.374718, 'loss_vowel': 0.261527, 'loss_consonant': 0.177817}\n",
      "  131 | 0.000014 | 160512/160635 | 4.2659 | 3.5768 |\n",
      "val: {'recall': 0.982558, 'recall_grapheme': 0.974399, 'recall_vowel': 0.990448, 'recall_consonant': 0.990986, 'acc_grapheme': 0.973063, 'acc_vowel': 0.99117, 'acc_consonant': 0.991319, 'loss_grapheme': 0.354992, 'loss_vowel': 0.247703, 'loss_consonant': 0.169273}\n",
      "  132 | 0.000013 | 160512/160635 | 4.3281 | 3.5660 |\n",
      "val: {'recall': 0.982766, 'recall_grapheme': 0.974743, 'recall_vowel': 0.990627, 'recall_consonant': 0.99095, 'acc_grapheme': 0.973262, 'acc_vowel': 0.99127, 'acc_consonant': 0.991369, 'loss_grapheme': 0.357039, 'loss_vowel': 0.24855, 'loss_consonant': 0.1696}\n",
      "  133 | 0.000011 | 160512/160635 | 3.4858 | 3.5405 |\n",
      "val: {'recall': 0.982681, 'recall_grapheme': 0.974668, 'recall_vowel': 0.990516, 'recall_consonant': 0.990871, 'acc_grapheme': 0.973262, 'acc_vowel': 0.991295, 'acc_consonant': 0.991319, 'loss_grapheme': 0.342833, 'loss_vowel': 0.238394, 'loss_consonant': 0.163853}\n",
      "  134 | 0.000010 | 160512/160635 | 3.3665 | 3.6732 |\n",
      "val: {'recall': 0.982826, 'recall_grapheme': 0.974853, 'recall_vowel': 0.990506, 'recall_consonant': 0.99109, 'acc_grapheme': 0.973138, 'acc_vowel': 0.991046, 'acc_consonant': 0.991319, 'loss_grapheme': 0.371822, 'loss_vowel': 0.259844, 'loss_consonant': 0.175669}\n",
      "###>>>>> saved\n",
      "  135 | 0.000009 | 160512/160635 | 2.6310 | 3.6231 |\n",
      "val: {'recall': 0.982694, 'recall_grapheme': 0.974602, 'recall_vowel': 0.990443, 'recall_consonant': 0.991127, 'acc_grapheme': 0.973163, 'acc_vowel': 0.991121, 'acc_consonant': 0.991344, 'loss_grapheme': 0.360379, 'loss_vowel': 0.251659, 'loss_consonant': 0.171235}\n",
      "  136 | 0.000007 | 160512/160635 | 4.6647 | 3.6450 |\n",
      "val: {'recall': 0.982755, 'recall_grapheme': 0.974676, 'recall_vowel': 0.990494, 'recall_consonant': 0.991174, 'acc_grapheme': 0.973088, 'acc_vowel': 0.99122, 'acc_consonant': 0.991394, 'loss_grapheme': 0.37753, 'loss_vowel': 0.263521, 'loss_consonant': 0.177461}\n",
      "  137 | 0.000006 | 160512/160635 | 4.7367 | 3.6230 |\n",
      "val: {'recall': 0.982865, 'recall_grapheme': 0.974952, 'recall_vowel': 0.990608, 'recall_consonant': 0.990948, 'acc_grapheme': 0.973138, 'acc_vowel': 0.991245, 'acc_consonant': 0.991295, 'loss_grapheme': 0.366202, 'loss_vowel': 0.254249, 'loss_consonant': 0.173052}\n",
      "###>>>>> saved\n",
      "  138 | 0.000005 | 160512/160635 | 2.3342 | 3.7434 |\n",
      "val: {'recall': 0.982677, 'recall_grapheme': 0.974636, 'recall_vowel': 0.990384, 'recall_consonant': 0.991053, 'acc_grapheme': 0.973187, 'acc_vowel': 0.991071, 'acc_consonant': 0.99127, 'loss_grapheme': 0.373267, 'loss_vowel': 0.260195, 'loss_consonant': 0.176336}\n",
      "  139 | 0.000004 | 160512/160635 | 4.9166 | 3.6859 |\n",
      "val: {'recall': 0.982701, 'recall_grapheme': 0.97478, 'recall_vowel': 0.990331, 'recall_consonant': 0.990912, 'acc_grapheme': 0.973187, 'acc_vowel': 0.991046, 'acc_consonant': 0.991245, 'loss_grapheme': 0.369931, 'loss_vowel': 0.25644, 'loss_consonant': 0.174405}\n",
      "  140 | 0.000004 | 160512/160635 | 2.4719 | 3.6992 |\n",
      "val: {'recall': 0.982709, 'recall_grapheme': 0.974704, 'recall_vowel': 0.99049, 'recall_consonant': 0.990941, 'acc_grapheme': 0.973138, 'acc_vowel': 0.99117, 'acc_consonant': 0.991145, 'loss_grapheme': 0.370137, 'loss_vowel': 0.256907, 'loss_consonant': 0.175733}\n",
      "  141 | 0.000003 | 160512/160635 | 4.1828 | 3.6308 |\n",
      "val: {'recall': 0.982782, 'recall_grapheme': 0.974868, 'recall_vowel': 0.99049, 'recall_consonant': 0.990903, 'acc_grapheme': 0.973237, 'acc_vowel': 0.99117, 'acc_consonant': 0.99122, 'loss_grapheme': 0.363814, 'loss_vowel': 0.251331, 'loss_consonant': 0.171719}\n",
      "  142 | 0.000002 | 160512/160635 | 4.1032 | 3.6095 |\n",
      "val: {'recall': 0.982744, 'recall_grapheme': 0.974824, 'recall_vowel': 0.990427, 'recall_consonant': 0.9909, 'acc_grapheme': 0.973262, 'acc_vowel': 0.99122, 'acc_consonant': 0.991195, 'loss_grapheme': 0.363755, 'loss_vowel': 0.253909, 'loss_consonant': 0.172781}\n",
      "  143 | 0.000002 | 160512/160635 | 2.8900 | 3.5969 |\n",
      "val: {'recall': 0.982816, 'recall_grapheme': 0.974907, 'recall_vowel': 0.990487, 'recall_consonant': 0.990964, 'acc_grapheme': 0.973362, 'acc_vowel': 0.991195, 'acc_consonant': 0.991245, 'loss_grapheme': 0.362451, 'loss_vowel': 0.252847, 'loss_consonant': 0.171517}\n",
      "  144 | 0.000001 | 160512/160635 | 3.2566 | 3.5126 |\n",
      "val: {'recall': 0.982731, 'recall_grapheme': 0.974723, 'recall_vowel': 0.990549, 'recall_consonant': 0.990928, 'acc_grapheme': 0.973262, 'acc_vowel': 0.991195, 'acc_consonant': 0.991344, 'loss_grapheme': 0.342743, 'loss_vowel': 0.238173, 'loss_consonant': 0.163548}\n",
      "  145 | 0.000001 | 160512/160635 | 4.3872 | 3.7731 |\n",
      "val: {'recall': 0.98274, 'recall_grapheme': 0.974784, 'recall_vowel': 0.990405, 'recall_consonant': 0.990989, 'acc_grapheme': 0.973337, 'acc_vowel': 0.991021, 'acc_consonant': 0.991245, 'loss_grapheme': 0.392394, 'loss_vowel': 0.271974, 'loss_consonant': 0.184389}\n",
      "  146 | 0.000000 | 160512/160635 | 1.3390 | 3.5219 |\n",
      "val: {'recall': 0.982716, 'recall_grapheme': 0.974759, 'recall_vowel': 0.990491, 'recall_consonant': 0.990853, 'acc_grapheme': 0.973362, 'acc_vowel': 0.991195, 'acc_consonant': 0.991245, 'loss_grapheme': 0.349743, 'loss_vowel': 0.24567, 'loss_consonant': 0.167739}\n",
      "  147 | 0.000000 | 160512/160635 | 3.7916 | 3.5799 |\n",
      "val: {'recall': 0.982787, 'recall_grapheme': 0.974822, 'recall_vowel': 0.990526, 'recall_consonant': 0.99098, 'acc_grapheme': 0.973362, 'acc_vowel': 0.991195, 'acc_consonant': 0.991295, 'loss_grapheme': 0.36451, 'loss_vowel': 0.254434, 'loss_consonant': 0.173201}\n",
      "  148 | 0.000000 | 160512/160635 | 2.8331 | 3.4904 |\n",
      "val: {'recall': 0.982707, 'recall_grapheme': 0.974797, 'recall_vowel': 0.990481, 'recall_consonant': 0.990754, 'acc_grapheme': 0.973237, 'acc_vowel': 0.991245, 'acc_consonant': 0.991319, 'loss_grapheme': 0.341931, 'loss_vowel': 0.237883, 'loss_consonant': 0.162625}\n",
      "  149 | 0.000000 | 160512/160635 | 4.2330 | 3.7256 |\n",
      "val: {'recall': 0.982787, 'recall_grapheme': 0.974741, 'recall_vowel': 0.990584, 'recall_consonant': 0.991084, 'acc_grapheme': 0.973212, 'acc_vowel': 0.991195, 'acc_consonant': 0.991295, 'loss_grapheme': 0.376426, 'loss_vowel': 0.263004, 'loss_consonant': 0.177867}\n",
      "CYCLE: 2\n",
      "{'recall': 0.982787, 'recall_grapheme': 0.974741, 'recall_vowel': 0.990584, 'recall_consonant': 0.991084, 'acc_grapheme': 0.973212, 'acc_vowel': 0.991195, 'acc_consonant': 0.991295, 'loss_grapheme': 0.376426, 'loss_vowel': 0.263004, 'loss_consonant': 0.177867}\n",
      "    0 | 0.000040 | 160512/160635 | 2.7910 | 3.6101 |\n",
      "val: {'recall': 0.982716, 'recall_grapheme': 0.974621, 'recall_vowel': 0.99069, 'recall_consonant': 0.990933, 'acc_grapheme': 0.973362, 'acc_vowel': 0.991245, 'acc_consonant': 0.99122, 'loss_grapheme': 0.364084, 'loss_vowel': 0.254208, 'loss_consonant': 0.171619}\n",
      "    1 | 0.000080 | 160512/160635 | 1.8589 | 3.6711 |\n",
      "val: {'recall': 0.982458, 'recall_grapheme': 0.973684, 'recall_vowel': 0.991032, 'recall_consonant': 0.991432, 'acc_grapheme': 0.97274, 'acc_vowel': 0.991195, 'acc_consonant': 0.99127, 'loss_grapheme': 0.368134, 'loss_vowel': 0.255543, 'loss_consonant': 0.173207}\n",
      "    2 | 0.000120 | 160512/160635 | 2.2728 | 3.6400 |\n",
      "val: {'recall': 0.982197, 'recall_grapheme': 0.974224, 'recall_vowel': 0.990514, 'recall_consonant': 0.989827, 'acc_grapheme': 0.972988, 'acc_vowel': 0.991096, 'acc_consonant': 0.991121, 'loss_grapheme': 0.372397, 'loss_vowel': 0.252463, 'loss_consonant': 0.174579}\n",
      "    3 | 0.000160 | 160512/160635 | 2.7643 | 3.6397 |\n",
      "val: {'recall': 0.982436, 'recall_grapheme': 0.974863, 'recall_vowel': 0.989786, 'recall_consonant': 0.99023, 'acc_grapheme': 0.973262, 'acc_vowel': 0.991046, 'acc_consonant': 0.991096, 'loss_grapheme': 0.363679, 'loss_vowel': 0.250499, 'loss_consonant': 0.168285}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    4 | 0.000199 | 160512/160635 | 4.2866 | 3.5615 |\n",
      "val: {'recall': 0.982602, 'recall_grapheme': 0.974245, 'recall_vowel': 0.990443, 'recall_consonant': 0.991477, 'acc_grapheme': 0.972143, 'acc_vowel': 0.991096, 'acc_consonant': 0.991245, 'loss_grapheme': 0.373254, 'loss_vowel': 0.255696, 'loss_consonant': 0.173489}\n",
      "    5 | 0.000239 | 160512/160635 | 2.2307 | 3.5325 |\n",
      "val: {'recall': 0.981962, 'recall_grapheme': 0.974145, 'recall_vowel': 0.989869, 'recall_consonant': 0.989691, 'acc_grapheme': 0.97259, 'acc_vowel': 0.991046, 'acc_consonant': 0.991145, 'loss_grapheme': 0.33888, 'loss_vowel': 0.235884, 'loss_consonant': 0.162801}\n",
      "    6 | 0.000278 | 160512/160635 | 4.5518 | 3.6522 |\n",
      "val: {'recall': 0.982194, 'recall_grapheme': 0.974562, 'recall_vowel': 0.989694, 'recall_consonant': 0.989956, 'acc_grapheme': 0.97259, 'acc_vowel': 0.990698, 'acc_consonant': 0.990573, 'loss_grapheme': 0.386066, 'loss_vowel': 0.264915, 'loss_consonant': 0.182362}\n",
      "    7 | 0.000318 | 160512/160635 | 3.0897 | 3.5515 |\n",
      "val: {'recall': 0.982031, 'recall_grapheme': 0.974294, 'recall_vowel': 0.989693, 'recall_consonant': 0.989844, 'acc_grapheme': 0.972317, 'acc_vowel': 0.990747, 'acc_consonant': 0.990747, 'loss_grapheme': 0.334093, 'loss_vowel': 0.229318, 'loss_consonant': 0.153051}\n",
      "    8 | 0.000357 | 160512/160635 | 3.1638 | 3.7934 |\n",
      "val: {'recall': 0.982129, 'recall_grapheme': 0.974917, 'recall_vowel': 0.988337, 'recall_consonant': 0.990345, 'acc_grapheme': 0.971919, 'acc_vowel': 0.989976, 'acc_consonant': 0.99015, 'loss_grapheme': 0.375061, 'loss_vowel': 0.269428, 'loss_consonant': 0.171768}\n",
      "    9 | 0.000395 | 160512/160635 | 4.6523 | 3.7034 |\n",
      "val: {'recall': 0.98229, 'recall_grapheme': 0.97381, 'recall_vowel': 0.990282, 'recall_consonant': 0.991259, 'acc_grapheme': 0.971969, 'acc_vowel': 0.990399, 'acc_consonant': 0.990723, 'loss_grapheme': 0.367635, 'loss_vowel': 0.273719, 'loss_consonant': 0.171717}\n",
      "   10 | 0.000395 | 160512/160635 | 5.0097 | 3.7573 |\n",
      "val: {'recall': 0.982412, 'recall_grapheme': 0.974748, 'recall_vowel': 0.989028, 'recall_consonant': 0.991123, 'acc_grapheme': 0.97167, 'acc_vowel': 0.990349, 'acc_consonant': 0.991369, 'loss_grapheme': 0.393829, 'loss_vowel': 0.269311, 'loss_consonant': 0.180932}\n",
      "   11 | 0.000394 | 160512/160635 | 1.7383 | 3.7195 |\n",
      "val: {'recall': 0.980933, 'recall_grapheme': 0.972686, 'recall_vowel': 0.988323, 'recall_consonant': 0.990035, 'acc_grapheme': 0.97172, 'acc_vowel': 0.991195, 'acc_consonant': 0.990623, 'loss_grapheme': 0.328079, 'loss_vowel': 0.228158, 'loss_consonant': 0.160923}\n",
      "   12 | 0.000393 | 160512/160635 | 1.5029 | 3.4912 |\n",
      "val: {'recall': 0.982484, 'recall_grapheme': 0.974323, 'recall_vowel': 0.989706, 'recall_consonant': 0.991584, 'acc_grapheme': 0.971645, 'acc_vowel': 0.990922, 'acc_consonant': 0.991046, 'loss_grapheme': 0.376051, 'loss_vowel': 0.246692, 'loss_consonant': 0.164759}\n",
      "   13 | 0.000391 | 160512/160635 | 3.7349 | 3.6785 |\n",
      "val: {'recall': 0.982133, 'recall_grapheme': 0.973968, 'recall_vowel': 0.988343, 'recall_consonant': 0.99225, 'acc_grapheme': 0.97167, 'acc_vowel': 0.990673, 'acc_consonant': 0.990399, 'loss_grapheme': 0.362976, 'loss_vowel': 0.255878, 'loss_consonant': 0.171667}\n",
      "   14 | 0.000390 | 160512/160635 | 3.9643 | 3.7083 |\n",
      "val: {'recall': 0.981995, 'recall_grapheme': 0.973881, 'recall_vowel': 0.989973, 'recall_consonant': 0.990245, 'acc_grapheme': 0.972093, 'acc_vowel': 0.990996, 'acc_consonant': 0.990573, 'loss_grapheme': 0.336013, 'loss_vowel': 0.226101, 'loss_consonant': 0.160943}\n",
      "   15 | 0.000389 | 160512/160635 | 3.1755 | 3.6206 |\n",
      "val: {'recall': 0.981038, 'recall_grapheme': 0.973266, 'recall_vowel': 0.988699, 'recall_consonant': 0.98892, 'acc_grapheme': 0.972541, 'acc_vowel': 0.99025, 'acc_consonant': 0.990797, 'loss_grapheme': 0.367937, 'loss_vowel': 0.272218, 'loss_consonant': 0.173912}\n",
      "   16 | 0.000387 | 160512/160635 | 4.1731 | 3.5724 |\n",
      "val: {'recall': 0.98242, 'recall_grapheme': 0.974664, 'recall_vowel': 0.988919, 'recall_consonant': 0.991432, 'acc_grapheme': 0.972441, 'acc_vowel': 0.990847, 'acc_consonant': 0.990996, 'loss_grapheme': 0.357854, 'loss_vowel': 0.244508, 'loss_consonant': 0.166521}\n",
      "   17 | 0.000386 | 160512/160635 | 4.9639 | 3.6478 |\n",
      "val: {'recall': 0.982732, 'recall_grapheme': 0.974986, 'recall_vowel': 0.990313, 'recall_consonant': 0.990642, 'acc_grapheme': 0.973884, 'acc_vowel': 0.991046, 'acc_consonant': 0.991046, 'loss_grapheme': 0.334126, 'loss_vowel': 0.233913, 'loss_consonant': 0.160756}\n",
      "   18 | 0.000384 | 160512/160635 | 4.1630 | 3.5659 |\n",
      "val: {'recall': 0.982926, 'recall_grapheme': 0.97499, 'recall_vowel': 0.989792, 'recall_consonant': 0.991931, 'acc_grapheme': 0.974108, 'acc_vowel': 0.990573, 'acc_consonant': 0.990797, 'loss_grapheme': 0.364571, 'loss_vowel': 0.252165, 'loss_consonant': 0.169037}\n",
      "###>>>>> saved\n",
      "   19 | 0.000383 | 160512/160635 | 2.7817 | 3.6542 |\n",
      "val: {'recall': 0.982334, 'recall_grapheme': 0.974443, 'recall_vowel': 0.990127, 'recall_consonant': 0.990322, 'acc_grapheme': 0.973958, 'acc_vowel': 0.990673, 'acc_consonant': 0.991145, 'loss_grapheme': 0.326698, 'loss_vowel': 0.22533, 'loss_consonant': 0.154658}\n",
      "   20 | 0.000381 | 160512/160635 | 2.5426 | 3.5756 |\n",
      "val: {'recall': 0.983397, 'recall_grapheme': 0.976006, 'recall_vowel': 0.990361, 'recall_consonant': 0.991215, 'acc_grapheme': 0.973511, 'acc_vowel': 0.990797, 'acc_consonant': 0.99117, 'loss_grapheme': 0.371059, 'loss_vowel': 0.26695, 'loss_consonant': 0.175222}\n",
      "###>>>>> saved\n",
      "   21 | 0.000379 | 160512/160635 | 2.0087 | 3.6438 |\n",
      "val: {'recall': 0.983585, 'recall_grapheme': 0.976122, 'recall_vowel': 0.991012, 'recall_consonant': 0.991085, 'acc_grapheme': 0.975078, 'acc_vowel': 0.991494, 'acc_consonant': 0.991469, 'loss_grapheme': 0.335062, 'loss_vowel': 0.237557, 'loss_consonant': 0.16235}\n",
      "###>>>>> saved\n",
      "   22 | 0.000377 | 160512/160635 | 4.4440 | 3.6379 |\n",
      "val: {'recall': 0.982769, 'recall_grapheme': 0.975452, 'recall_vowel': 0.990134, 'recall_consonant': 0.990036, 'acc_grapheme': 0.973212, 'acc_vowel': 0.991394, 'acc_consonant': 0.991693, 'loss_grapheme': 0.345249, 'loss_vowel': 0.250039, 'loss_consonant': 0.170027}\n",
      "   23 | 0.000375 | 160512/160635 | 3.8920 | 3.4976 |\n",
      "val: {'recall': 0.981415, 'recall_grapheme': 0.972491, 'recall_vowel': 0.989073, 'recall_consonant': 0.991605, 'acc_grapheme': 0.971844, 'acc_vowel': 0.991369, 'acc_consonant': 0.990822, 'loss_grapheme': 0.364749, 'loss_vowel': 0.255789, 'loss_consonant': 0.169476}\n",
      "   24 | 0.000373 | 160512/160635 | 4.4177 | 3.5804 |\n",
      "val: {'recall': 0.982662, 'recall_grapheme': 0.975356, 'recall_vowel': 0.9897, 'recall_consonant': 0.990236, 'acc_grapheme': 0.973958, 'acc_vowel': 0.990673, 'acc_consonant': 0.990897, 'loss_grapheme': 0.348548, 'loss_vowel': 0.247217, 'loss_consonant': 0.168807}\n",
      "   25 | 0.000371 | 160512/160635 | 3.2900 | 3.6350 |\n",
      "val: {'recall': 0.984055, 'recall_grapheme': 0.976877, 'recall_vowel': 0.990352, 'recall_consonant': 0.992115, 'acc_grapheme': 0.974282, 'acc_vowel': 0.991245, 'acc_consonant': 0.990847, 'loss_grapheme': 0.409979, 'loss_vowel': 0.28777, 'loss_consonant': 0.187397}\n",
      "###>>>>> saved\n",
      "   26 | 0.000369 | 160512/160635 | 4.6736 | 3.6020 |\n",
      "val: {'recall': 0.984093, 'recall_grapheme': 0.977299, 'recall_vowel': 0.989935, 'recall_consonant': 0.99184, 'acc_grapheme': 0.975227, 'acc_vowel': 0.991369, 'acc_consonant': 0.991245, 'loss_grapheme': 0.336705, 'loss_vowel': 0.243452, 'loss_consonant': 0.165872}\n",
      "###>>>>> saved\n",
      "   27 | 0.000367 | 160512/160635 | 4.9257 | 3.6178 |\n",
      "val: {'recall': 0.984639, 'recall_grapheme': 0.977768, 'recall_vowel': 0.990255, 'recall_consonant': 0.992764, 'acc_grapheme': 0.975874, 'acc_vowel': 0.991319, 'acc_consonant': 0.99117, 'loss_grapheme': 0.395239, 'loss_vowel': 0.282525, 'loss_consonant': 0.188866}\n",
      "###>>>>> saved\n",
      "   28 | 0.000364 | 160512/160635 | 2.7592 | 3.6655 |\n",
      "val: {'recall': 0.983421, 'recall_grapheme': 0.977203, 'recall_vowel': 0.989418, 'recall_consonant': 0.989861, 'acc_grapheme': 0.974456, 'acc_vowel': 0.989802, 'acc_consonant': 0.991046, 'loss_grapheme': 0.339845, 'loss_vowel': 0.248719, 'loss_consonant': 0.168719}\n",
      "   29 | 0.000362 | 160512/160635 | 2.9163 | 3.6158 |\n",
      "val: {'recall': 0.984544, 'recall_grapheme': 0.977978, 'recall_vowel': 0.990481, 'recall_consonant': 0.991738, 'acc_grapheme': 0.975426, 'acc_vowel': 0.991369, 'acc_consonant': 0.991593, 'loss_grapheme': 0.352725, 'loss_vowel': 0.247701, 'loss_consonant': 0.168015}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   30 | 0.000361 | 072960/160635 | 3.0609 | 3.6239 |\n",
      "val: {'recall': 0.983607, 'recall_grapheme': 0.976195, 'recall_vowel': 0.990136, 'recall_consonant': 0.991902, 'acc_grapheme': 0.975152, 'acc_vowel': 0.991444, 'acc_consonant': 0.991518, 'loss_grapheme': 0.355737, 'loss_vowel': 0.262766, 'loss_consonant': 0.177989}\n",
      "   31 | 0.000357 | 160512/160635 | 3.3371 | 3.6396 |\n",
      "val: {'recall': 0.984323, 'recall_grapheme': 0.977224, 'recall_vowel': 0.991192, 'recall_consonant': 0.991653, 'acc_grapheme': 0.975575, 'acc_vowel': 0.991742, 'acc_consonant': 0.991916, 'loss_grapheme': 0.364106, 'loss_vowel': 0.251111, 'loss_consonant': 0.172879}\n",
      "   32 | 0.000354 | 160512/160635 | 4.4661 | 3.4681 |\n",
      "val: {'recall': 0.984107, 'recall_grapheme': 0.977871, 'recall_vowel': 0.990204, 'recall_consonant': 0.990481, 'acc_grapheme': 0.975376, 'acc_vowel': 0.991145, 'acc_consonant': 0.991419, 'loss_grapheme': 0.324085, 'loss_vowel': 0.22759, 'loss_consonant': 0.16003}\n",
      "   33 | 0.000351 | 160512/160635 | 3.8224 | 3.5344 |\n",
      "val: {'recall': 0.984555, 'recall_grapheme': 0.978392, 'recall_vowel': 0.991082, 'recall_consonant': 0.990353, 'acc_grapheme': 0.976222, 'acc_vowel': 0.991693, 'acc_consonant': 0.991593, 'loss_grapheme': 0.312551, 'loss_vowel': 0.224641, 'loss_consonant': 0.151955}\n",
      "   34 | 0.000349 | 160512/160635 | 1.7070 | 3.5282 |\n",
      "val: {'recall': 0.984009, 'recall_grapheme': 0.977949, 'recall_vowel': 0.989564, 'recall_consonant': 0.990575, 'acc_grapheme': 0.975351, 'acc_vowel': 0.991394, 'acc_consonant': 0.991717, 'loss_grapheme': 0.346846, 'loss_vowel': 0.248936, 'loss_consonant': 0.162349}\n",
      "   35 | 0.000346 | 160512/160635 | 3.7773 | 3.5773 |\n",
      "val: {'recall': 0.982574, 'recall_grapheme': 0.975687, 'recall_vowel': 0.989556, 'recall_consonant': 0.989365, 'acc_grapheme': 0.975103, 'acc_vowel': 0.990946, 'acc_consonant': 0.991195, 'loss_grapheme': 0.321669, 'loss_vowel': 0.232389, 'loss_consonant': 0.156327}\n",
      "   36 | 0.000343 | 160512/160635 | 2.0058 | 3.5455 |\n",
      "val: {'recall': 0.985064, 'recall_grapheme': 0.978543, 'recall_vowel': 0.991292, 'recall_consonant': 0.991879, 'acc_grapheme': 0.976421, 'acc_vowel': 0.992016, 'acc_consonant': 0.991668, 'loss_grapheme': 0.34413, 'loss_vowel': 0.255986, 'loss_consonant': 0.170641}\n",
      "###>>>>> saved\n",
      "   37 | 0.000340 | 160512/160635 | 1.9139 | 3.5134 |\n",
      "val: {'recall': 0.983132, 'recall_grapheme': 0.976702, 'recall_vowel': 0.989527, 'recall_consonant': 0.989595, 'acc_grapheme': 0.975177, 'acc_vowel': 0.989951, 'acc_consonant': 0.990822, 'loss_grapheme': 0.32828, 'loss_vowel': 0.237921, 'loss_consonant': 0.15907}\n",
      "   38 | 0.000337 | 160512/160635 | 4.5069 | 3.5543 |\n",
      "val: {'recall': 0.984797, 'recall_grapheme': 0.979399, 'recall_vowel': 0.989945, 'recall_consonant': 0.990447, 'acc_grapheme': 0.977515, 'acc_vowel': 0.991568, 'acc_consonant': 0.991643, 'loss_grapheme': 0.333533, 'loss_vowel': 0.242853, 'loss_consonant': 0.161884}\n",
      "   39 | 0.000334 | 160512/160635 | 3.9239 | 3.5020 |\n",
      "val: {'recall': 0.983588, 'recall_grapheme': 0.975448, 'recall_vowel': 0.991609, 'recall_consonant': 0.991847, 'acc_grapheme': 0.975948, 'acc_vowel': 0.991618, 'acc_consonant': 0.991742, 'loss_grapheme': 0.314308, 'loss_vowel': 0.230676, 'loss_consonant': 0.161546}\n",
      "   40 | 0.000331 | 160512/160635 | 4.4986 | 3.5388 |\n",
      "val: {'recall': 0.984789, 'recall_grapheme': 0.978877, 'recall_vowel': 0.990483, 'recall_consonant': 0.990917, 'acc_grapheme': 0.977515, 'acc_vowel': 0.991792, 'acc_consonant': 0.99224, 'loss_grapheme': 0.34688, 'loss_vowel': 0.242029, 'loss_consonant': 0.169723}\n",
      "   41 | 0.000328 | 160512/160635 | 4.7654 | 3.5143 |\n",
      "val: {'recall': 0.985088, 'recall_grapheme': 0.978334, 'recall_vowel': 0.991648, 'recall_consonant': 0.992037, 'acc_grapheme': 0.976471, 'acc_vowel': 0.991817, 'acc_consonant': 0.99224, 'loss_grapheme': 0.338179, 'loss_vowel': 0.235885, 'loss_consonant': 0.167597}\n",
      "###>>>>> saved\n",
      "   42 | 0.000324 | 160512/160635 | 4.8345 | 3.3845 |\n",
      "val: {'recall': 0.984925, 'recall_grapheme': 0.978478, 'recall_vowel': 0.99157, 'recall_consonant': 0.991174, 'acc_grapheme': 0.977615, 'acc_vowel': 0.992215, 'acc_consonant': 0.992314, 'loss_grapheme': 0.319953, 'loss_vowel': 0.228211, 'loss_consonant': 0.153235}\n",
      "   43 | 0.000321 | 160512/160635 | 2.1409 | 3.5644 |\n",
      "val: {'recall': 0.985235, 'recall_grapheme': 0.979341, 'recall_vowel': 0.991574, 'recall_consonant': 0.990683, 'acc_grapheme': 0.977167, 'acc_vowel': 0.991742, 'acc_consonant': 0.992091, 'loss_grapheme': 0.346398, 'loss_vowel': 0.251361, 'loss_consonant': 0.171087}\n",
      "###>>>>> saved\n",
      "   44 | 0.000318 | 160512/160635 | 3.5544 | 3.4747 |\n",
      "val: {'recall': 0.985413, 'recall_grapheme': 0.979878, 'recall_vowel': 0.991338, 'recall_consonant': 0.990559, 'acc_grapheme': 0.977615, 'acc_vowel': 0.99224, 'acc_consonant': 0.99219, 'loss_grapheme': 0.296983, 'loss_vowel': 0.21631, 'loss_consonant': 0.1508}\n",
      "###>>>>> saved\n",
      "   45 | 0.000314 | 160512/160635 | 3.2284 | 3.5511 |\n",
      "val: {'recall': 0.985774, 'recall_grapheme': 0.979845, 'recall_vowel': 0.991446, 'recall_consonant': 0.991961, 'acc_grapheme': 0.978535, 'acc_vowel': 0.992464, 'acc_consonant': 0.992314, 'loss_grapheme': 0.339838, 'loss_vowel': 0.255752, 'loss_consonant': 0.168449}\n",
      "###>>>>> saved\n",
      "   46 | 0.000311 | 160512/160635 | 2.4835 | 3.4514 |\n",
      "val: {'recall': 0.985613, 'recall_grapheme': 0.979254, 'recall_vowel': 0.991504, 'recall_consonant': 0.99244, 'acc_grapheme': 0.977267, 'acc_vowel': 0.992339, 'acc_consonant': 0.992488, 'loss_grapheme': 0.307357, 'loss_vowel': 0.220341, 'loss_consonant': 0.152874}\n",
      "   47 | 0.000307 | 160512/160635 | 3.2206 | 3.4421 |\n",
      "val: {'recall': 0.984189, 'recall_grapheme': 0.977639, 'recall_vowel': 0.991357, 'recall_consonant': 0.990121, 'acc_grapheme': 0.976545, 'acc_vowel': 0.99122, 'acc_consonant': 0.991319, 'loss_grapheme': 0.325938, 'loss_vowel': 0.236302, 'loss_consonant': 0.159062}\n",
      "   48 | 0.000304 | 160512/160635 | 4.5603 | 3.5117 |\n",
      "val: {'recall': 0.985524, 'recall_grapheme': 0.979253, 'recall_vowel': 0.991882, 'recall_consonant': 0.991707, 'acc_grapheme': 0.978485, 'acc_vowel': 0.992513, 'acc_consonant': 0.992314, 'loss_grapheme': 0.313114, 'loss_vowel': 0.229676, 'loss_consonant': 0.156025}\n",
      "   49 | 0.000300 | 160512/160635 | 3.7271 | 3.5777 |\n",
      "val: {'recall': 0.986119, 'recall_grapheme': 0.980304, 'recall_vowel': 0.992091, 'recall_consonant': 0.991775, 'acc_grapheme': 0.978659, 'acc_vowel': 0.992563, 'acc_consonant': 0.992439, 'loss_grapheme': 0.335279, 'loss_vowel': 0.245047, 'loss_consonant': 0.164062}\n",
      "###>>>>> saved\n",
      "   50 | 0.000296 | 160512/160635 | 4.4064 | 3.4433 |\n",
      "val: {'recall': 0.985824, 'recall_grapheme': 0.979936, 'recall_vowel': 0.991242, 'recall_consonant': 0.992182, 'acc_grapheme': 0.978734, 'acc_vowel': 0.992439, 'acc_consonant': 0.992588, 'loss_grapheme': 0.298871, 'loss_vowel': 0.219751, 'loss_consonant': 0.148876}\n",
      "   51 | 0.000293 | 160512/160635 | 4.3021 | 3.5151 |\n",
      "val: {'recall': 0.985057, 'recall_grapheme': 0.978555, 'recall_vowel': 0.991286, 'recall_consonant': 0.991829, 'acc_grapheme': 0.977068, 'acc_vowel': 0.992339, 'acc_consonant': 0.992016, 'loss_grapheme': 0.356619, 'loss_vowel': 0.253674, 'loss_consonant': 0.174931}\n",
      "   52 | 0.000289 | 160512/160635 | 4.1828 | 3.4062 |\n",
      "val: {'recall': 0.985892, 'recall_grapheme': 0.980241, 'recall_vowel': 0.990898, 'recall_consonant': 0.992187, 'acc_grapheme': 0.978858, 'acc_vowel': 0.992414, 'acc_consonant': 0.992339, 'loss_grapheme': 0.305219, 'loss_vowel': 0.226029, 'loss_consonant': 0.149699}\n",
      "   53 | 0.000285 | 160512/160635 | 3.1033 | 3.4817 |\n",
      "val: {'recall': 0.986441, 'recall_grapheme': 0.980673, 'recall_vowel': 0.992252, 'recall_consonant': 0.992167, 'acc_grapheme': 0.978784, 'acc_vowel': 0.992538, 'acc_consonant': 0.992265, 'loss_grapheme': 0.311602, 'loss_vowel': 0.237331, 'loss_consonant': 0.155269}\n",
      "###>>>>> saved\n",
      "   54 | 0.000281 | 160512/160635 | 4.4836 | 3.4503 |\n",
      "val: {'recall': 0.98582, 'recall_grapheme': 0.980539, 'recall_vowel': 0.991264, 'recall_consonant': 0.990937, 'acc_grapheme': 0.978361, 'acc_vowel': 0.991643, 'acc_consonant': 0.991916, 'loss_grapheme': 0.315706, 'loss_vowel': 0.24394, 'loss_consonant': 0.160773}\n",
      "   55 | 0.000278 | 160512/160635 | 1.8401 | 3.4746 |\n",
      "val: {'recall': 0.986283, 'recall_grapheme': 0.980693, 'recall_vowel': 0.991799, 'recall_consonant': 0.991946, 'acc_grapheme': 0.978311, 'acc_vowel': 0.99219, 'acc_consonant': 0.991767, 'loss_grapheme': 0.315049, 'loss_vowel': 0.231854, 'loss_consonant': 0.155342}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   56 | 0.000274 | 160512/160635 | 1.9706 | 3.4794 |\n",
      "val: {'recall': 0.986499, 'recall_grapheme': 0.980921, 'recall_vowel': 0.992193, 'recall_consonant': 0.991961, 'acc_grapheme': 0.978883, 'acc_vowel': 0.992513, 'acc_consonant': 0.992638, 'loss_grapheme': 0.340048, 'loss_vowel': 0.251965, 'loss_consonant': 0.168291}\n",
      "###>>>>> saved\n",
      "   57 | 0.000270 | 160512/160635 | 3.4429 | 3.4568 |\n",
      "val: {'recall': 0.985924, 'recall_grapheme': 0.980309, 'recall_vowel': 0.990998, 'recall_consonant': 0.99208, 'acc_grapheme': 0.978436, 'acc_vowel': 0.991892, 'acc_consonant': 0.992464, 'loss_grapheme': 0.294088, 'loss_vowel': 0.215891, 'loss_consonant': 0.149896}\n",
      "   58 | 0.000266 | 160512/160635 | 4.1137 | 3.3725 |\n",
      "val: {'recall': 0.985482, 'recall_grapheme': 0.979865, 'recall_vowel': 0.99074, 'recall_consonant': 0.991456, 'acc_grapheme': 0.978361, 'acc_vowel': 0.991419, 'acc_consonant': 0.991295, 'loss_grapheme': 0.320197, 'loss_vowel': 0.225581, 'loss_consonant': 0.153715}\n",
      "   59 | 0.000262 | 160512/160635 | 2.7965 | 3.4729 |\n",
      "val: {'recall': 0.986096, 'recall_grapheme': 0.980411, 'recall_vowel': 0.991325, 'recall_consonant': 0.992239, 'acc_grapheme': 0.978809, 'acc_vowel': 0.99229, 'acc_consonant': 0.992439, 'loss_grapheme': 0.339987, 'loss_vowel': 0.254695, 'loss_consonant': 0.166677}\n",
      "   60 | 0.000258 | 160512/160635 | 3.6780 | 3.5010 |\n",
      "val: {'recall': 0.986457, 'recall_grapheme': 0.981444, 'recall_vowel': 0.991969, 'recall_consonant': 0.990972, 'acc_grapheme': 0.979306, 'acc_vowel': 0.992538, 'acc_consonant': 0.992613, 'loss_grapheme': 0.341955, 'loss_vowel': 0.246431, 'loss_consonant': 0.168146}\n",
      "   61 | 0.000254 | 160512/160635 | 2.8104 | 3.4361 |\n",
      "val: {'recall': 0.986474, 'recall_grapheme': 0.981131, 'recall_vowel': 0.991343, 'recall_consonant': 0.992292, 'acc_grapheme': 0.97948, 'acc_vowel': 0.992961, 'acc_consonant': 0.992613, 'loss_grapheme': 0.307136, 'loss_vowel': 0.225009, 'loss_consonant': 0.158197}\n",
      "   62 | 0.000250 | 160512/160635 | 4.3938 | 3.4947 |\n",
      "val: {'recall': 0.986452, 'recall_grapheme': 0.981269, 'recall_vowel': 0.991441, 'recall_consonant': 0.991827, 'acc_grapheme': 0.979679, 'acc_vowel': 0.992439, 'acc_consonant': 0.992488, 'loss_grapheme': 0.316824, 'loss_vowel': 0.236891, 'loss_consonant': 0.162974}\n",
      "   63 | 0.000246 | 160512/160635 | 2.1743 | 3.4271 |\n",
      "val: {'recall': 0.98647, 'recall_grapheme': 0.981073, 'recall_vowel': 0.991635, 'recall_consonant': 0.992097, 'acc_grapheme': 0.979605, 'acc_vowel': 0.992513, 'acc_consonant': 0.992613, 'loss_grapheme': 0.28327, 'loss_vowel': 0.215368, 'loss_consonant': 0.147036}\n",
      "   64 | 0.000242 | 160512/160635 | 4.1431 | 3.4868 |\n",
      "val: {'recall': 0.987134, 'recall_grapheme': 0.981725, 'recall_vowel': 0.992477, 'recall_consonant': 0.99261, 'acc_grapheme': 0.980525, 'acc_vowel': 0.992886, 'acc_consonant': 0.992389, 'loss_grapheme': 0.299182, 'loss_vowel': 0.224788, 'loss_consonant': 0.149591}\n",
      "###>>>>> saved\n",
      "   65 | 0.000237 | 160512/160635 | 2.6163 | 3.5539 |\n",
      "val: {'recall': 0.986099, 'recall_grapheme': 0.98077, 'recall_vowel': 0.991642, 'recall_consonant': 0.991213, 'acc_grapheme': 0.979406, 'acc_vowel': 0.992638, 'acc_consonant': 0.992787, 'loss_grapheme': 0.301489, 'loss_vowel': 0.235809, 'loss_consonant': 0.157463}\n",
      "   66 | 0.000233 | 160512/160635 | 3.9283 | 3.4770 |\n",
      "val: {'recall': 0.986248, 'recall_grapheme': 0.980238, 'recall_vowel': 0.992121, 'recall_consonant': 0.992396, 'acc_grapheme': 0.979207, 'acc_vowel': 0.992936, 'acc_consonant': 0.992886, 'loss_grapheme': 0.321393, 'loss_vowel': 0.236846, 'loss_consonant': 0.161203}\n",
      "   67 | 0.000229 | 160512/160635 | 4.3839 | 3.4786 |\n",
      "val: {'recall': 0.986595, 'recall_grapheme': 0.98125, 'recall_vowel': 0.991644, 'recall_consonant': 0.992237, 'acc_grapheme': 0.979779, 'acc_vowel': 0.992638, 'acc_consonant': 0.992638, 'loss_grapheme': 0.326554, 'loss_vowel': 0.239753, 'loss_consonant': 0.16291}\n",
      "   68 | 0.000225 | 160512/160635 | 3.7129 | 3.3390 |\n",
      "val: {'recall': 0.987613, 'recall_grapheme': 0.98241, 'recall_vowel': 0.992718, 'recall_consonant': 0.992915, 'acc_grapheme': 0.980425, 'acc_vowel': 0.992936, 'acc_consonant': 0.993085, 'loss_grapheme': 0.295304, 'loss_vowel': 0.219499, 'loss_consonant': 0.148051}\n",
      "###>>>>> saved\n",
      "   69 | 0.000221 | 160512/160635 | 4.0666 | 3.4381 |\n",
      "val: {'recall': 0.986527, 'recall_grapheme': 0.981113, 'recall_vowel': 0.99185, 'recall_consonant': 0.992029, 'acc_grapheme': 0.979878, 'acc_vowel': 0.992737, 'acc_consonant': 0.992862, 'loss_grapheme': 0.29414, 'loss_vowel': 0.226796, 'loss_consonant': 0.154575}\n",
      "   70 | 0.000217 | 160512/160635 | 3.4001 | 3.4306 |\n",
      "val: {'recall': 0.986453, 'recall_grapheme': 0.980816, 'recall_vowel': 0.991616, 'recall_consonant': 0.992566, 'acc_grapheme': 0.979704, 'acc_vowel': 0.992414, 'acc_consonant': 0.992762, 'loss_grapheme': 0.322891, 'loss_vowel': 0.236472, 'loss_consonant': 0.166483}\n",
      "   71 | 0.000213 | 160512/160635 | 4.1309 | 3.4732 |\n",
      "val: {'recall': 0.986863, 'recall_grapheme': 0.981574, 'recall_vowel': 0.991701, 'recall_consonant': 0.992604, 'acc_grapheme': 0.980152, 'acc_vowel': 0.992687, 'acc_consonant': 0.992837, 'loss_grapheme': 0.326374, 'loss_vowel': 0.245427, 'loss_consonant': 0.165688}\n",
      "   72 | 0.000208 | 160512/160635 | 3.0719 | 3.5738 |\n",
      "val: {'recall': 0.986288, 'recall_grapheme': 0.981353, 'recall_vowel': 0.990482, 'recall_consonant': 0.991962, 'acc_grapheme': 0.979107, 'acc_vowel': 0.991121, 'acc_consonant': 0.991892, 'loss_grapheme': 0.320587, 'loss_vowel': 0.246282, 'loss_consonant': 0.165876}\n",
      "   73 | 0.000204 | 160512/160635 | 2.8958 | 3.3799 |\n",
      "val: {'recall': 0.986858, 'recall_grapheme': 0.981695, 'recall_vowel': 0.991987, 'recall_consonant': 0.992057, 'acc_grapheme': 0.980301, 'acc_vowel': 0.992762, 'acc_consonant': 0.992812, 'loss_grapheme': 0.289265, 'loss_vowel': 0.215628, 'loss_consonant': 0.150883}\n",
      "   74 | 0.000200 | 160512/160635 | 4.6633 | 3.3905 |\n",
      "val: {'recall': 0.987473, 'recall_grapheme': 0.982412, 'recall_vowel': 0.992323, 'recall_consonant': 0.992744, 'acc_grapheme': 0.980823, 'acc_vowel': 0.992886, 'acc_consonant': 0.99316, 'loss_grapheme': 0.304021, 'loss_vowel': 0.217445, 'loss_consonant': 0.152734}\n",
      "   75 | 0.000196 | 160512/160635 | 4.5687 | 3.4578 |\n",
      "val: {'recall': 0.986072, 'recall_grapheme': 0.980778, 'recall_vowel': 0.990883, 'recall_consonant': 0.991851, 'acc_grapheme': 0.979704, 'acc_vowel': 0.992488, 'acc_consonant': 0.992737, 'loss_grapheme': 0.298311, 'loss_vowel': 0.222411, 'loss_consonant': 0.148293}\n",
      "   76 | 0.000192 | 160512/160635 | 3.4151 | 3.4458 |\n",
      "val: {'recall': 0.98717, 'recall_grapheme': 0.981982, 'recall_vowel': 0.99202, 'recall_consonant': 0.992695, 'acc_grapheme': 0.980102, 'acc_vowel': 0.992837, 'acc_consonant': 0.993235, 'loss_grapheme': 0.316861, 'loss_vowel': 0.240969, 'loss_consonant': 0.158998}\n",
      "   77 | 0.000187 | 160512/160635 | 4.1057 | 3.3339 |\n",
      "val: {'recall': 0.987189, 'recall_grapheme': 0.982665, 'recall_vowel': 0.991629, 'recall_consonant': 0.991795, 'acc_grapheme': 0.980898, 'acc_vowel': 0.992588, 'acc_consonant': 0.992936, 'loss_grapheme': 0.295132, 'loss_vowel': 0.225507, 'loss_consonant': 0.150001}\n",
      "   78 | 0.000183 | 160512/160635 | 3.9894 | 3.4749 |\n",
      "val: {'recall': 0.987474, 'recall_grapheme': 0.982372, 'recall_vowel': 0.992702, 'recall_consonant': 0.992448, 'acc_grapheme': 0.980649, 'acc_vowel': 0.993185, 'acc_consonant': 0.992886, 'loss_grapheme': 0.314472, 'loss_vowel': 0.23577, 'loss_consonant': 0.157586}\n",
      "   79 | 0.000179 | 160512/160635 | 3.0186 | 3.3306 |\n",
      "val: {'recall': 0.986707, 'recall_grapheme': 0.981319, 'recall_vowel': 0.992206, 'recall_consonant': 0.991982, 'acc_grapheme': 0.980077, 'acc_vowel': 0.993061, 'acc_consonant': 0.992886, 'loss_grapheme': 0.285013, 'loss_vowel': 0.219976, 'loss_consonant': 0.145664}\n",
      "   80 | 0.000175 | 160512/160635 | 1.9818 | 3.3588 |\n",
      "val: {'recall': 0.987129, 'recall_grapheme': 0.982842, 'recall_vowel': 0.991727, 'recall_consonant': 0.991104, 'acc_grapheme': 0.980699, 'acc_vowel': 0.992663, 'acc_consonant': 0.992812, 'loss_grapheme': 0.301982, 'loss_vowel': 0.226508, 'loss_consonant': 0.153141}\n",
      "   81 | 0.000171 | 160512/160635 | 2.7724 | 3.3755 |\n",
      "val: {'recall': 0.987386, 'recall_grapheme': 0.982895, 'recall_vowel': 0.992073, 'recall_consonant': 0.99168, 'acc_grapheme': 0.980823, 'acc_vowel': 0.992936, 'acc_consonant': 0.992812, 'loss_grapheme': 0.307345, 'loss_vowel': 0.229712, 'loss_consonant': 0.155244}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   82 | 0.000167 | 160512/160635 | 4.1373 | 3.3973 |\n",
      "val: {'recall': 0.987853, 'recall_grapheme': 0.983068, 'recall_vowel': 0.992725, 'recall_consonant': 0.992551, 'acc_grapheme': 0.981122, 'acc_vowel': 0.992936, 'acc_consonant': 0.992986, 'loss_grapheme': 0.327791, 'loss_vowel': 0.247594, 'loss_consonant': 0.162827}\n",
      "###>>>>> saved\n",
      "   83 | 0.000163 | 160512/160635 | 3.6404 | 3.3588 |\n",
      "val: {'recall': 0.987524, 'recall_grapheme': 0.982307, 'recall_vowel': 0.992643, 'recall_consonant': 0.992842, 'acc_grapheme': 0.980774, 'acc_vowel': 0.993235, 'acc_consonant': 0.993085, 'loss_grapheme': 0.29357, 'loss_vowel': 0.221547, 'loss_consonant': 0.150074}\n",
      "   84 | 0.000158 | 160512/160635 | 3.0715 | 3.3186 |\n",
      "val: {'recall': 0.987242, 'recall_grapheme': 0.982246, 'recall_vowel': 0.991801, 'recall_consonant': 0.992676, 'acc_grapheme': 0.980301, 'acc_vowel': 0.992663, 'acc_consonant': 0.992862, 'loss_grapheme': 0.300316, 'loss_vowel': 0.226131, 'loss_consonant': 0.154012}\n",
      "   85 | 0.000154 | 160512/160635 | 3.6272 | 3.4082 |\n",
      "val: {'recall': 0.987219, 'recall_grapheme': 0.981989, 'recall_vowel': 0.992158, 'recall_consonant': 0.992741, 'acc_grapheme': 0.980923, 'acc_vowel': 0.992737, 'acc_consonant': 0.992936, 'loss_grapheme': 0.300255, 'loss_vowel': 0.227325, 'loss_consonant': 0.154416}\n",
      "   86 | 0.000150 | 160512/160635 | 3.3580 | 3.4050 |\n",
      "val: {'recall': 0.987731, 'recall_grapheme': 0.983114, 'recall_vowel': 0.99245, 'recall_consonant': 0.992245, 'acc_grapheme': 0.981122, 'acc_vowel': 0.993185, 'acc_consonant': 0.993135, 'loss_grapheme': 0.298186, 'loss_vowel': 0.225713, 'loss_consonant': 0.155222}\n",
      "   87 | 0.000146 | 160512/160635 | 2.1697 | 3.3750 |\n",
      "val: {'recall': 0.986985, 'recall_grapheme': 0.981746, 'recall_vowel': 0.991522, 'recall_consonant': 0.992928, 'acc_grapheme': 0.980624, 'acc_vowel': 0.992613, 'acc_consonant': 0.992862, 'loss_grapheme': 0.2981, 'loss_vowel': 0.227876, 'loss_consonant': 0.154427}\n",
      "   88 | 0.000142 | 160512/160635 | 3.7569 | 3.4590 |\n",
      "val: {'recall': 0.987547, 'recall_grapheme': 0.982984, 'recall_vowel': 0.99169, 'recall_consonant': 0.99253, 'acc_grapheme': 0.981271, 'acc_vowel': 0.992961, 'acc_consonant': 0.993185, 'loss_grapheme': 0.312114, 'loss_vowel': 0.236915, 'loss_consonant': 0.163889}\n",
      "   89 | 0.000138 | 160512/160635 | 3.3428 | 3.2314 |\n",
      "val: {'recall': 0.986862, 'recall_grapheme': 0.981686, 'recall_vowel': 0.991486, 'recall_consonant': 0.992592, 'acc_grapheme': 0.980127, 'acc_vowel': 0.993061, 'acc_consonant': 0.992911, 'loss_grapheme': 0.270721, 'loss_vowel': 0.19779, 'loss_consonant': 0.136014}\n",
      "   90 | 0.000134 | 160512/160635 | 3.4152 | 3.4404 |\n",
      "val: {'recall': 0.987873, 'recall_grapheme': 0.9833, 'recall_vowel': 0.992513, 'recall_consonant': 0.99238, 'acc_grapheme': 0.981196, 'acc_vowel': 0.993508, 'acc_consonant': 0.993334, 'loss_grapheme': 0.298439, 'loss_vowel': 0.228885, 'loss_consonant': 0.152739}\n",
      "###>>>>> saved\n",
      "   91 | 0.000130 | 160512/160635 | 2.0785 | 3.4313 |\n",
      "val: {'recall': 0.987591, 'recall_grapheme': 0.983031, 'recall_vowel': 0.992288, 'recall_consonant': 0.992015, 'acc_grapheme': 0.981296, 'acc_vowel': 0.993384, 'acc_consonant': 0.992787, 'loss_grapheme': 0.285004, 'loss_vowel': 0.220929, 'loss_consonant': 0.14814}\n",
      "   92 | 0.000126 | 160512/160635 | 2.9954 | 3.3559 |\n",
      "val: {'recall': 0.987594, 'recall_grapheme': 0.982755, 'recall_vowel': 0.992405, 'recall_consonant': 0.992462, 'acc_grapheme': 0.981047, 'acc_vowel': 0.993309, 'acc_consonant': 0.99321, 'loss_grapheme': 0.299894, 'loss_vowel': 0.227758, 'loss_consonant': 0.155816}\n",
      "   93 | 0.000123 | 160512/160635 | 4.0007 | 3.3433 |\n",
      "val: {'recall': 0.987673, 'recall_grapheme': 0.98277, 'recall_vowel': 0.992492, 'recall_consonant': 0.992662, 'acc_grapheme': 0.981122, 'acc_vowel': 0.993284, 'acc_consonant': 0.993185, 'loss_grapheme': 0.285727, 'loss_vowel': 0.218448, 'loss_consonant': 0.150117}\n",
      "   94 | 0.000119 | 160512/160635 | 4.5166 | 3.4873 |\n",
      "val: {'recall': 0.988029, 'recall_grapheme': 0.983331, 'recall_vowel': 0.992858, 'recall_consonant': 0.992596, 'acc_grapheme': 0.981221, 'acc_vowel': 0.993508, 'acc_consonant': 0.99311, 'loss_grapheme': 0.320202, 'loss_vowel': 0.241189, 'loss_consonant': 0.165}\n",
      "###>>>>> saved\n",
      "   95 | 0.000115 | 160512/160635 | 2.9960 | 3.2791 |\n",
      "val: {'recall': 0.98806, 'recall_grapheme': 0.983498, 'recall_vowel': 0.992771, 'recall_consonant': 0.992473, 'acc_grapheme': 0.981768, 'acc_vowel': 0.993658, 'acc_consonant': 0.993359, 'loss_grapheme': 0.2751, 'loss_vowel': 0.206608, 'loss_consonant': 0.142549}\n",
      "###>>>>> saved\n",
      "   96 | 0.000111 | 160512/160635 | 1.6954 | 3.3553 |\n",
      "val: {'recall': 0.987332, 'recall_grapheme': 0.982485, 'recall_vowel': 0.991931, 'recall_consonant': 0.992428, 'acc_grapheme': 0.980425, 'acc_vowel': 0.992687, 'acc_consonant': 0.992837, 'loss_grapheme': 0.29474, 'loss_vowel': 0.22352, 'loss_consonant': 0.151905}\n",
      "   97 | 0.000107 | 160512/160635 | 2.7480 | 3.3578 |\n",
      "val: {'recall': 0.987622, 'recall_grapheme': 0.982847, 'recall_vowel': 0.99235, 'recall_consonant': 0.992445, 'acc_grapheme': 0.981395, 'acc_vowel': 0.993284, 'acc_consonant': 0.993284, 'loss_grapheme': 0.292825, 'loss_vowel': 0.224843, 'loss_consonant': 0.151744}\n",
      "   98 | 0.000104 | 160512/160635 | 4.0540 | 3.3584 |\n",
      "val: {'recall': 0.987368, 'recall_grapheme': 0.982739, 'recall_vowel': 0.992041, 'recall_consonant': 0.991953, 'acc_grapheme': 0.98147, 'acc_vowel': 0.992911, 'acc_consonant': 0.99311, 'loss_grapheme': 0.28463, 'loss_vowel': 0.218324, 'loss_consonant': 0.148296}\n",
      "   99 | 0.000103 | 049920/160635 | 3.8061 | 3.4215 |"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-6df1386d1bbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-6e909f510157>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcycle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_cycles\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CYCLE:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtrain_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         scheduler = WarmupCyclicalLR(\n\u001b[1;32m     15\u001b[0m             \"cos\", args.base_lr, args.num_epochs, iters_per_epoch=len(train_loader), warmup_epochs=args.warmup_epochs)\n",
      "\u001b[0;32m<ipython-input-33-6e909f510157>\u001b[0m in \u001b[0;36mtrain_cycle\u001b[0;34m(args, model, optimizer, lr_scheduler)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0;31m#loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(args, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_model(model, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
