{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time, gc\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pretrainedmodels\n",
    "from argparse import Namespace\n",
    "from sklearn.utils import shuffle\n",
    "from apex import amp\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from cvcore.data.auto_augment import RandAugment\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_map.csv\t\t       train.csv\r\n",
      "sample_submission.csv\t       train.csv.zip\r\n",
      "test.csv\t\t       train_image_data_0.parquet\r\n",
      "test_image_data_0.parquet      train_image_data_0.parquet.zip\r\n",
      "test_image_data_0.parquet.zip  train_image_data_1.parquet\r\n",
      "test_image_data_1.parquet      train_image_data_1.parquet.zip\r\n",
      "test_image_data_1.parquet.zip  train_image_data_2.parquet\r\n",
      "test_image_data_2.parquet      train_image_data_2.parquet.zip\r\n",
      "test_image_data_2.parquet.zip  train_image_data_3.parquet\r\n",
      "test_image_data_3.parquet      train_image_data_3.parquet.zip\r\n",
      "test_image_data_3.parquet.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/chec/data/bengali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /home/chec/data/bengali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/chec/data/bengali'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "class_map_df = pd.read_csv(f'{DATA_DIR}/class_map.csv')\n",
    "sample_sub_df = pd.read_csv(f'{DATA_DIR}/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>ক্ট্রো</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_1</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>হ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>খ্রী</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_3</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>র্টি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_4</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>থ্রো</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  grapheme_root  vowel_diacritic  consonant_diacritic grapheme\n",
       "0  Train_0             15                9                    5   ক্ট্রো\n",
       "1  Train_1            159                0                    0        হ\n",
       "2  Train_2             22                3                    5     খ্রী\n",
       "3  Train_3             53                2                    2     র্টি\n",
       "4  Train_4             71                9                    5     থ্রো"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 137\n",
    "WIDTH = 236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import albumentations as albu\n",
    "def get_train_augs():\n",
    "    return RandAugment(n=2, m=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.arange(10).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class BengaliDataset(Dataset):\n",
    "    def __init__(self, df, img_df, train_mode=True, test_mode=False):\n",
    "        self.df = df\n",
    "        self.img_df = img_df\n",
    "        self.train_mode = train_mode\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = self.get_img(row.image_id)\n",
    "        orig_img = img.copy()\n",
    "        #print(img.shape)\n",
    "        if self.train_mode:\n",
    "            augs = get_train_augs()\n",
    "            #img = augs(image=img)['image']\n",
    "            img = np.asarray(augs(Image.fromarray(img)))\n",
    "        \n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "        orig_img = np.expand_dims(orig_img, axis=-1)\n",
    "        \n",
    "        #print('###', img.shape)\n",
    "        #img = np.concatenate([img, img, img], 2)\n",
    "        #print('>>>', img.shape)\n",
    "        \n",
    "        # taken from https://www.kaggle.com/iafoss/image-preprocessing-128x128\n",
    "        #MEAN = [ 0.06922848809290576,  0.06922848809290576,  0.06922848809290576]\n",
    "        #STD = [ 0.20515700083327537,  0.20515700083327537,  0.20515700083327537]\n",
    "        \n",
    "        img = transforms.functional.to_tensor(img)\n",
    "        orig_img = transforms.functional.to_tensor(orig_img)\n",
    "        \n",
    "        #img = transforms.functional.normalize(img, mean=MEAN, std=STD)\n",
    "        \n",
    "        if self.test_mode:\n",
    "            return img\n",
    "        elif self.train_mode:\n",
    "            return img, orig_img, torch.tensor([row.grapheme_root, row.vowel_diacritic, row.consonant_diacritic, row.word_label])\n",
    "        else:\n",
    "            return img, torch.tensor([row.grapheme_root, row.vowel_diacritic, row.consonant_diacritic, row.word_label])\n",
    "                    \n",
    "    def get_img(self, img_id):\n",
    "        return 255 - self.img_df.loc[img_id].values.reshape(HEIGHT, WIDTH).astype(np.uint8)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "def get_train_val_loaders(batch_size=4, val_batch_size=4, ifold=0, dev_mode=False):\n",
    "    train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "\n",
    "    train_df = shuffle(train_df, random_state=1234)\n",
    "\n",
    "    grapheme_words = np.unique(train_df.grapheme.values)\n",
    "    grapheme_words_dict = {grapheme: i for i, grapheme in enumerate(grapheme_words)}\n",
    "    train_df['word_label'] = train_df['grapheme'].map(lambda x: grapheme_words_dict[x])\n",
    "\n",
    "    print(train_df.shape)\n",
    "\n",
    "    if dev_mode:\n",
    "        img_df = pd.read_parquet(f'{DATA_DIR}/train_image_data_0.parquet').set_index('image_id')\n",
    "        train_df = train_df.iloc[:1000]\n",
    "    else:\n",
    "        img_dfs = [pd.read_parquet(f'{DATA_DIR}/train_image_data_{i}.parquet') for i in range(4)]\n",
    "        img_df = pd.concat(img_dfs, axis=0).set_index('image_id')\n",
    "    print(img_df.shape)\n",
    "    #split_index = int(len(train_df) * 0.9)\n",
    "    \n",
    "    #train = train_df.iloc[:split_index]\n",
    "    #val = train_df.iloc[split_index:]\n",
    "    \n",
    "    kf = StratifiedKFold(5, random_state=1234, shuffle=True)\n",
    "    for i, (train_idx, val_idx) in enumerate(kf.split(train_df, train_df['grapheme_root'].values)):\n",
    "        if i == ifold:\n",
    "            #print(val_idx)\n",
    "            train = train_df.iloc[train_idx]\n",
    "            val = train_df.iloc[val_idx]\n",
    "            break\n",
    "    assert i == ifold\n",
    "    print(train.shape, val.shape)\n",
    "    \n",
    "    train_ds = BengaliDataset(train, img_df, True, False)\n",
    "    val_ds = BengaliDataset(val, img_df, False, False)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "    train_loader.num = len(train_ds)\n",
    "\n",
    "    val_loader = DataLoader(val_ds, batch_size=val_batch_size, shuffle=False, num_workers=8, drop_last=False)\n",
    "    val_loader.num = len(val_ds)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader, val_loader = get_train_val_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x in train_loader:\n",
    "#    print(x)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'resnet50' # could be fbresnet152 or inceptionresnetv2\n",
    "#model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet').cuda()\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import timm\n",
    "from timm.models.activations import Swish, Mish\n",
    "from timm.models.adaptive_avgmax_pool import SelectAdaptivePool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = [ 0.06922848809290576 ]\n",
    "STD = [ 0.20515700083327537 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = [ 0.06922848809290576 ]\n",
    "STD = [ 0.20515700083327537 ]\n",
    "import timm\n",
    "from timm.models.activations import Swish, Mish\n",
    "from timm.models.adaptive_avgmax_pool import SelectAdaptivePool2d\n",
    "\n",
    "class BengaliNet4(nn.Module):\n",
    "    def __init__(self, backbone_name='se_resnext101_32x4d'):\n",
    "        super(BengaliNet4, self).__init__()\n",
    "        self.n_grapheme = 168\n",
    "        self.n_vowel = 11\n",
    "        self.n_consonant = 7\n",
    "        self.n_word = 1295\n",
    "        self.backbone_name = backbone_name\n",
    "        \n",
    "        self.num_classes = self.n_grapheme + self.n_vowel + self.n_consonant + self.n_word\n",
    "        \n",
    "        self.backbone = pretrainedmodels.__dict__[self.backbone_name](num_classes=1000, pretrained='imagenet')\n",
    "        self.fc = nn.Linear(self.backbone.last_linear.in_features, self.num_classes)\n",
    "        \n",
    "        self.num_p2_features = self.backbone.layer2[-1].se_module.fc2.out_channels\n",
    "        self.num_p3_features = self.backbone.layer3[-1].se_module.fc2.out_channels\n",
    "        self.p2_head = nn.Conv2d(self.num_p2_features, self.num_p2_features * 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.p3_head = nn.Conv2d(self.num_p3_features, self.num_p3_features * 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.num_p2_features * 4)\n",
    "        self.bn3 = nn.BatchNorm2d(self.num_p3_features * 4)\n",
    "        self.act2 = Swish()\n",
    "        self.act3 = Swish()\n",
    "        \n",
    "        self.fc_aux1 = nn.Linear(self.num_p3_features * 4, self.num_classes)\n",
    "        self.fc_aux2 = nn.Linear(self.num_p2_features * 4, self.num_classes)\n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        for fc in [self.fc, self.fc_aux1, self.fc_aux2]:\n",
    "            nn.init.zeros_(fc.bias.data)\n",
    "\n",
    "        print('init model4')\n",
    "        \n",
    "    def features(self, x):\n",
    "        x = self.backbone.layer0(x); #print(x.size())\n",
    "        x = self.backbone.layer1(x); #print(x.size())\n",
    "        x = self.backbone.layer2(x); p2 = x; p2 = self.p2_head(p2); p2 = self.bn2(p2); p2 = self.act2(p2) #print(x.size())\n",
    "        x = self.backbone.layer3(x); p3 = x; p3 = self.p3_head(p3); p3 = self.bn3(p3); p3 = self.act3(p3) #print(x.size())\n",
    "        x = self.backbone.layer4(x); #print(x.size())\n",
    "        return x, p2, p3\n",
    "        \n",
    "    def logits(self, x, p2, p3):\n",
    "        x = self.avg_pool(x)\n",
    "        #x = F.dropout2d(x, 0.2, self.training)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        p2 = self.avg_pool(p2)\n",
    "        p2 = torch.flatten(p2, 1)\n",
    "        \n",
    "        p3 = self.avg_pool(p3)\n",
    "        p3 = torch.flatten(p3, 1)\n",
    "        return self.fc(x), self.fc_aux1(p3), self.fc_aux2(p2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, size=(224,224), mode='bilinear', align_corners=False)\n",
    "        for i in range(len(x)):\n",
    "            transforms.functional.normalize(x[i], mean=MEAN, std=STD, inplace=True)\n",
    "        x = torch.cat([x,x,x], 1)\n",
    "        #x = self.conv0(x)\n",
    "        #print(x.size())\n",
    "        x, p2, p3 = self.features(x)\n",
    "        x, logits_aux1, logits_aux2 = self.logits(x, p2, p3)\n",
    "\n",
    "        return x, logits_aux1, logits_aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './model4-ckps'\n",
    "def create_model(args):\n",
    "    model = BengaliNet4(args.backbone)\n",
    "    model_file = os.path.join(MODEL_DIR, args.backbone, args.ckp_name)\n",
    "\n",
    "    parent_dir = os.path.dirname(model_file)\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "\n",
    "    print('model file: {}, exist: {}'.format(model_file, os.path.exists(model_file)))\n",
    "\n",
    "    if os.path.exists(model_file):\n",
    "        print('loading {}...'.format(model_file))\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "    \n",
    "    return model, model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bnet = BengaliNet('se_resnext50_32x4d').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bnet(torch.randn((2, 1, 137, 236)).cuda()).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.111111"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(1/9, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "\n",
    "\n",
    "def calc_metrics(preds0, preds1, preds2, preds3, y):\n",
    "    assert len(y) == len(preds0) == len(preds1) == len(preds2) == len(preds3)\n",
    "\n",
    "    recall_grapheme = sklearn.metrics.recall_score(preds0, y[:, 0], average='macro')\n",
    "    recall_vowel = sklearn.metrics.recall_score(preds1, y[:, 1], average='macro')\n",
    "    recall_consonant = sklearn.metrics.recall_score(preds2, y[:, 2], average='macro')\n",
    "    recall_word = sklearn.metrics.recall_score(preds3, y[:, 3], average='macro')\n",
    "    \n",
    "    scores = [recall_grapheme, recall_vowel, recall_consonant]\n",
    "    final_recall_score = np.average(scores, weights=[2, 1, 1])\n",
    "    \n",
    "    metrics = {}\n",
    "    metrics['recall'] = round(final_recall_score, 6)\n",
    "    metrics['recall_grapheme'] = round(recall_grapheme, 6)\n",
    "    metrics['recall_vowel'] = round(recall_vowel, 6)\n",
    "    metrics['recall_consonant'] = round(recall_consonant, 6)\n",
    "    metrics['recall_word'] = round(recall_word, 6)\n",
    "    \n",
    "    metrics['acc_grapheme'] = round((preds0 == y[:, 0]).sum() / len(y), 6)\n",
    "    metrics['acc_vowel'] = round((preds1 == y[:, 1]).sum() / len(y), 6)\n",
    "    metrics['acc_consonant'] = round((preds2 == y[:, 2]).sum() / len(y), 6)\n",
    "    metrics['acc_word'] = round((preds3 == y[:, 3]).sum() / len(y), 6)    \n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(outputs, y_true):\n",
    "    # outputs: (N, 182)\n",
    "    # y_true: (N, 3)\n",
    "    \n",
    "    outputs = torch.split(outputs, [168, 11, 7, 1295], dim=1)\n",
    "    loss0 = F.cross_entropy(outputs[0], y_true[:, 0], reduction='mean')\n",
    "    loss1 = F.cross_entropy(outputs[1], y_true[:, 1], reduction='mean')\n",
    "    loss2 = F.cross_entropy(outputs[2], y_true[:, 2], reduction='mean')\n",
    "    loss3 = F.cross_entropy(outputs[3], y_true[:, 3], reduction='mean')\n",
    "    \n",
    "    return loss0 + loss1 + loss2 + loss3 #, loss0.item(), loss1.item(), loss2.item()\n",
    "    #return loss3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    loss0, loss1, loss2, loss3 = 0., 0., 0., 0.\n",
    "    preds0, preds1, preds2, preds3 = [], [], [], []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            y_true.append(y)\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            outputs, outputs_aux1, outputs_aux2 = model(x)\n",
    "            #avg_outputs = torch.mean(torch.stack([outputs, outputs_aux1, outputs_aux2], 0), 0)\n",
    "            outputs = torch.split(outputs, [168, 11, 7, 1295], dim=1)\n",
    "            \n",
    "            preds0.append(torch.max(outputs[0], dim=1)[1])\n",
    "            preds1.append(torch.max(outputs[1], dim=1)[1])\n",
    "            preds2.append(torch.max(outputs[2], dim=1)[1])\n",
    "            preds3.append(torch.max(outputs[3], dim=1)[1])\n",
    "            loss0 += F.cross_entropy(outputs[0], y[:, 0], reduction='sum').item()\n",
    "            loss1 += F.cross_entropy(outputs[1], y[:, 1], reduction='sum').item()\n",
    "            loss2 += F.cross_entropy(outputs[2], y[:, 2], reduction='sum').item()\n",
    "            loss3 += F.cross_entropy(outputs[3], y[:, 3], reduction='sum').item()\n",
    "            \n",
    "            # for debug\n",
    "            #metrics = {}\n",
    "            #metrics['loss_grapheme'] =  F.cross_entropy(outputs[0], y[:, 0], reduction='mean').item()\n",
    "            #metrics['loss_vowel'] =  F.cross_entropy(outputs[1], y[:, 1], reduction='mean').item()\n",
    "            #metrics['loss_consonant'] =  F.cross_entropy(outputs[2], y[:, 2], reduction='mean').item()\n",
    "            #return metrics\n",
    "    \n",
    "    preds0 = torch.cat(preds0, 0).cpu().numpy()\n",
    "    preds1 = torch.cat(preds1, 0).cpu().numpy()\n",
    "    preds2 = torch.cat(preds2, 0).cpu().numpy()\n",
    "    preds3 = torch.cat(preds3, 0).cpu().numpy()\n",
    "    \n",
    "    y_true = torch.cat(y_true, 0).numpy()\n",
    "    \n",
    "    #print('y_true:', y_true.shape)\n",
    "    #print('preds0:', preds0.shape)\n",
    "    \n",
    "    metrics = calc_metrics(preds0, preds1, preds2, preds3, y_true)\n",
    "    metrics['loss_grapheme'] = round(loss0 / val_loader.num, 6)\n",
    "    metrics['loss_vowel'] = round(loss1 / val_loader.num, 6)\n",
    "    metrics['loss_consonant'] = round(loss2 / val_loader.num, 6)\n",
    "    metrics['loss_word'] = round(loss3 / val_loader.num, 6)\n",
    "    \n",
    "    return metrics\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lrs(optimizer):\n",
    "    lrs = []\n",
    "    for pgs in optimizer.state_dict()['param_groups']:\n",
    "        lrs.append(pgs['lr'])\n",
    "    lrs = ['{:.6f}'.format(x) for x in lrs]\n",
    "    return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_file):\n",
    "    parent_dir = os.path.dirname(model_file)\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        torch.save(model.module.state_dict(), model_file)\n",
    "    else:\n",
    "        torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(data, targets, alpha=1):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets = targets[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    data = data * lam + shuffled_data * (1 - lam)\n",
    "    targets = (targets, shuffled_targets, lam)\n",
    "\n",
    "    return data, targets\n",
    "\n",
    "\n",
    "def mixup_criterion(outputs, targets):\n",
    "    targets1, targets2, lam = targets\n",
    "    #criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    return lam * criterion(outputs, targets1) + (1 - lam) * criterion(outputs, targets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox_old(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    H = size[2]\n",
    "    W = size[3]\n",
    "\n",
    "    x_margin_rate = 0.2\n",
    "\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * (1-x_margin_rate*2) * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "    \n",
    "    min_x_center = np.int(W * x_margin_rate + cut_w / 2)\n",
    "    max_x_center = np.int(W * (1-x_margin_rate) - cut_w / 2)\n",
    "    #print(min_x_center, max_x_center, lam, cut_w)\n",
    "    min_y_center = cut_h // 2\n",
    "    max_y_center = H - cut_h // 2\n",
    "    if max_y_center == min_y_center:\n",
    "        max_y_center += 1\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(min_x_center, max_x_center)\n",
    "    cy = np.random.randint(min_y_center, max_y_center)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    #print(bbx1, bbx2, bby1, bby2)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9607663071143503"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from over9000.over9000 import Over9000\n",
    "from over9000.radam import RAdam\n",
    "from gridmask import GridMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvcore.solver import WarmupCyclicalLR\n",
    "def make_optimizer(model, base_lr=4e-4, weight_decay=0., weight_decay_bias=0., epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    Create optimizer with per-layer learning rate and weight decay.\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    for key, value in model.named_parameters():\n",
    "        if not value.requires_grad:\n",
    "            continue\n",
    "        lr = base_lr\n",
    "        params += [{\"params\": [value], \"lr\": lr, \"weight_decay\": weight_decay_bias if 'bias' in key else weight_decay}]\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(params, lr, eps=epsilon)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metrics = 0.\n",
    "\n",
    "def train(args, model):\n",
    "    optimizer = make_optimizer(model)\n",
    "    scheduler = WarmupCyclicalLR(\n",
    "        \"cos\", args.base_lr, args.num_epochs, iters_per_epoch=len(train_loader), warmup_epochs=args.warmup_epochs)\n",
    "    \n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    for cycle in range(1, args.num_cycles+1):\n",
    "        print('CYCLE:', cycle)\n",
    "        train_cycle(args, model, optimizer, scheduler)\n",
    "        #args.base_lr = 1e-4\n",
    "        #args.num_epochs = 80\n",
    "        args.warmup_epochs = 5\n",
    "        scheduler = WarmupCyclicalLR(\n",
    "            \"cos\", args.base_lr, args.num_epochs, iters_per_epoch=len(train_loader), warmup_epochs=args.warmup_epochs)\n",
    "\n",
    "def train_cycle(args, model, optimizer, lr_scheduler):\n",
    "    global best_metrics\n",
    "    best_key = 'recall'\n",
    "    \n",
    "    val_metrics = validate(model, val_loader)\n",
    "    print(val_metrics)\n",
    "\n",
    "    if val_metrics[best_key] > best_metrics:\n",
    "        best_metrics = val_metrics[best_key]\n",
    "    \n",
    "    model.train()\n",
    "    train_iter = 0\n",
    "    grid = GridMask(64, 128, rotate=15, ratio=0.6, mode=1, prob=1.)\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        grid.set_prob(epoch, args.st_epochs)\n",
    "\n",
    "        train_loss = 0\n",
    "\n",
    "        bg = time.time()\n",
    "        for batch_idx, (img, orig_img, targets) in enumerate(train_loader):\n",
    "            train_iter += 1\n",
    "            img, orig_img, targets  = img.cuda(), orig_img.cuda(), targets.cuda()\n",
    "            batch_size = img.size(0)\n",
    "            r = np.random.rand()\n",
    "\n",
    "            if r < 0.3:\n",
    "                # generate mixed sample\n",
    "                lam = np.random.beta(args.beta, args.beta)\n",
    "                rand_index = torch.randperm(img.size()[0]).cuda()\n",
    "                target_a = targets\n",
    "                target_b = targets[rand_index]\n",
    "                bbx1, bby1, bbx2, bby2 = rand_bbox(img.size(), lam)\n",
    "                #img[:, :, bbx1:bbx2, bby1:bby2] = img[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "                img[:, :, bby1:bby2, bbx1:bbx2] = img[rand_index, :, bby1:bby2, bbx1:bbx2]\n",
    "                \n",
    "                # adjust lambda to exactly match pixel ratio\n",
    "                lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (img.size()[-1] * img.size()[-2]))\n",
    "                # compute output\n",
    "                outputs, outputs_aux1, outputs_aux2 = model(img)\n",
    "                loss_primary = criterion(outputs, target_a) * lam + criterion(outputs, target_b) * (1. - lam)\n",
    "                loss_aux1 = criterion(outputs_aux1, target_a) * lam + criterion(outputs_aux1, target_b) * (1. - lam)\n",
    "                loss_aux2 = criterion(outputs_aux2, target_a) * lam + criterion(outputs_aux2, target_b) * (1. - lam)\n",
    "                loss = loss_primary + loss_aux1*0.9 + loss_aux2*0.8\n",
    "            elif r > 0.7:\n",
    "                img = grid(img)\n",
    "                outputs, outputs_aux1, outputs_aux2 = model(img)\n",
    "                loss_primary = criterion(outputs, targets)\n",
    "                loss_aux1 = criterion(outputs_aux1, targets)\n",
    "                loss_aux2 = criterion(outputs_aux2, targets)\n",
    "                loss = loss_primary + loss_aux1*0.9 + loss_aux2*0.8\n",
    "            else:\n",
    "                orig_img, targets = mixup(orig_img, targets)\n",
    "                outputs, outputs_aux1, outputs_aux2 = model(orig_img)\n",
    "                loss_primary = mixup_criterion(outputs, targets)\n",
    "                loss_aux1 = mixup_criterion(outputs_aux1, targets)\n",
    "                loss_aux2 = mixup_criterion(outputs_aux2, targets)\n",
    "                loss = loss_primary + loss_aux1*0.9 + loss_aux2*0.8\n",
    "                #loss = criterion(outputs, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "            #loss.backward()\n",
    "            lr_scheduler(optimizer, batch_idx, epoch)\n",
    "            optimizer.step()            \n",
    "            \n",
    "            current_lr = get_lrs(optimizer)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            print('\\r {:4d} | {:.6f} | {:06d}/{} | {:.4f} | {:.4f} |'.format(\n",
    "                epoch, float(current_lr[0]), batch_size*(batch_idx+1), train_loader.num, \n",
    "                loss.item(), train_loss/(batch_idx+1)), end='')\n",
    "\n",
    "        if True:#train_iter > 0 and train_iter % args.iter_val == 0:\n",
    "            val_metrics = validate(model, val_loader)\n",
    "            print('\\nval:', val_metrics)\n",
    "                \n",
    "            if val_metrics[best_key] > best_metrics:\n",
    "                best_metrics = val_metrics[best_key]\n",
    "                save_model(model, model_file)\n",
    "                print('###>>>>> saved')\n",
    "                \n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.backbone = 'se_resnext101_32x4d'\n",
    "args.ckp_name = 'model4_se_resnext101_fold3_224.pth'\n",
    "\n",
    "args.base_lr = 2e-4\n",
    "args.num_epochs = 80\n",
    "args.warmup_epochs = 5\n",
    "args.num_cycles = 100\n",
    "args.batch_size = 440\n",
    "args.val_batch_size = 1024\n",
    "args.st_epochs = 5\n",
    "\n",
    "args.beta = 1.0\n",
    "args.cutmix_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200840, 6)\n",
      "(200840, 32332)\n",
      "(160716, 6) (40124, 6)\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_train_val_loaders(batch_size=args.batch_size, val_batch_size=args.val_batch_size, ifold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init model4\n",
      "model file: ./model4-ckps/se_resnext101_32x4d/model4_se_resnext101_fold3_224.pth, exist: True\n",
      "loading ./model4-ckps/se_resnext101_32x4d/model4_se_resnext101_fold3_224.pth...\n"
     ]
    }
   ],
   "source": [
    "model, model_file = create_model(args)\n",
    "#if torch.cuda.device_count() > 1:\n",
    "#    model = nn.DataParallel(model)\n",
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for param in model.backbone.parameters():\n",
    "#    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validate(nn.DataParallel(model), val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CYCLE: 1\n",
      "{'recall': 0.974315, 'recall_grapheme': 0.957545, 'recall_vowel': 0.989539, 'recall_consonant': 0.992631, 'recall_word': 0.937249, 'acc_grapheme': 0.955638, 'acc_vowel': 0.989358, 'acc_consonant': 0.987663, 'acc_word': 0.92411, 'loss_grapheme': 0.204077, 'loss_vowel': 0.058991, 'loss_consonant': 0.056561, 'loss_word': 0.329869}\n",
      "    0 | 0.000040 | 160600/160716 | 5.7291 | 10.5365 ||\n",
      "val: {'recall': 0.989055, 'recall_grapheme': 0.984259, 'recall_vowel': 0.993084, 'recall_consonant': 0.994619, 'recall_word': 0.983246, 'acc_grapheme': 0.983526, 'acc_vowel': 0.994193, 'acc_consonant': 0.994068, 'acc_word': 0.983152, 'loss_grapheme': 0.068397, 'loss_vowel': 0.039506, 'loss_consonant': 0.033124, 'loss_word': 0.067768}\n",
      "###>>>>> saved\n",
      "    1 | 0.000080 | 160600/160716 | 1.7460 | 9.4575 |||\n",
      "val: {'recall': 0.990077, 'recall_grapheme': 0.985977, 'recall_vowel': 0.993699, 'recall_consonant': 0.994656, 'recall_word': 0.985932, 'acc_grapheme': 0.98537, 'acc_vowel': 0.994517, 'acc_consonant': 0.994392, 'acc_word': 0.985794, 'loss_grapheme': 0.061491, 'loss_vowel': 0.040159, 'loss_consonant': 0.031983, 'loss_word': 0.057672}\n",
      "###>>>>> saved\n",
      "    2 | 0.000119 | 160600/160716 | 7.6391 | 9.9685 |||\n",
      "val: {'recall': 0.98953, 'recall_grapheme': 0.985162, 'recall_vowel': 0.993145, 'recall_consonant': 0.994653, 'recall_word': 0.984675, 'acc_grapheme': 0.984673, 'acc_vowel': 0.994367, 'acc_consonant': 0.994442, 'acc_word': 0.984523, 'loss_grapheme': 0.072847, 'loss_vowel': 0.044708, 'loss_consonant': 0.035521, 'loss_word': 0.066656}\n",
      "    3 | 0.000159 | 160600/160716 | 1.9420 | 9.4064 |||\n",
      "val: {'recall': 0.988903, 'recall_grapheme': 0.984726, 'recall_vowel': 0.993129, 'recall_consonant': 0.993032, 'recall_word': 0.983634, 'acc_grapheme': 0.984124, 'acc_vowel': 0.994492, 'acc_consonant': 0.994343, 'acc_word': 0.983526, 'loss_grapheme': 0.067227, 'loss_vowel': 0.038379, 'loss_consonant': 0.031802, 'loss_word': 0.066978}\n",
      "    4 | 0.000198 | 160600/160716 | 7.9644 | 10.4677 ||\n",
      "val: {'recall': 0.988943, 'recall_grapheme': 0.984209, 'recall_vowel': 0.993137, 'recall_consonant': 0.994218, 'recall_word': 0.982953, 'acc_grapheme': 0.982878, 'acc_vowel': 0.993869, 'acc_consonant': 0.994118, 'acc_word': 0.982704, 'loss_grapheme': 0.077249, 'loss_vowel': 0.045362, 'loss_consonant': 0.034155, 'loss_word': 0.075085}\n",
      "    5 | 0.000197 | 160600/160716 | 2.2999 | 10.2100 ||\n",
      "val: {'recall': 0.989562, 'recall_grapheme': 0.986042, 'recall_vowel': 0.993285, 'recall_consonant': 0.992879, 'recall_word': 0.985637, 'acc_grapheme': 0.985146, 'acc_vowel': 0.993819, 'acc_consonant': 0.994343, 'acc_word': 0.98537, 'loss_grapheme': 0.065195, 'loss_vowel': 0.041242, 'loss_consonant': 0.031692, 'loss_word': 0.063192}\n",
      "    6 | 0.000196 | 160600/160716 | 14.1896 | 9.1750 ||\n",
      "val: {'recall': 0.989594, 'recall_grapheme': 0.985346, 'recall_vowel': 0.993514, 'recall_consonant': 0.994168, 'recall_word': 0.984291, 'acc_grapheme': 0.984697, 'acc_vowel': 0.994392, 'acc_consonant': 0.994667, 'acc_word': 0.984025, 'loss_grapheme': 0.064267, 'loss_vowel': 0.032504, 'loss_consonant': 0.026737, 'loss_word': 0.065515}\n",
      "    7 | 0.000195 | 160600/160716 | 25.8571 | 10.8187 |\n",
      "val: {'recall': 0.989358, 'recall_grapheme': 0.985618, 'recall_vowel': 0.992739, 'recall_consonant': 0.993457, 'recall_word': 0.98539, 'acc_grapheme': 0.984373, 'acc_vowel': 0.994218, 'acc_consonant': 0.994293, 'acc_word': 0.985171, 'loss_grapheme': 0.078026, 'loss_vowel': 0.058871, 'loss_consonant': 0.041718, 'loss_word': 0.067531}\n",
      "    8 | 0.000194 | 160600/160716 | 11.7121 | 9.6186 ||\n",
      "val: {'recall': 0.988895, 'recall_grapheme': 0.984137, 'recall_vowel': 0.992546, 'recall_consonant': 0.994762, 'recall_word': 0.984079, 'acc_grapheme': 0.983725, 'acc_vowel': 0.994318, 'acc_consonant': 0.994168, 'acc_word': 0.9838, 'loss_grapheme': 0.076002, 'loss_vowel': 0.04614, 'loss_consonant': 0.038777, 'loss_word': 0.072493}\n",
      "    9 | 0.000192 | 160600/160716 | 11.8632 | 10.7039 |\n",
      "val: {'recall': 0.989992, 'recall_grapheme': 0.986342, 'recall_vowel': 0.993129, 'recall_consonant': 0.994155, 'recall_word': 0.9861, 'acc_grapheme': 0.985271, 'acc_vowel': 0.994492, 'acc_consonant': 0.994243, 'acc_word': 0.985968, 'loss_grapheme': 0.07476, 'loss_vowel': 0.049668, 'loss_consonant': 0.041234, 'loss_word': 0.066055}\n",
      "   10 | 0.000191 | 160600/160716 | 1.9024 | 9.4725 |||\n",
      "val: {'recall': 0.988773, 'recall_grapheme': 0.983748, 'recall_vowel': 0.992598, 'recall_consonant': 0.994998, 'recall_word': 0.984541, 'acc_grapheme': 0.983875, 'acc_vowel': 0.994617, 'acc_consonant': 0.994143, 'acc_word': 0.984349, 'loss_grapheme': 0.075324, 'loss_vowel': 0.050194, 'loss_consonant': 0.041415, 'loss_word': 0.068798}\n",
      "   11 | 0.000189 | 160600/160716 | 19.0053 | 9.7739 ||\n",
      "val: {'recall': 0.989627, 'recall_grapheme': 0.985564, 'recall_vowel': 0.993857, 'recall_consonant': 0.993524, 'recall_word': 0.985478, 'acc_grapheme': 0.985021, 'acc_vowel': 0.994766, 'acc_consonant': 0.994367, 'acc_word': 0.985246, 'loss_grapheme': 0.08971, 'loss_vowel': 0.062813, 'loss_consonant': 0.045783, 'loss_word': 0.080139}\n",
      "   12 | 0.000187 | 160600/160716 | 2.0295 | 9.2504 ||\n",
      "val: {'recall': 0.989663, 'recall_grapheme': 0.985957, 'recall_vowel': 0.993886, 'recall_consonant': 0.992851, 'recall_word': 0.985558, 'acc_grapheme': 0.985096, 'acc_vowel': 0.994467, 'acc_consonant': 0.994766, 'acc_word': 0.985321, 'loss_grapheme': 0.087835, 'loss_vowel': 0.072068, 'loss_consonant': 0.047376, 'loss_word': 0.073319}\n",
      "   13 | 0.000185 | 160600/160716 | 2.0788 | 9.8360 |||\n",
      "val: {'recall': 0.988791, 'recall_grapheme': 0.983989, 'recall_vowel': 0.994232, 'recall_consonant': 0.992955, 'recall_word': 0.984142, 'acc_grapheme': 0.98385, 'acc_vowel': 0.994667, 'acc_consonant': 0.994068, 'acc_word': 0.9839, 'loss_grapheme': 0.084417, 'loss_vowel': 0.052197, 'loss_consonant': 0.04291, 'loss_word': 0.075866}\n",
      "   14 | 0.000183 | 160600/160716 | 1.7038 | 9.4502 |||\n",
      "val: {'recall': 0.990566, 'recall_grapheme': 0.986988, 'recall_vowel': 0.993949, 'recall_consonant': 0.994341, 'recall_word': 0.986295, 'acc_grapheme': 0.986467, 'acc_vowel': 0.994816, 'acc_consonant': 0.994916, 'acc_word': 0.985944, 'loss_grapheme': 0.05193, 'loss_vowel': 0.027083, 'loss_consonant': 0.022609, 'loss_word': 0.052957}\n",
      "###>>>>> saved\n",
      "   15 | 0.000181 | 160600/160716 | 1.7471 | 8.9783 |||\n",
      "val: {'recall': 0.989638, 'recall_grapheme': 0.98525, 'recall_vowel': 0.993786, 'recall_consonant': 0.994266, 'recall_word': 0.985459, 'acc_grapheme': 0.985395, 'acc_vowel': 0.994791, 'acc_consonant': 0.994492, 'acc_word': 0.985296, 'loss_grapheme': 0.06153, 'loss_vowel': 0.035945, 'loss_consonant': 0.030921, 'loss_word': 0.058219}\n",
      "   16 | 0.000179 | 160600/160716 | 5.7319 | 9.3108 |||\n",
      "val: {'recall': 0.991083, 'recall_grapheme': 0.987358, 'recall_vowel': 0.99344, 'recall_consonant': 0.996176, 'recall_word': 0.986523, 'acc_grapheme': 0.985894, 'acc_vowel': 0.994716, 'acc_consonant': 0.995065, 'acc_word': 0.986268, 'loss_grapheme': 0.061361, 'loss_vowel': 0.039506, 'loss_consonant': 0.03149, 'loss_word': 0.057598}\n",
      "###>>>>> saved\n",
      "   17 | 0.000176 | 160600/160716 | 24.3287 | 8.8012 ||\n",
      "val: {'recall': 0.990403, 'recall_grapheme': 0.985735, 'recall_vowel': 0.994556, 'recall_consonant': 0.995585, 'recall_word': 0.985334, 'acc_grapheme': 0.985296, 'acc_vowel': 0.994966, 'acc_consonant': 0.994841, 'acc_word': 0.985096, 'loss_grapheme': 0.068717, 'loss_vowel': 0.050831, 'loss_consonant': 0.036852, 'loss_word': 0.0647}\n",
      "   18 | 0.000173 | 160600/160716 | 8.0854 | 9.6252 |||\n",
      "val: {'recall': 0.989926, 'recall_grapheme': 0.985071, 'recall_vowel': 0.99486, 'recall_consonant': 0.9947, 'recall_word': 0.983373, 'acc_grapheme': 0.983651, 'acc_vowel': 0.994766, 'acc_consonant': 0.994343, 'acc_word': 0.982828, 'loss_grapheme': 0.080803, 'loss_vowel': 0.050243, 'loss_consonant': 0.037266, 'loss_word': 0.080543}\n",
      "   19 | 0.000171 | 160600/160716 | 6.7846 | 9.3728 |||\n",
      "val: {'recall': 0.989459, 'recall_grapheme': 0.985245, 'recall_vowel': 0.993444, 'recall_consonant': 0.993903, 'recall_word': 0.984211, 'acc_grapheme': 0.984448, 'acc_vowel': 0.994218, 'acc_consonant': 0.994043, 'acc_word': 0.984074, 'loss_grapheme': 0.111764, 'loss_vowel': 0.087106, 'loss_consonant': 0.061918, 'loss_word': 0.096456}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20 | 0.000168 | 160600/160716 | 8.0262 | 9.8860 |||\n",
      "val: {'recall': 0.990934, 'recall_grapheme': 0.98712, 'recall_vowel': 0.99417, 'recall_consonant': 0.995324, 'recall_word': 0.986762, 'acc_grapheme': 0.986716, 'acc_vowel': 0.995015, 'acc_consonant': 0.995065, 'acc_word': 0.986517, 'loss_grapheme': 0.07838, 'loss_vowel': 0.060853, 'loss_consonant': 0.04413, 'loss_word': 0.068844}\n",
      "   21 | 0.000165 | 160600/160716 | 1.6605 | 9.1677 |||\n",
      "val: {'recall': 0.99013, 'recall_grapheme': 0.985327, 'recall_vowel': 0.99385, 'recall_consonant': 0.996014, 'recall_word': 0.985874, 'acc_grapheme': 0.985395, 'acc_vowel': 0.994891, 'acc_consonant': 0.99519, 'acc_word': 0.985495, 'loss_grapheme': 0.073322, 'loss_vowel': 0.049508, 'loss_consonant': 0.038459, 'loss_word': 0.066963}\n",
      "   22 | 0.000162 | 160600/160716 | 24.4420 | 9.5924 ||\n",
      "val: {'recall': 0.99141, 'recall_grapheme': 0.986719, 'recall_vowel': 0.995302, 'recall_consonant': 0.996901, 'recall_word': 0.987243, 'acc_grapheme': 0.986866, 'acc_vowel': 0.995439, 'acc_consonant': 0.99529, 'acc_word': 0.987065, 'loss_grapheme': 0.053632, 'loss_vowel': 0.033589, 'loss_consonant': 0.024973, 'loss_word': 0.051123}\n",
      "###>>>>> saved\n",
      "   23 | 0.000159 | 160600/160716 | 1.6486 | 9.2929 |||\n",
      "val: {'recall': 0.989233, 'recall_grapheme': 0.983722, 'recall_vowel': 0.993911, 'recall_consonant': 0.995575, 'recall_word': 0.98335, 'acc_grapheme': 0.983526, 'acc_vowel': 0.994617, 'acc_consonant': 0.994218, 'acc_word': 0.982903, 'loss_grapheme': 0.070202, 'loss_vowel': 0.034799, 'loss_consonant': 0.030089, 'loss_word': 0.069677}\n",
      "   24 | 0.000156 | 160600/160716 | 21.5453 | 10.0217 |\n",
      "val: {'recall': 0.991094, 'recall_grapheme': 0.987253, 'recall_vowel': 0.993968, 'recall_consonant': 0.995901, 'recall_word': 0.986773, 'acc_grapheme': 0.986467, 'acc_vowel': 0.994866, 'acc_consonant': 0.995065, 'acc_word': 0.986517, 'loss_grapheme': 0.072248, 'loss_vowel': 0.048459, 'loss_consonant': 0.036579, 'loss_word': 0.06543}\n",
      "   25 | 0.000152 | 160600/160716 | 2.2598 | 9.1222 |||\n",
      "val: {'recall': 0.991354, 'recall_grapheme': 0.988806, 'recall_vowel': 0.994322, 'recall_consonant': 0.993483, 'recall_word': 0.98742, 'acc_grapheme': 0.987663, 'acc_vowel': 0.995439, 'acc_consonant': 0.995614, 'acc_word': 0.987165, 'loss_grapheme': 0.049992, 'loss_vowel': 0.027461, 'loss_consonant': 0.021602, 'loss_word': 0.050512}\n",
      "   26 | 0.000149 | 160600/160716 | 1.7623 | 8.9365 ||\n",
      "val: {'recall': 0.991925, 'recall_grapheme': 0.988425, 'recall_vowel': 0.994885, 'recall_consonant': 0.995965, 'recall_word': 0.986218, 'acc_grapheme': 0.986691, 'acc_vowel': 0.995265, 'acc_consonant': 0.99519, 'acc_word': 0.985919, 'loss_grapheme': 0.052466, 'loss_vowel': 0.028319, 'loss_consonant': 0.023259, 'loss_word': 0.054507}\n",
      "###>>>>> saved\n",
      "   27 | 0.000145 | 160600/160716 | 21.7665 | 9.1295 ||\n",
      "val: {'recall': 0.992246, 'recall_grapheme': 0.989277, 'recall_vowel': 0.994826, 'recall_consonant': 0.995605, 'recall_word': 0.988136, 'acc_grapheme': 0.988536, 'acc_vowel': 0.995389, 'acc_consonant': 0.995888, 'acc_word': 0.988037, 'loss_grapheme': 0.049875, 'loss_vowel': 0.033234, 'loss_consonant': 0.02588, 'loss_word': 0.047022}\n",
      "###>>>>> saved\n",
      "   28 | 0.000142 | 160600/160716 | 1.8378 | 8.1873 |||\n",
      "val: {'recall': 0.991177, 'recall_grapheme': 0.987323, 'recall_vowel': 0.994932, 'recall_consonant': 0.995131, 'recall_word': 0.986768, 'acc_grapheme': 0.986891, 'acc_vowel': 0.995339, 'acc_consonant': 0.995738, 'acc_word': 0.986616, 'loss_grapheme': 0.050012, 'loss_vowel': 0.029389, 'loss_consonant': 0.024317, 'loss_word': 0.050691}\n",
      "   29 | 0.000138 | 160600/160716 | 10.6321 | 8.9684 |\n",
      "val: {'recall': 0.990958, 'recall_grapheme': 0.987147, 'recall_vowel': 0.994806, 'recall_consonant': 0.994733, 'recall_word': 0.985359, 'acc_grapheme': 0.986243, 'acc_vowel': 0.995414, 'acc_consonant': 0.995015, 'acc_word': 0.985221, 'loss_grapheme': 0.068639, 'loss_vowel': 0.042531, 'loss_consonant': 0.034523, 'loss_word': 0.065796}\n",
      "   30 | 0.000135 | 160600/160716 | 8.8513 | 9.7458 ||\n",
      "val: {'recall': 0.990433, 'recall_grapheme': 0.985544, 'recall_vowel': 0.994365, 'recall_consonant': 0.996282, 'recall_word': 0.985107, 'acc_grapheme': 0.985321, 'acc_vowel': 0.99504, 'acc_consonant': 0.994966, 'acc_word': 0.984797, 'loss_grapheme': 0.074967, 'loss_vowel': 0.057673, 'loss_consonant': 0.041603, 'loss_word': 0.069683}\n",
      "   31 | 0.000131 | 160600/160716 | 3.9339 | 8.5702 ||\n",
      "val: {'recall': 0.991695, 'recall_grapheme': 0.987662, 'recall_vowel': 0.995024, 'recall_consonant': 0.996433, 'recall_word': 0.986374, 'acc_grapheme': 0.987165, 'acc_vowel': 0.995464, 'acc_consonant': 0.995713, 'acc_word': 0.986342, 'loss_grapheme': 0.056659, 'loss_vowel': 0.031037, 'loss_consonant': 0.025737, 'loss_word': 0.056934}\n",
      "   32 | 0.000127 | 160600/160716 | 10.9770 | 8.6785 ||\n",
      "val: {'recall': 0.991032, 'recall_grapheme': 0.98709, 'recall_vowel': 0.994709, 'recall_consonant': 0.995239, 'recall_word': 0.986505, 'acc_grapheme': 0.986691, 'acc_vowel': 0.995439, 'acc_consonant': 0.995564, 'acc_word': 0.986218, 'loss_grapheme': 0.063512, 'loss_vowel': 0.041607, 'loss_consonant': 0.031971, 'loss_word': 0.061909}\n",
      "   33 | 0.000123 | 160600/160716 | 22.7748 | 9.2834 ||\n",
      "val: {'recall': 0.992439, 'recall_grapheme': 0.98876, 'recall_vowel': 0.995124, 'recall_consonant': 0.997112, 'recall_word': 0.988606, 'acc_grapheme': 0.987738, 'acc_vowel': 0.995639, 'acc_consonant': 0.995863, 'acc_word': 0.988261, 'loss_grapheme': 0.048973, 'loss_vowel': 0.030908, 'loss_consonant': 0.024306, 'loss_word': 0.046675}\n",
      "###>>>>> saved\n",
      "   34 | 0.000120 | 160600/160716 | 1.6585 | 8.6837 |||\n",
      "val: {'recall': 0.98943, 'recall_grapheme': 0.985098, 'recall_vowel': 0.994039, 'recall_consonant': 0.993484, 'recall_word': 0.982508, 'acc_grapheme': 0.984025, 'acc_vowel': 0.994816, 'acc_consonant': 0.994642, 'acc_word': 0.982105, 'loss_grapheme': 0.073543, 'loss_vowel': 0.042981, 'loss_consonant': 0.034641, 'loss_word': 0.074405}\n",
      "   35 | 0.000116 | 160600/160716 | 9.7024 | 9.1464 |||\n",
      "val: {'recall': 0.991764, 'recall_grapheme': 0.987931, 'recall_vowel': 0.994601, 'recall_consonant': 0.996594, 'recall_word': 0.986513, 'acc_grapheme': 0.987314, 'acc_vowel': 0.995464, 'acc_consonant': 0.995015, 'acc_word': 0.986243, 'loss_grapheme': 0.075741, 'loss_vowel': 0.061343, 'loss_consonant': 0.047562, 'loss_word': 0.067362}\n",
      "   36 | 0.000112 | 160600/160716 | 1.5311 | 8.9749 |||\n",
      "val: {'recall': 0.992214, 'recall_grapheme': 0.988748, 'recall_vowel': 0.994971, 'recall_consonant': 0.99639, 'recall_word': 0.987744, 'acc_grapheme': 0.988261, 'acc_vowel': 0.995539, 'acc_consonant': 0.995888, 'acc_word': 0.987514, 'loss_grapheme': 0.054916, 'loss_vowel': 0.040606, 'loss_consonant': 0.030765, 'loss_word': 0.052777}\n",
      "   37 | 0.000108 | 160600/160716 | 21.4644 | 9.2838 ||\n",
      "val: {'recall': 0.992221, 'recall_grapheme': 0.988955, 'recall_vowel': 0.994853, 'recall_consonant': 0.996121, 'recall_word': 0.987907, 'acc_grapheme': 0.987763, 'acc_vowel': 0.995539, 'acc_consonant': 0.995414, 'acc_word': 0.987688, 'loss_grapheme': 0.054628, 'loss_vowel': 0.040483, 'loss_consonant': 0.032084, 'loss_word': 0.050893}\n",
      "   38 | 0.000104 | 160600/160716 | 0.4817 | 8.7302 ||\n",
      "val: {'recall': 0.991814, 'recall_grapheme': 0.988425, 'recall_vowel': 0.994672, 'recall_consonant': 0.995732, 'recall_word': 0.988182, 'acc_grapheme': 0.988212, 'acc_vowel': 0.995514, 'acc_consonant': 0.995813, 'acc_word': 0.988012, 'loss_grapheme': 0.046208, 'loss_vowel': 0.02835, 'loss_consonant': 0.023251, 'loss_word': 0.046325}\n",
      "   39 | 0.000100 | 160600/160716 | 6.1769 | 8.6962 ||\n",
      "val: {'recall': 0.992575, 'recall_grapheme': 0.989227, 'recall_vowel': 0.994982, 'recall_consonant': 0.996865, 'recall_word': 0.9877, 'acc_grapheme': 0.987987, 'acc_vowel': 0.995514, 'acc_consonant': 0.995514, 'acc_word': 0.987539, 'loss_grapheme': 0.046726, 'loss_vowel': 0.026629, 'loss_consonant': 0.023202, 'loss_word': 0.048958}\n",
      "###>>>>> saved\n",
      "   40 | 0.000096 | 160600/160716 | 1.7788 | 8.4952 ||\n",
      "val: {'recall': 0.993479, 'recall_grapheme': 0.990671, 'recall_vowel': 0.994946, 'recall_consonant': 0.997629, 'recall_word': 0.990075, 'acc_grapheme': 0.989707, 'acc_vowel': 0.995813, 'acc_consonant': 0.996212, 'acc_word': 0.989807, 'loss_grapheme': 0.042389, 'loss_vowel': 0.028739, 'loss_consonant': 0.021242, 'loss_word': 0.041857}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###>>>>> saved\n",
      "   41 | 0.000092 | 160600/160716 | 1.5070 | 8.6557 |||\n",
      "val: {'recall': 0.992322, 'recall_grapheme': 0.988741, 'recall_vowel': 0.994258, 'recall_consonant': 0.997551, 'recall_word': 0.98757, 'acc_grapheme': 0.987937, 'acc_vowel': 0.995489, 'acc_consonant': 0.995813, 'acc_word': 0.98724, 'loss_grapheme': 0.050951, 'loss_vowel': 0.030896, 'loss_consonant': 0.026127, 'loss_word': 0.05112}\n",
      "   42 | 0.000088 | 160600/160716 | 10.2968 | 8.5079 ||\n",
      "val: {'recall': 0.991637, 'recall_grapheme': 0.987464, 'recall_vowel': 0.995304, 'recall_consonant': 0.996318, 'recall_word': 0.986679, 'acc_grapheme': 0.987489, 'acc_vowel': 0.995738, 'acc_consonant': 0.995614, 'acc_word': 0.986417, 'loss_grapheme': 0.057776, 'loss_vowel': 0.033966, 'loss_consonant': 0.028304, 'loss_word': 0.056438}\n",
      "   43 | 0.000084 | 160600/160716 | 1.2920 | 9.1411 |||\n",
      "val: {'recall': 0.989792, 'recall_grapheme': 0.984614, 'recall_vowel': 0.994327, 'recall_consonant': 0.995612, 'recall_word': 0.983806, 'acc_grapheme': 0.984398, 'acc_vowel': 0.99509, 'acc_consonant': 0.994891, 'acc_word': 0.983526, 'loss_grapheme': 0.083924, 'loss_vowel': 0.054962, 'loss_consonant': 0.041186, 'loss_word': 0.078225}\n",
      "   44 | 0.000081 | 160600/160716 | 22.6272 | 9.5263 ||\n",
      "val: {'recall': 0.992239, 'recall_grapheme': 0.988537, 'recall_vowel': 0.994765, 'recall_consonant': 0.997116, 'recall_word': 0.98725, 'acc_grapheme': 0.988087, 'acc_vowel': 0.995663, 'acc_consonant': 0.995863, 'acc_word': 0.986965, 'loss_grapheme': 0.056447, 'loss_vowel': 0.038945, 'loss_consonant': 0.030186, 'loss_word': 0.055183}\n",
      "   45 | 0.000077 | 160600/160716 | 6.5167 | 9.0964 ||\n",
      "val: {'recall': 0.990342, 'recall_grapheme': 0.985698, 'recall_vowel': 0.994736, 'recall_consonant': 0.995236, 'recall_word': 0.984441, 'acc_grapheme': 0.985321, 'acc_vowel': 0.99509, 'acc_consonant': 0.99529, 'acc_word': 0.984224, 'loss_grapheme': 0.0803, 'loss_vowel': 0.058388, 'loss_consonant': 0.040933, 'loss_word': 0.073773}\n",
      "   46 | 0.000073 | 160600/160716 | 1.8122 | 8.4211 ||\n",
      "val: {'recall': 0.993238, 'recall_grapheme': 0.989986, 'recall_vowel': 0.995029, 'recall_consonant': 0.99795, 'recall_word': 0.989069, 'acc_grapheme': 0.989483, 'acc_vowel': 0.995938, 'acc_consonant': 0.996361, 'acc_word': 0.988884, 'loss_grapheme': 0.042322, 'loss_vowel': 0.028048, 'loss_consonant': 0.021984, 'loss_word': 0.041195}\n",
      "   47 | 0.000069 | 160600/160716 | 2.0309 | 8.9691 ||\n",
      "val: {'recall': 0.992386, 'recall_grapheme': 0.98865, 'recall_vowel': 0.9954, 'recall_consonant': 0.996843, 'recall_word': 0.987086, 'acc_grapheme': 0.988012, 'acc_vowel': 0.995688, 'acc_consonant': 0.995763, 'acc_word': 0.986916, 'loss_grapheme': 0.052235, 'loss_vowel': 0.030129, 'loss_consonant': 0.023651, 'loss_word': 0.053904}\n",
      "   48 | 0.000065 | 160600/160716 | 1.4498 | 8.7885 ||\n",
      "val: {'recall': 0.993462, 'recall_grapheme': 0.990108, 'recall_vowel': 0.995931, 'recall_consonant': 0.997701, 'recall_word': 0.988685, 'acc_grapheme': 0.989607, 'acc_vowel': 0.996137, 'acc_consonant': 0.996287, 'acc_word': 0.988585, 'loss_grapheme': 0.04181, 'loss_vowel': 0.024139, 'loss_consonant': 0.01905, 'loss_word': 0.042208}\n",
      "   49 | 0.000062 | 160600/160716 | 9.7596 | 9.0217 |||\n",
      "val: {'recall': 0.992278, 'recall_grapheme': 0.988623, 'recall_vowel': 0.995519, 'recall_consonant': 0.996345, 'recall_word': 0.987265, 'acc_grapheme': 0.987888, 'acc_vowel': 0.995663, 'acc_consonant': 0.995713, 'acc_word': 0.987165, 'loss_grapheme': 0.062768, 'loss_vowel': 0.051146, 'loss_consonant': 0.039485, 'loss_word': 0.056973}\n",
      "   50 | 0.000058 | 160600/160716 | 7.6830 | 8.1605 ||\n",
      "val: {'recall': 0.992086, 'recall_grapheme': 0.987809, 'recall_vowel': 0.995692, 'recall_consonant': 0.997034, 'recall_word': 0.987208, 'acc_grapheme': 0.987489, 'acc_vowel': 0.996012, 'acc_consonant': 0.995713, 'acc_word': 0.98699, 'loss_grapheme': 0.056742, 'loss_vowel': 0.036348, 'loss_consonant': 0.029362, 'loss_word': 0.055756}\n",
      "   51 | 0.000055 | 160600/160716 | 22.7985 | 8.4023 ||\n",
      "val: {'recall': 0.992308, 'recall_grapheme': 0.989304, 'recall_vowel': 0.995354, 'recall_consonant': 0.995272, 'recall_word': 0.987992, 'acc_grapheme': 0.988336, 'acc_vowel': 0.995838, 'acc_consonant': 0.996162, 'acc_word': 0.987838, 'loss_grapheme': 0.060065, 'loss_vowel': 0.044694, 'loss_consonant': 0.033608, 'loss_word': 0.056438}\n",
      "   52 | 0.000051 | 160600/160716 | 1.5044 | 9.2804 |||\n",
      "val: {'recall': 0.993349, 'recall_grapheme': 0.990302, 'recall_vowel': 0.99567, 'recall_consonant': 0.997124, 'recall_word': 0.988357, 'acc_grapheme': 0.989358, 'acc_vowel': 0.995838, 'acc_consonant': 0.996336, 'acc_word': 0.988212, 'loss_grapheme': 0.044345, 'loss_vowel': 0.025478, 'loss_consonant': 0.020069, 'loss_word': 0.044758}\n",
      "   53 | 0.000048 | 160600/160716 | 6.8317 | 7.9532 ||\n",
      "val: {'recall': 0.992092, 'recall_grapheme': 0.988262, 'recall_vowel': 0.99547, 'recall_consonant': 0.996373, 'recall_word': 0.987079, 'acc_grapheme': 0.987838, 'acc_vowel': 0.995938, 'acc_consonant': 0.995713, 'acc_word': 0.986916, 'loss_grapheme': 0.052626, 'loss_vowel': 0.032981, 'loss_consonant': 0.027422, 'loss_word': 0.052288}\n",
      "   54 | 0.000044 | 160600/160716 | 13.0155 | 8.6893 ||\n",
      "val: {'recall': 0.992269, 'recall_grapheme': 0.988578, 'recall_vowel': 0.995514, 'recall_consonant': 0.996409, 'recall_word': 0.987292, 'acc_grapheme': 0.987912, 'acc_vowel': 0.995813, 'acc_consonant': 0.995738, 'acc_word': 0.98704, 'loss_grapheme': 0.061467, 'loss_vowel': 0.046247, 'loss_consonant': 0.036491, 'loss_word': 0.058183}\n",
      "   55 | 0.000041 | 160600/160716 | 1.5080 | 9.3552 |||\n",
      "val: {'recall': 0.992634, 'recall_grapheme': 0.988972, 'recall_vowel': 0.995484, 'recall_consonant': 0.997109, 'recall_word': 0.987691, 'acc_grapheme': 0.988162, 'acc_vowel': 0.995738, 'acc_consonant': 0.995987, 'acc_word': 0.987389, 'loss_grapheme': 0.054816, 'loss_vowel': 0.037043, 'loss_consonant': 0.029175, 'loss_word': 0.054054}\n",
      "   56 | 0.000038 | 160600/160716 | 1.4003 | 8.9151 |||\n",
      "val: {'recall': 0.992942, 'recall_grapheme': 0.989381, 'recall_vowel': 0.995916, 'recall_consonant': 0.997091, 'recall_word': 0.987824, 'acc_grapheme': 0.988336, 'acc_vowel': 0.996012, 'acc_consonant': 0.996187, 'acc_word': 0.987539, 'loss_grapheme': 0.051922, 'loss_vowel': 0.033726, 'loss_consonant': 0.026749, 'loss_word': 0.051648}\n",
      "   57 | 0.000035 | 160600/160716 | 9.5628 | 8.4793 |||\n",
      "val: {'recall': 0.990775, 'recall_grapheme': 0.986597, 'recall_vowel': 0.994447, 'recall_consonant': 0.99546, 'recall_word': 0.984711, 'acc_grapheme': 0.986093, 'acc_vowel': 0.995564, 'acc_consonant': 0.995265, 'acc_word': 0.984423, 'loss_grapheme': 0.076316, 'loss_vowel': 0.054194, 'loss_consonant': 0.041538, 'loss_word': 0.072005}\n",
      "   58 | 0.000032 | 160600/160716 | 24.3683 | 8.7974 |\n",
      "val: {'recall': 0.993104, 'recall_grapheme': 0.989934, 'recall_vowel': 0.99586, 'recall_consonant': 0.996687, 'recall_word': 0.988273, 'acc_grapheme': 0.988835, 'acc_vowel': 0.995838, 'acc_consonant': 0.996187, 'acc_word': 0.988062, 'loss_grapheme': 0.062467, 'loss_vowel': 0.050424, 'loss_consonant': 0.037918, 'loss_word': 0.057048}\n",
      "   59 | 0.000029 | 160600/160716 | 22.2815 | 8.5397 |\n",
      "val: {'recall': 0.994226, 'recall_grapheme': 0.991689, 'recall_vowel': 0.995633, 'recall_consonant': 0.997894, 'recall_word': 0.990759, 'acc_grapheme': 0.990953, 'acc_vowel': 0.996212, 'acc_consonant': 0.996885, 'acc_word': 0.990654, 'loss_grapheme': 0.034709, 'loss_vowel': 0.020399, 'loss_consonant': 0.016148, 'loss_word': 0.034547}\n",
      "###>>>>> saved\n",
      "   60 | 0.000027 | 160600/160716 | 9.5616 | 8.4132 |||\n",
      "val: {'recall': 0.992101, 'recall_grapheme': 0.988417, 'recall_vowel': 0.994987, 'recall_consonant': 0.996585, 'recall_word': 0.986707, 'acc_grapheme': 0.987389, 'acc_vowel': 0.995614, 'acc_consonant': 0.995763, 'acc_word': 0.986517, 'loss_grapheme': 0.065717, 'loss_vowel': 0.050519, 'loss_consonant': 0.040877, 'loss_word': 0.061513}\n",
      "   61 | 0.000024 | 160600/160716 | 6.5258 | 9.0252 ||\n",
      "val: {'recall': 0.992399, 'recall_grapheme': 0.988902, 'recall_vowel': 0.99521, 'recall_consonant': 0.996584, 'recall_word': 0.987157, 'acc_grapheme': 0.988087, 'acc_vowel': 0.995738, 'acc_consonant': 0.995763, 'acc_word': 0.986965, 'loss_grapheme': 0.055124, 'loss_vowel': 0.036559, 'loss_consonant': 0.03083, 'loss_word': 0.054054}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   62 | 0.000021 | 160600/160716 | 19.6101 | 8.4000 ||\n",
      "val: {'recall': 0.994551, 'recall_grapheme': 0.991918, 'recall_vowel': 0.996319, 'recall_consonant': 0.998049, 'recall_word': 0.990766, 'acc_grapheme': 0.991053, 'acc_vowel': 0.996386, 'acc_consonant': 0.997109, 'acc_word': 0.990629, 'loss_grapheme': 0.033562, 'loss_vowel': 0.018168, 'loss_consonant': 0.014472, 'loss_word': 0.034619}\n",
      "###>>>>> saved\n",
      "   63 | 0.000019 | 160600/160716 | 1.3499 | 8.7183 |||\n",
      "val: {'recall': 0.993533, 'recall_grapheme': 0.990477, 'recall_vowel': 0.99579, 'recall_consonant': 0.997389, 'recall_word': 0.98921, 'acc_grapheme': 0.989632, 'acc_vowel': 0.996137, 'acc_consonant': 0.996536, 'acc_word': 0.988934, 'loss_grapheme': 0.042751, 'loss_vowel': 0.025261, 'loss_consonant': 0.020895, 'loss_word': 0.043512}\n",
      "   64 | 0.000017 | 160600/160716 | 1.5855 | 9.3146 |||\n",
      "val: {'recall': 0.993643, 'recall_grapheme': 0.990681, 'recall_vowel': 0.99579, 'recall_consonant': 0.997421, 'recall_word': 0.988928, 'acc_grapheme': 0.989657, 'acc_vowel': 0.996062, 'acc_consonant': 0.996336, 'acc_word': 0.988685, 'loss_grapheme': 0.040959, 'loss_vowel': 0.021746, 'loss_consonant': 0.018109, 'loss_word': 0.043347}\n",
      "   65 | 0.000015 | 160600/160716 | 1.2740 | 8.0583 |||\n",
      "val: {'recall': 0.993531, 'recall_grapheme': 0.990651, 'recall_vowel': 0.995558, 'recall_consonant': 0.997262, 'recall_word': 0.989087, 'acc_grapheme': 0.989732, 'acc_vowel': 0.996062, 'acc_consonant': 0.996262, 'acc_word': 0.98886, 'loss_grapheme': 0.039621, 'loss_vowel': 0.019438, 'loss_consonant': 0.016601, 'loss_word': 0.042947}\n",
      "   66 | 0.000013 | 160600/160716 | 22.2540 | 7.9268 ||\n",
      "val: {'recall': 0.994155, 'recall_grapheme': 0.991474, 'recall_vowel': 0.996177, 'recall_consonant': 0.997495, 'recall_word': 0.990436, 'acc_grapheme': 0.990804, 'acc_vowel': 0.996486, 'acc_consonant': 0.996885, 'acc_word': 0.990255, 'loss_grapheme': 0.035715, 'loss_vowel': 0.021423, 'loss_consonant': 0.017838, 'loss_word': 0.035511}\n",
      "   67 | 0.000011 | 160600/160716 | 6.8366 | 8.8564 |||\n",
      "val: {'recall': 0.993038, 'recall_grapheme': 0.989602, 'recall_vowel': 0.995635, 'recall_consonant': 0.997311, 'recall_word': 0.987985, 'acc_grapheme': 0.988835, 'acc_vowel': 0.996012, 'acc_consonant': 0.996386, 'acc_word': 0.987738, 'loss_grapheme': 0.048651, 'loss_vowel': 0.028543, 'loss_consonant': 0.024075, 'loss_word': 0.049763}\n",
      "   68 | 0.000009 | 160600/160716 | 8.1194 | 8.5372 ||\n",
      "val: {'recall': 0.992503, 'recall_grapheme': 0.988813, 'recall_vowel': 0.995351, 'recall_consonant': 0.997034, 'recall_word': 0.987097, 'acc_grapheme': 0.988037, 'acc_vowel': 0.995888, 'acc_consonant': 0.995987, 'acc_word': 0.986965, 'loss_grapheme': 0.049624, 'loss_vowel': 0.027761, 'loss_consonant': 0.023301, 'loss_word': 0.05121}\n",
      "   69 | 0.000008 | 160600/160716 | 20.4720 | 8.6026 ||\n",
      "val: {'recall': 0.993769, 'recall_grapheme': 0.990903, 'recall_vowel': 0.996049, 'recall_consonant': 0.997224, 'recall_word': 0.989337, 'acc_grapheme': 0.989931, 'acc_vowel': 0.996311, 'acc_consonant': 0.996511, 'acc_word': 0.989109, 'loss_grapheme': 0.043499, 'loss_vowel': 0.028187, 'loss_consonant': 0.022512, 'loss_word': 0.043681}\n",
      "   70 | 0.000006 | 160600/160716 | 1.1503 | 7.8125 |||\n",
      "val: {'recall': 0.990963, 'recall_grapheme': 0.986729, 'recall_vowel': 0.995213, 'recall_consonant': 0.99518, 'recall_word': 0.985892, 'acc_grapheme': 0.986791, 'acc_vowel': 0.995663, 'acc_consonant': 0.995614, 'acc_word': 0.985744, 'loss_grapheme': 0.064577, 'loss_vowel': 0.044031, 'loss_consonant': 0.033704, 'loss_word': 0.062678}\n",
      "   71 | 0.000005 | 160600/160716 | 10.4971 | 8.4209 |\n",
      "val: {'recall': 0.991863, 'recall_grapheme': 0.987878, 'recall_vowel': 0.99529, 'recall_consonant': 0.996407, 'recall_word': 0.986747, 'acc_grapheme': 0.987588, 'acc_vowel': 0.995713, 'acc_consonant': 0.995663, 'acc_word': 0.986517, 'loss_grapheme': 0.072447, 'loss_vowel': 0.061826, 'loss_consonant': 0.047305, 'loss_word': 0.064925}\n",
      "   72 | 0.000004 | 160600/160716 | 5.3803 | 8.3224 |||\n",
      "val: {'recall': 0.993723, 'recall_grapheme': 0.990726, 'recall_vowel': 0.995966, 'recall_consonant': 0.997475, 'recall_word': 0.988707, 'acc_grapheme': 0.989906, 'acc_vowel': 0.996287, 'acc_consonant': 0.996436, 'acc_word': 0.988486, 'loss_grapheme': 0.04352, 'loss_vowel': 0.025355, 'loss_consonant': 0.02107, 'loss_word': 0.045103}\n",
      "   73 | 0.000003 | 160600/160716 | 6.4374 | 9.2138 ||\n",
      "val: {'recall': 0.992781, 'recall_grapheme': 0.989399, 'recall_vowel': 0.995663, 'recall_consonant': 0.996665, 'recall_word': 0.986955, 'acc_grapheme': 0.988112, 'acc_vowel': 0.995963, 'acc_consonant': 0.996087, 'acc_word': 0.986716, 'loss_grapheme': 0.067662, 'loss_vowel': 0.050815, 'loss_consonant': 0.039684, 'loss_word': 0.064585}\n",
      "   74 | 0.000002 | 160600/160716 | 1.4116 | 8.3627 ||\n",
      "val: {'recall': 0.993751, 'recall_grapheme': 0.990745, 'recall_vowel': 0.996131, 'recall_consonant': 0.997384, 'recall_word': 0.988953, 'acc_grapheme': 0.989856, 'acc_vowel': 0.996262, 'acc_consonant': 0.996287, 'acc_word': 0.98871, 'loss_grapheme': 0.040777, 'loss_vowel': 0.021067, 'loss_consonant': 0.017801, 'loss_word': 0.043574}\n",
      "   75 | 0.000001 | 160600/160716 | 9.9137 | 8.6933 |||\n",
      "val: {'recall': 0.991877, 'recall_grapheme': 0.987846, 'recall_vowel': 0.99539, 'recall_consonant': 0.996424, 'recall_word': 0.987074, 'acc_grapheme': 0.987638, 'acc_vowel': 0.995813, 'acc_consonant': 0.995639, 'acc_word': 0.986916, 'loss_grapheme': 0.062228, 'loss_vowel': 0.049407, 'loss_consonant': 0.038631, 'loss_word': 0.058189}\n",
      "   76 | 0.000001 | 160600/160716 | 21.4128 | 8.1084 |\n",
      "val: {'recall': 0.993017, 'recall_grapheme': 0.989559, 'recall_vowel': 0.995778, 'recall_consonant': 0.997172, 'recall_word': 0.987774, 'acc_grapheme': 0.98876, 'acc_vowel': 0.995963, 'acc_consonant': 0.995938, 'acc_word': 0.987564, 'loss_grapheme': 0.054194, 'loss_vowel': 0.037096, 'loss_consonant': 0.030305, 'loss_word': 0.053607}\n",
      "   77 | 0.000000 | 160600/160716 | 1.1184 | 8.8860 ||\n",
      "val: {'recall': 0.993874, 'recall_grapheme': 0.99104, 'recall_vowel': 0.99612, 'recall_consonant': 0.997295, 'recall_word': 0.989406, 'acc_grapheme': 0.990205, 'acc_vowel': 0.996336, 'acc_consonant': 0.996411, 'acc_word': 0.989208, 'loss_grapheme': 0.039096, 'loss_vowel': 0.020659, 'loss_consonant': 0.016941, 'loss_word': 0.041223}\n",
      "   78 | 0.000000 | 160600/160716 | 9.9276 | 8.4183 |||\n",
      "val: {'recall': 0.99415, 'recall_grapheme': 0.991437, 'recall_vowel': 0.996332, 'recall_consonant': 0.997395, 'recall_word': 0.989515, 'acc_grapheme': 0.990504, 'acc_vowel': 0.996436, 'acc_consonant': 0.996735, 'acc_word': 0.989308, 'loss_grapheme': 0.039189, 'loss_vowel': 0.023094, 'loss_consonant': 0.018734, 'loss_word': 0.039908}\n",
      "   79 | 0.000000 | 160600/160716 | 7.1166 | 8.1763 |||\n",
      "val: {'recall': 0.993501, 'recall_grapheme': 0.990376, 'recall_vowel': 0.995964, 'recall_consonant': 0.997285, 'recall_word': 0.98891, 'acc_grapheme': 0.989433, 'acc_vowel': 0.996262, 'acc_consonant': 0.996262, 'acc_word': 0.98871, 'loss_grapheme': 0.049474, 'loss_vowel': 0.038374, 'loss_consonant': 0.030058, 'loss_word': 0.046647}\n",
      "CYCLE: 2\n",
      "{'recall': 0.993501, 'recall_grapheme': 0.990376, 'recall_vowel': 0.995964, 'recall_consonant': 0.997285, 'recall_word': 0.98891, 'acc_grapheme': 0.989433, 'acc_vowel': 0.996262, 'acc_consonant': 0.996262, 'acc_word': 0.98871, 'loss_grapheme': 0.049474, 'loss_vowel': 0.038374, 'loss_consonant': 0.030058, 'loss_word': 0.046647}\n",
      "    0 | 0.000040 | 160600/160716 | 10.5327 | 8.1793 |\n",
      "val: {'recall': 0.992564, 'recall_grapheme': 0.989001, 'recall_vowel': 0.995574, 'recall_consonant': 0.996678, 'recall_word': 0.987657, 'acc_grapheme': 0.988311, 'acc_vowel': 0.995963, 'acc_consonant': 0.996012, 'acc_word': 0.987464, 'loss_grapheme': 0.058571, 'loss_vowel': 0.042837, 'loss_consonant': 0.033572, 'loss_word': 0.056935}\n",
      "    1 | 0.000080 | 160600/160716 | 5.8276 | 9.1969 |||\n",
      "val: {'recall': 0.99276, 'recall_grapheme': 0.989392, 'recall_vowel': 0.995344, 'recall_consonant': 0.996913, 'recall_word': 0.988234, 'acc_grapheme': 0.989009, 'acc_vowel': 0.995987, 'acc_consonant': 0.995838, 'acc_word': 0.988062, 'loss_grapheme': 0.046208, 'loss_vowel': 0.029742, 'loss_consonant': 0.024226, 'loss_word': 0.046807}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2 | 0.000119 | 160600/160716 | 6.2717 | 8.0934 |||\n",
      "val: {'recall': 0.992794, 'recall_grapheme': 0.989417, 'recall_vowel': 0.995289, 'recall_consonant': 0.997052, 'recall_word': 0.987423, 'acc_grapheme': 0.988685, 'acc_vowel': 0.995913, 'acc_consonant': 0.996112, 'acc_word': 0.987314, 'loss_grapheme': 0.047483, 'loss_vowel': 0.025202, 'loss_consonant': 0.021114, 'loss_word': 0.050361}\n",
      "    3 | 0.000159 | 160600/160716 | 4.0098 | 9.0679 ||\n",
      "val: {'recall': 0.992112, 'recall_grapheme': 0.988294, 'recall_vowel': 0.995234, 'recall_consonant': 0.996625, 'recall_word': 0.98679, 'acc_grapheme': 0.987638, 'acc_vowel': 0.995738, 'acc_consonant': 0.995963, 'acc_word': 0.986567, 'loss_grapheme': 0.048227, 'loss_vowel': 0.021475, 'loss_consonant': 0.0195, 'loss_word': 0.05244}\n",
      "    4 | 0.000198 | 160600/160716 | 2.0486 | 8.8183 |||\n",
      "val: {'recall': 0.992865, 'recall_grapheme': 0.989807, 'recall_vowel': 0.995811, 'recall_consonant': 0.996036, 'recall_word': 0.988811, 'acc_grapheme': 0.988984, 'acc_vowel': 0.995863, 'acc_consonant': 0.996436, 'acc_word': 0.988585, 'loss_grapheme': 0.042383, 'loss_vowel': 0.0225, 'loss_consonant': 0.017754, 'loss_word': 0.04372}\n",
      "    5 | 0.000197 | 160600/160716 | 8.6818 | 9.1125 |||\n",
      "val: {'recall': 0.992124, 'recall_grapheme': 0.988023, 'recall_vowel': 0.995444, 'recall_consonant': 0.997005, 'recall_word': 0.987192, 'acc_grapheme': 0.987638, 'acc_vowel': 0.995663, 'acc_consonant': 0.995215, 'acc_word': 0.987015, 'loss_grapheme': 0.056142, 'loss_vowel': 0.032963, 'loss_consonant': 0.029472, 'loss_word': 0.054247}\n",
      "    6 | 0.000196 | 160600/160716 | 3.8239 | 9.3343 ||\n",
      "val: {'recall': 0.991783, 'recall_grapheme': 0.987831, 'recall_vowel': 0.994819, 'recall_consonant': 0.996651, 'recall_word': 0.986181, 'acc_grapheme': 0.987165, 'acc_vowel': 0.995265, 'acc_consonant': 0.995713, 'acc_word': 0.985919, 'loss_grapheme': 0.058629, 'loss_vowel': 0.032507, 'loss_consonant': 0.025592, 'loss_word': 0.058459}\n",
      "    7 | 0.000195 | 160600/160716 | 11.1636 | 8.4606 ||\n",
      "val: {'recall': 0.990124, 'recall_grapheme': 0.986258, 'recall_vowel': 0.994285, 'recall_consonant': 0.993697, 'recall_word': 0.984386, 'acc_grapheme': 0.985246, 'acc_vowel': 0.99504, 'acc_consonant': 0.994642, 'acc_word': 0.984049, 'loss_grapheme': 0.079897, 'loss_vowel': 0.053771, 'loss_consonant': 0.039616, 'loss_word': 0.074919}\n",
      "    8 | 0.000194 | 160600/160716 | 1.6796 | 8.9617 |||\n",
      "val: {'recall': 0.991377, 'recall_grapheme': 0.987325, 'recall_vowel': 0.994242, 'recall_consonant': 0.996614, 'recall_word': 0.98593, 'acc_grapheme': 0.986741, 'acc_vowel': 0.995539, 'acc_consonant': 0.995564, 'acc_word': 0.985669, 'loss_grapheme': 0.055924, 'loss_vowel': 0.033303, 'loss_consonant': 0.025716, 'loss_word': 0.055767}\n",
      "    9 | 0.000192 | 160600/160716 | 1.6739 | 8.9987 |||\n",
      "val: {'recall': 0.992015, 'recall_grapheme': 0.988405, 'recall_vowel': 0.994867, 'recall_consonant': 0.996381, 'recall_word': 0.987289, 'acc_grapheme': 0.98709, 'acc_vowel': 0.995389, 'acc_consonant': 0.995489, 'acc_word': 0.986916, 'loss_grapheme': 0.066559, 'loss_vowel': 0.047153, 'loss_consonant': 0.037103, 'loss_word': 0.062874}\n",
      "   10 | 0.000191 | 160600/160716 | 21.4096 | 8.9295 ||\n",
      "val: {'recall': 0.993539, 'recall_grapheme': 0.990424, 'recall_vowel': 0.995558, 'recall_consonant': 0.997751, 'recall_word': 0.990199, 'acc_grapheme': 0.990081, 'acc_vowel': 0.995838, 'acc_consonant': 0.996087, 'acc_word': 0.990006, 'loss_grapheme': 0.038941, 'loss_vowel': 0.02218, 'loss_consonant': 0.018879, 'loss_word': 0.038369}\n",
      "   11 | 0.000189 | 160600/160716 | 8.5903 | 8.4587 |||\n",
      "val: {'recall': 0.991749, 'recall_grapheme': 0.987887, 'recall_vowel': 0.995673, 'recall_consonant': 0.995551, 'recall_word': 0.98719, 'acc_grapheme': 0.987339, 'acc_vowel': 0.995663, 'acc_consonant': 0.995888, 'acc_word': 0.986866, 'loss_grapheme': 0.065359, 'loss_vowel': 0.050137, 'loss_consonant': 0.036966, 'loss_word': 0.0599}\n",
      "   12 | 0.000187 | 160600/160716 | 10.3253 | 9.3889 |\n",
      "val: {'recall': 0.992555, 'recall_grapheme': 0.990222, 'recall_vowel': 0.99465, 'recall_consonant': 0.995126, 'recall_word': 0.988222, 'acc_grapheme': 0.988286, 'acc_vowel': 0.99519, 'acc_consonant': 0.995938, 'acc_word': 0.988162, 'loss_grapheme': 0.060872, 'loss_vowel': 0.046432, 'loss_consonant': 0.0356, 'loss_word': 0.053389}\n",
      "   13 | 0.000185 | 160600/160716 | 23.8101 | 9.1032 ||\n",
      "val: {'recall': 0.991376, 'recall_grapheme': 0.988278, 'recall_vowel': 0.994079, 'recall_consonant': 0.99487, 'recall_word': 0.987347, 'acc_grapheme': 0.987065, 'acc_vowel': 0.994966, 'acc_consonant': 0.995614, 'acc_word': 0.98719, 'loss_grapheme': 0.076616, 'loss_vowel': 0.07103, 'loss_consonant': 0.052824, 'loss_word': 0.063476}\n",
      "   14 | 0.000183 | 160600/160716 | 2.2651 | 8.1529 |||\n",
      "val: {'recall': 0.99267, 'recall_grapheme': 0.989297, 'recall_vowel': 0.995058, 'recall_consonant': 0.997028, 'recall_word': 0.988245, 'acc_grapheme': 0.988286, 'acc_vowel': 0.995863, 'acc_consonant': 0.995663, 'acc_word': 0.987987, 'loss_grapheme': 0.049593, 'loss_vowel': 0.027157, 'loss_consonant': 0.023077, 'loss_word': 0.049593}\n",
      "   15 | 0.000181 | 160600/160716 | 1.6872 | 8.7212 ||\n",
      "val: {'recall': 0.992742, 'recall_grapheme': 0.990299, 'recall_vowel': 0.995092, 'recall_consonant': 0.995276, 'recall_word': 0.989636, 'acc_grapheme': 0.989383, 'acc_vowel': 0.995863, 'acc_consonant': 0.996361, 'acc_word': 0.989557, 'loss_grapheme': 0.041064, 'loss_vowel': 0.021436, 'loss_consonant': 0.018926, 'loss_word': 0.040465}\n",
      "   16 | 0.000179 | 160600/160716 | 20.9438 | 9.0315 |\n",
      "val: {'recall': 0.992072, 'recall_grapheme': 0.98802, 'recall_vowel': 0.994904, 'recall_consonant': 0.997343, 'recall_word': 0.987639, 'acc_grapheme': 0.987439, 'acc_vowel': 0.995489, 'acc_consonant': 0.995913, 'acc_word': 0.987289, 'loss_grapheme': 0.051968, 'loss_vowel': 0.031941, 'loss_consonant': 0.025399, 'loss_word': 0.051045}\n",
      "   17 | 0.000176 | 160600/160716 | 1.5267 | 7.5620 |||\n",
      "val: {'recall': 0.992724, 'recall_grapheme': 0.988708, 'recall_vowel': 0.995773, 'recall_consonant': 0.997706, 'recall_word': 0.988519, 'acc_grapheme': 0.988411, 'acc_vowel': 0.995913, 'acc_consonant': 0.996187, 'acc_word': 0.988361, 'loss_grapheme': 0.049595, 'loss_vowel': 0.035812, 'loss_consonant': 0.0266, 'loss_word': 0.047139}\n",
      "   18 | 0.000173 | 160600/160716 | 7.7189 | 8.0144 ||\n",
      "val: {'recall': 0.991934, 'recall_grapheme': 0.988092, 'recall_vowel': 0.99429, 'recall_consonant': 0.99726, 'recall_word': 0.985384, 'acc_grapheme': 0.987115, 'acc_vowel': 0.995115, 'acc_consonant': 0.995688, 'acc_word': 0.985246, 'loss_grapheme': 0.064115, 'loss_vowel': 0.042263, 'loss_consonant': 0.031377, 'loss_word': 0.062836}\n",
      "   19 | 0.000171 | 160600/160716 | 8.5769 | 9.4582 |||\n",
      "val: {'recall': 0.993675, 'recall_grapheme': 0.990294, 'recall_vowel': 0.996185, 'recall_consonant': 0.997926, 'recall_word': 0.989638, 'acc_grapheme': 0.990031, 'acc_vowel': 0.996237, 'acc_consonant': 0.996685, 'acc_word': 0.989408, 'loss_grapheme': 0.049009, 'loss_vowel': 0.03578, 'loss_consonant': 0.027451, 'loss_word': 0.044967}\n",
      "   20 | 0.000168 | 160600/160716 | 1.6026 | 8.6370 |||\n",
      "val: {'recall': 0.993, 'recall_grapheme': 0.990012, 'recall_vowel': 0.995581, 'recall_consonant': 0.996397, 'recall_word': 0.989617, 'acc_grapheme': 0.989732, 'acc_vowel': 0.996287, 'acc_consonant': 0.996411, 'acc_word': 0.989358, 'loss_grapheme': 0.041029, 'loss_vowel': 0.022249, 'loss_consonant': 0.018564, 'loss_word': 0.040892}\n",
      "   21 | 0.000165 | 160600/160716 | 1.7329 | 9.3652 |||\n",
      "val: {'recall': 0.993107, 'recall_grapheme': 0.989757, 'recall_vowel': 0.995726, 'recall_consonant': 0.997189, 'recall_word': 0.989236, 'acc_grapheme': 0.989159, 'acc_vowel': 0.995888, 'acc_consonant': 0.996237, 'acc_word': 0.989034, 'loss_grapheme': 0.053196, 'loss_vowel': 0.045281, 'loss_consonant': 0.032035, 'loss_word': 0.047586}\n",
      "   22 | 0.000162 | 160600/160716 | 1.8925 | 8.7235 |||\n",
      "val: {'recall': 0.993382, 'recall_grapheme': 0.990066, 'recall_vowel': 0.995894, 'recall_consonant': 0.997502, 'recall_word': 0.988697, 'acc_grapheme': 0.989034, 'acc_vowel': 0.995938, 'acc_consonant': 0.996262, 'acc_word': 0.988461, 'loss_grapheme': 0.044146, 'loss_vowel': 0.027316, 'loss_consonant': 0.021636, 'loss_word': 0.0445}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   23 | 0.000159 | 160600/160716 | 1.5050 | 8.2598 |||\n",
      "val: {'recall': 0.992994, 'recall_grapheme': 0.989738, 'recall_vowel': 0.995263, 'recall_consonant': 0.997235, 'recall_word': 0.989203, 'acc_grapheme': 0.989632, 'acc_vowel': 0.995838, 'acc_consonant': 0.996187, 'acc_word': 0.989059, 'loss_grapheme': 0.046067, 'loss_vowel': 0.032504, 'loss_consonant': 0.023884, 'loss_word': 0.043583}\n",
      "   24 | 0.000156 | 160600/160716 | 2.8431 | 9.0982 |||\n",
      "val: {'recall': 0.993058, 'recall_grapheme': 0.990005, 'recall_vowel': 0.995386, 'recall_consonant': 0.996837, 'recall_word': 0.988458, 'acc_grapheme': 0.988585, 'acc_vowel': 0.996087, 'acc_consonant': 0.996262, 'acc_word': 0.988236, 'loss_grapheme': 0.049604, 'loss_vowel': 0.028385, 'loss_consonant': 0.022866, 'loss_word': 0.048434}\n",
      "   25 | 0.000152 | 160600/160716 | 22.0933 | 9.0264 |\n",
      "val: {'recall': 0.991885, 'recall_grapheme': 0.988229, 'recall_vowel': 0.995104, 'recall_consonant': 0.995976, 'recall_word': 0.986251, 'acc_grapheme': 0.987539, 'acc_vowel': 0.995315, 'acc_consonant': 0.996062, 'acc_word': 0.985944, 'loss_grapheme': 0.060131, 'loss_vowel': 0.043121, 'loss_consonant': 0.031471, 'loss_word': 0.058371}\n",
      "   26 | 0.000149 | 160600/160716 | 8.7873 | 8.5749 |||\n",
      "val: {'recall': 0.992293, 'recall_grapheme': 0.988816, 'recall_vowel': 0.995004, 'recall_consonant': 0.996535, 'recall_word': 0.987447, 'acc_grapheme': 0.988187, 'acc_vowel': 0.995713, 'acc_consonant': 0.995987, 'acc_word': 0.987314, 'loss_grapheme': 0.059537, 'loss_vowel': 0.050352, 'loss_consonant': 0.036075, 'loss_word': 0.056209}\n",
      "   27 | 0.000145 | 160600/160716 | 6.8497 | 8.5287 |||\n",
      "val: {'recall': 0.993411, 'recall_grapheme': 0.990203, 'recall_vowel': 0.995352, 'recall_consonant': 0.997886, 'recall_word': 0.988375, 'acc_grapheme': 0.989084, 'acc_vowel': 0.995888, 'acc_consonant': 0.996586, 'acc_word': 0.988187, 'loss_grapheme': 0.043526, 'loss_vowel': 0.021785, 'loss_consonant': 0.019715, 'loss_word': 0.044694}\n",
      "   28 | 0.000142 | 160600/160716 | 1.4604 | 8.9053 |||\n",
      "val: {'recall': 0.992451, 'recall_grapheme': 0.98933, 'recall_vowel': 0.995872, 'recall_consonant': 0.995271, 'recall_word': 0.988024, 'acc_grapheme': 0.988212, 'acc_vowel': 0.995938, 'acc_consonant': 0.996062, 'acc_word': 0.987713, 'loss_grapheme': 0.05498, 'loss_vowel': 0.041514, 'loss_consonant': 0.033631, 'loss_word': 0.051578}\n",
      "   29 | 0.000138 | 160600/160716 | 14.0142 | 8.9749 ||\n",
      "val: {'recall': 0.993541, 'recall_grapheme': 0.990586, 'recall_vowel': 0.995431, 'recall_consonant': 0.997561, 'recall_word': 0.988922, 'acc_grapheme': 0.989508, 'acc_vowel': 0.996012, 'acc_consonant': 0.996461, 'acc_word': 0.988685, 'loss_grapheme': 0.04406, 'loss_vowel': 0.027483, 'loss_consonant': 0.02247, 'loss_word': 0.04386}\n",
      "   30 | 0.000135 | 160600/160716 | 9.0202 | 8.4417 |||\n",
      "val: {'recall': 0.992705, 'recall_grapheme': 0.988998, 'recall_vowel': 0.995925, 'recall_consonant': 0.996899, 'recall_word': 0.987425, 'acc_grapheme': 0.988062, 'acc_vowel': 0.995888, 'acc_consonant': 0.995938, 'acc_word': 0.98719, 'loss_grapheme': 0.058377, 'loss_vowel': 0.041002, 'loss_consonant': 0.033508, 'loss_word': 0.054918}\n",
      "   31 | 0.000131 | 160600/160716 | 21.2217 | 8.6615 ||\n",
      "val: {'recall': 0.992411, 'recall_grapheme': 0.988926, 'recall_vowel': 0.994936, 'recall_consonant': 0.996858, 'recall_word': 0.987667, 'acc_grapheme': 0.988635, 'acc_vowel': 0.995813, 'acc_consonant': 0.995863, 'acc_word': 0.987489, 'loss_grapheme': 0.072203, 'loss_vowel': 0.064905, 'loss_consonant': 0.049096, 'loss_word': 0.061339}\n",
      "   32 | 0.000127 | 160600/160716 | 12.3625 | 7.7341 |\n",
      "val: {'recall': 0.993861, 'recall_grapheme': 0.991141, 'recall_vowel': 0.995813, 'recall_consonant': 0.997351, 'recall_word': 0.989956, 'acc_grapheme': 0.990255, 'acc_vowel': 0.996436, 'acc_consonant': 0.997034, 'acc_word': 0.989657, 'loss_grapheme': 0.040325, 'loss_vowel': 0.024339, 'loss_consonant': 0.019318, 'loss_word': 0.040351}\n",
      "   33 | 0.000123 | 160600/160716 | 12.0724 | 8.3100 ||\n",
      "val: {'recall': 0.99392, 'recall_grapheme': 0.99124, 'recall_vowel': 0.995875, 'recall_consonant': 0.997327, 'recall_word': 0.988966, 'acc_grapheme': 0.989881, 'acc_vowel': 0.996187, 'acc_consonant': 0.996635, 'acc_word': 0.98871, 'loss_grapheme': 0.045418, 'loss_vowel': 0.025934, 'loss_consonant': 0.021726, 'loss_word': 0.045285}\n",
      "   34 | 0.000120 | 160600/160716 | 3.9644 | 7.9928 |||\n",
      "val: {'recall': 0.992975, 'recall_grapheme': 0.989928, 'recall_vowel': 0.995314, 'recall_consonant': 0.996729, 'recall_word': 0.987526, 'acc_grapheme': 0.988236, 'acc_vowel': 0.995788, 'acc_consonant': 0.996212, 'acc_word': 0.98724, 'loss_grapheme': 0.056858, 'loss_vowel': 0.041477, 'loss_consonant': 0.031773, 'loss_word': 0.054288}\n",
      "   35 | 0.000116 | 160600/160716 | 1.2781 | 9.0850 |||\n",
      "val: {'recall': 0.992949, 'recall_grapheme': 0.989714, 'recall_vowel': 0.995502, 'recall_consonant': 0.996865, 'recall_word': 0.988214, 'acc_grapheme': 0.989059, 'acc_vowel': 0.996187, 'acc_consonant': 0.996311, 'acc_word': 0.988112, 'loss_grapheme': 0.046246, 'loss_vowel': 0.028699, 'loss_consonant': 0.024566, 'loss_word': 0.046477}\n",
      "   36 | 0.000112 | 160600/160716 | 1.1526 | 7.8491 ||\n",
      "val: {'recall': 0.994255, 'recall_grapheme': 0.991494, 'recall_vowel': 0.996022, 'recall_consonant': 0.998009, 'recall_word': 0.990219, 'acc_grapheme': 0.990604, 'acc_vowel': 0.996635, 'acc_consonant': 0.99681, 'acc_word': 0.990006, 'loss_grapheme': 0.037429, 'loss_vowel': 0.020029, 'loss_consonant': 0.016684, 'loss_word': 0.037822}\n",
      "   37 | 0.000108 | 160600/160716 | 10.9601 | 8.4223 ||\n",
      "val: {'recall': 0.994134, 'recall_grapheme': 0.991437, 'recall_vowel': 0.996205, 'recall_consonant': 0.997458, 'recall_word': 0.990172, 'acc_grapheme': 0.990729, 'acc_vowel': 0.996311, 'acc_consonant': 0.997009, 'acc_word': 0.990156, 'loss_grapheme': 0.037212, 'loss_vowel': 0.024462, 'loss_consonant': 0.019056, 'loss_word': 0.036143}\n",
      "   38 | 0.000104 | 160600/160716 | 8.0621 | 8.2334 |||\n",
      "val: {'recall': 0.990489, 'recall_grapheme': 0.98631, 'recall_vowel': 0.994616, 'recall_consonant': 0.994719, 'recall_word': 0.984971, 'acc_grapheme': 0.986243, 'acc_vowel': 0.995364, 'acc_consonant': 0.995414, 'acc_word': 0.984697, 'loss_grapheme': 0.071101, 'loss_vowel': 0.048967, 'loss_consonant': 0.036684, 'loss_word': 0.068506}\n",
      "   39 | 0.000102 | 087120/160716 | 1.2746 | 8.0726 |||"
     ]
    }
   ],
   "source": [
    "train(args, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CYCLE: 1\n",
      "{'recall': 0.971385, 'recall_grapheme': 0.958317, 'recall_vowel': 0.982632, 'recall_consonant': 0.986275, 'recall_word': 0.953239, 'acc_grapheme': 0.953843, 'acc_vowel': 0.987838, 'acc_consonant': 0.987339, 'acc_word': 0.949855, 'loss_grapheme': 0.253012, 'loss_vowel': 0.098051, 'loss_consonant': 0.079705, 'loss_word': 0.261843}\n",
      "    0 | 0.000040 | 160640/160716 | 6.5186 | 13.2028 ||\n",
      "val: {'recall': 0.979137, 'recall_grapheme': 0.969254, 'recall_vowel': 0.989295, 'recall_consonant': 0.988745, 'recall_word': 0.967247, 'acc_grapheme': 0.967451, 'acc_vowel': 0.99018, 'acc_consonant': 0.990729, 'acc_word': 0.966529, 'loss_grapheme': 0.1693, 'loss_vowel': 0.080292, 'loss_consonant': 0.065291, 'loss_word': 0.16706}\n",
      "###>>>>> saved\n",
      "    1 | 0.000080 | 160640/160716 | 3.1096 | 13.1037 ||\n",
      "val: {'recall': 0.98112, 'recall_grapheme': 0.971663, 'recall_vowel': 0.99, 'recall_consonant': 0.991156, 'recall_word': 0.970476, 'acc_grapheme': 0.970093, 'acc_vowel': 0.990928, 'acc_consonant': 0.991152, 'acc_word': 0.970093, 'loss_grapheme': 0.160879, 'loss_vowel': 0.079969, 'loss_consonant': 0.067267, 'loss_word': 0.146915}\n",
      "###>>>>> saved\n",
      "    2 | 0.000119 | 160640/160716 | 13.4475 | 11.3694 |\n",
      "val: {'recall': 0.981871, 'recall_grapheme': 0.97202, 'recall_vowel': 0.990693, 'recall_consonant': 0.992748, 'recall_word': 0.970969, 'acc_grapheme': 0.971065, 'acc_vowel': 0.991476, 'acc_consonant': 0.990579, 'acc_word': 0.970467, 'loss_grapheme': 0.172216, 'loss_vowel': 0.097915, 'loss_consonant': 0.076578, 'loss_word': 0.152287}\n",
      "###>>>>> saved\n",
      "    3 | 0.000159 | 160640/160716 | 3.5097 | 11.9890 ||\n",
      "val: {'recall': 0.981402, 'recall_grapheme': 0.971633, 'recall_vowel': 0.99118, 'recall_consonant': 0.991161, 'recall_word': 0.970975, 'acc_grapheme': 0.971015, 'acc_vowel': 0.991452, 'acc_consonant': 0.991476, 'acc_word': 0.970791, 'loss_grapheme': 0.147745, 'loss_vowel': 0.06922, 'loss_consonant': 0.057841, 'loss_word': 0.134251}\n",
      "    4 | 0.000198 | 160640/160716 | 3.8180 | 12.8478 ||\n",
      "val: {'recall': 0.981376, 'recall_grapheme': 0.971441, 'recall_vowel': 0.99163, 'recall_consonant': 0.990991, 'recall_word': 0.970933, 'acc_grapheme': 0.971139, 'acc_vowel': 0.991626, 'acc_consonant': 0.990654, 'acc_word': 0.970541, 'loss_grapheme': 0.179302, 'loss_vowel': 0.10625, 'loss_consonant': 0.081797, 'loss_word': 0.153558}\n",
      "    5 | 0.000237 | 160640/160716 | 12.3060 | 11.7362 |\n",
      "val: {'recall': 0.982148, 'recall_grapheme': 0.973118, 'recall_vowel': 0.990664, 'recall_consonant': 0.991692, 'recall_word': 0.971045, 'acc_grapheme': 0.970815, 'acc_vowel': 0.991377, 'acc_consonant': 0.9919, 'acc_word': 0.970417, 'loss_grapheme': 0.186031, 'loss_vowel': 0.094546, 'loss_consonant': 0.074269, 'loss_word': 0.168004}\n",
      "###>>>>> saved\n",
      "    6 | 0.000275 | 160640/160716 | 3.4424 | 12.8329 ||\n",
      "val: {'recall': 0.98193, 'recall_grapheme': 0.972846, 'recall_vowel': 0.991011, 'recall_consonant': 0.99102, 'recall_word': 0.972342, 'acc_grapheme': 0.97246, 'acc_vowel': 0.99185, 'acc_consonant': 0.991476, 'acc_word': 0.972037, 'loss_grapheme': 0.171004, 'loss_vowel': 0.097385, 'loss_consonant': 0.078231, 'loss_word': 0.146605}\n",
      "    7 | 0.000312 | 160640/160716 | 15.8961 | 12.3659 |\n",
      "val: {'recall': 0.981658, 'recall_grapheme': 0.972764, 'recall_vowel': 0.990387, 'recall_consonant': 0.990719, 'recall_word': 0.971568, 'acc_grapheme': 0.971065, 'acc_vowel': 0.990953, 'acc_consonant': 0.991053, 'acc_word': 0.970791, 'loss_grapheme': 0.207605, 'loss_vowel': 0.115117, 'loss_consonant': 0.100968, 'loss_word': 0.175556}\n",
      "    8 | 0.000349 | 160640/160716 | 3.9067 | 11.2240 ||\n",
      "val: {'recall': 0.983097, 'recall_grapheme': 0.974632, 'recall_vowel': 0.991627, 'recall_consonant': 0.991496, 'recall_word': 0.973036, 'acc_grapheme': 0.973457, 'acc_vowel': 0.991751, 'acc_consonant': 0.991402, 'acc_word': 0.972485, 'loss_grapheme': 0.141103, 'loss_vowel': 0.065695, 'loss_consonant': 0.058456, 'loss_word': 0.132765}\n",
      "###>>>>> saved\n",
      "    9 | 0.000385 | 160640/160716 | 3.2018 | 12.2462 ||\n",
      "val: {'recall': 0.982374, 'recall_grapheme': 0.97278, 'recall_vowel': 0.991803, 'recall_consonant': 0.992132, 'recall_word': 0.972838, 'acc_grapheme': 0.971613, 'acc_vowel': 0.991925, 'acc_consonant': 0.990978, 'acc_word': 0.972261, 'loss_grapheme': 0.198219, 'loss_vowel': 0.116773, 'loss_consonant': 0.102107, 'loss_word': 0.167705}\n",
      "   10 | 0.000382 | 160640/160716 | 6.0672 | 13.0988 ||\n",
      "val: {'recall': 0.983409, 'recall_grapheme': 0.974209, 'recall_vowel': 0.99201, 'recall_consonant': 0.993209, 'recall_word': 0.973348, 'acc_grapheme': 0.973457, 'acc_vowel': 0.991302, 'acc_consonant': 0.992025, 'acc_word': 0.973034, 'loss_grapheme': 0.142434, 'loss_vowel': 0.072983, 'loss_consonant': 0.061601, 'loss_word': 0.121935}\n",
      "###>>>>> saved\n",
      "   11 | 0.000378 | 160640/160716 | 7.2617 | 12.2731 ||\n",
      "val: {'recall': 0.982574, 'recall_grapheme': 0.974008, 'recall_vowel': 0.991516, 'recall_consonant': 0.990763, 'recall_word': 0.973813, 'acc_grapheme': 0.97266, 'acc_vowel': 0.991576, 'acc_consonant': 0.991751, 'acc_word': 0.973383, 'loss_grapheme': 0.157744, 'loss_vowel': 0.091219, 'loss_consonant': 0.068421, 'loss_word': 0.133836}\n",
      "   12 | 0.000375 | 160640/160716 | 26.6509 | 12.2985 |\n",
      "val: {'recall': 0.982167, 'recall_grapheme': 0.973506, 'recall_vowel': 0.990148, 'recall_consonant': 0.991508, 'recall_word': 0.973595, 'acc_grapheme': 0.97251, 'acc_vowel': 0.991377, 'acc_consonant': 0.990828, 'acc_word': 0.973034, 'loss_grapheme': 0.208549, 'loss_vowel': 0.141843, 'loss_consonant': 0.107758, 'loss_word': 0.174516}\n",
      "   13 | 0.000371 | 160640/160716 | 23.3068 | 12.1479 |\n",
      "val: {'recall': 0.981861, 'recall_grapheme': 0.973377, 'recall_vowel': 0.991808, 'recall_consonant': 0.988883, 'recall_word': 0.973965, 'acc_grapheme': 0.972934, 'acc_vowel': 0.992025, 'acc_consonant': 0.991501, 'acc_word': 0.973831, 'loss_grapheme': 0.20446, 'loss_vowel': 0.142803, 'loss_consonant': 0.097953, 'loss_word': 0.173315}\n",
      "   14 | 0.000366 | 160640/160716 | 8.2217 | 10.8881 ||\n",
      "val: {'recall': 0.984202, 'recall_grapheme': 0.975866, 'recall_vowel': 0.991992, 'recall_consonant': 0.993083, 'recall_word': 0.974671, 'acc_grapheme': 0.975277, 'acc_vowel': 0.992598, 'acc_consonant': 0.992124, 'acc_word': 0.974404, 'loss_grapheme': 0.121646, 'loss_vowel': 0.052821, 'loss_consonant': 0.051441, 'loss_word': 0.111577}\n",
      "###>>>>> saved\n",
      "   15 | 0.000362 | 160640/160716 | 13.0017 | 11.4482 |\n",
      "val: {'recall': 0.983495, 'recall_grapheme': 0.974323, 'recall_vowel': 0.991941, 'recall_consonant': 0.993392, 'recall_word': 0.97407, 'acc_grapheme': 0.973657, 'acc_vowel': 0.992249, 'acc_consonant': 0.992199, 'acc_word': 0.973557, 'loss_grapheme': 0.192599, 'loss_vowel': 0.122342, 'loss_consonant': 0.097731, 'loss_word': 0.165422}\n",
      "   16 | 0.000357 | 160640/160716 | 24.9333 | 11.6559 |\n",
      "val: {'recall': 0.984602, 'recall_grapheme': 0.977421, 'recall_vowel': 0.991491, 'recall_consonant': 0.992073, 'recall_word': 0.976776, 'acc_grapheme': 0.976199, 'acc_vowel': 0.992498, 'acc_consonant': 0.992399, 'acc_word': 0.976373, 'loss_grapheme': 0.138928, 'loss_vowel': 0.085026, 'loss_consonant': 0.074846, 'loss_word': 0.117796}\n",
      "###>>>>> saved\n",
      "   17 | 0.000352 | 160640/160716 | 3.0041 | 12.3717 ||\n",
      "val: {'recall': 0.984205, 'recall_grapheme': 0.975813, 'recall_vowel': 0.991832, 'recall_consonant': 0.993362, 'recall_word': 0.975576, 'acc_grapheme': 0.974205, 'acc_vowel': 0.991775, 'acc_consonant': 0.991875, 'acc_word': 0.975077, 'loss_grapheme': 0.140123, 'loss_vowel': 0.091285, 'loss_consonant': 0.07724, 'loss_word': 0.123916}\n",
      "   18 | 0.000347 | 160640/160716 | 26.0354 | 12.5382 |\n",
      "val: {'recall': 0.983934, 'recall_grapheme': 0.976781, 'recall_vowel': 0.990365, 'recall_consonant': 0.991808, 'recall_word': 0.976935, 'acc_grapheme': 0.975451, 'acc_vowel': 0.992299, 'acc_consonant': 0.991526, 'acc_word': 0.976473, 'loss_grapheme': 0.117394, 'loss_vowel': 0.068867, 'loss_consonant': 0.057294, 'loss_word': 0.103562}\n",
      "   19 | 0.000341 | 160640/160716 | 6.5357 | 11.4915 ||\n",
      "val: {'recall': 0.985108, 'recall_grapheme': 0.977602, 'recall_vowel': 0.991308, 'recall_consonant': 0.993921, 'recall_word': 0.976469, 'acc_grapheme': 0.976024, 'acc_vowel': 0.992673, 'acc_consonant': 0.992324, 'acc_word': 0.975925, 'loss_grapheme': 0.104246, 'loss_vowel': 0.045986, 'loss_consonant': 0.043202, 'loss_word': 0.099588}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###>>>>> saved\n",
      "   20 | 0.000336 | 160640/160716 | 8.1848 | 9.8692 |||\n",
      "val: {'recall': 0.984405, 'recall_grapheme': 0.977018, 'recall_vowel': 0.991063, 'recall_consonant': 0.992519, 'recall_word': 0.976616, 'acc_grapheme': 0.975775, 'acc_vowel': 0.992299, 'acc_consonant': 0.992374, 'acc_word': 0.976124, 'loss_grapheme': 0.117133, 'loss_vowel': 0.06618, 'loss_consonant': 0.054526, 'loss_word': 0.10982}\n",
      "   21 | 0.000330 | 160640/160716 | 13.9724 | 11.1551 |\n",
      "val: {'recall': 0.983682, 'recall_grapheme': 0.974904, 'recall_vowel': 0.992285, 'recall_consonant': 0.992636, 'recall_word': 0.974476, 'acc_grapheme': 0.974903, 'acc_vowel': 0.993071, 'acc_consonant': 0.992174, 'acc_word': 0.973781, 'loss_grapheme': 0.116202, 'loss_vowel': 0.063143, 'loss_consonant': 0.052422, 'loss_word': 0.112797}\n",
      "   22 | 0.000324 | 160640/160716 | 26.0440 | 10.5056 |\n",
      "val: {'recall': 0.985931, 'recall_grapheme': 0.978673, 'recall_vowel': 0.992499, 'recall_consonant': 0.993877, 'recall_word': 0.979078, 'acc_grapheme': 0.97742, 'acc_vowel': 0.993022, 'acc_consonant': 0.992822, 'acc_word': 0.978766, 'loss_grapheme': 0.119511, 'loss_vowel': 0.068945, 'loss_consonant': 0.056631, 'loss_word': 0.099032}\n",
      "###>>>>> saved\n",
      "   23 | 0.000318 | 160640/160716 | 13.3316 | 11.0585 |\n",
      "val: {'recall': 0.984859, 'recall_grapheme': 0.978071, 'recall_vowel': 0.990363, 'recall_consonant': 0.992931, 'recall_word': 0.978859, 'acc_grapheme': 0.977121, 'acc_vowel': 0.992448, 'acc_consonant': 0.992025, 'acc_word': 0.978641, 'loss_grapheme': 0.167086, 'loss_vowel': 0.111308, 'loss_consonant': 0.086625, 'loss_word': 0.122371}\n",
      "   24 | 0.000311 | 160640/160716 | 25.6151 | 10.9616 |\n",
      "val: {'recall': 0.986744, 'recall_grapheme': 0.980234, 'recall_vowel': 0.992258, 'recall_consonant': 0.994251, 'recall_word': 0.98083, 'acc_grapheme': 0.979289, 'acc_vowel': 0.993071, 'acc_consonant': 0.993096, 'acc_word': 0.980485, 'loss_grapheme': 0.092784, 'loss_vowel': 0.053999, 'loss_consonant': 0.043853, 'loss_word': 0.08241}\n",
      "###>>>>> saved\n",
      "   25 | 0.000305 | 160640/160716 | 10.8594 | 10.4692 |\n",
      "val: {'recall': 0.986231, 'recall_grapheme': 0.979453, 'recall_vowel': 0.991932, 'recall_consonant': 0.994088, 'recall_word': 0.978513, 'acc_grapheme': 0.977943, 'acc_vowel': 0.992872, 'acc_consonant': 0.992997, 'acc_word': 0.978093, 'loss_grapheme': 0.132911, 'loss_vowel': 0.076994, 'loss_consonant': 0.059721, 'loss_word': 0.112615}\n",
      "   26 | 0.000298 | 160640/160716 | 26.2115 | 11.0693 |\n",
      "val: {'recall': 0.985797, 'recall_grapheme': 0.979353, 'recall_vowel': 0.991336, 'recall_consonant': 0.993147, 'recall_word': 0.979606, 'acc_grapheme': 0.978766, 'acc_vowel': 0.993096, 'acc_consonant': 0.992772, 'acc_word': 0.97914, 'loss_grapheme': 0.112487, 'loss_vowel': 0.079122, 'loss_consonant': 0.062325, 'loss_word': 0.094905}\n",
      "   27 | 0.000291 | 160640/160716 | 2.1125 | 10.0397 ||\n",
      "val: {'recall': 0.987184, 'recall_grapheme': 0.981051, 'recall_vowel': 0.99237, 'recall_consonant': 0.994266, 'recall_word': 0.979824, 'acc_grapheme': 0.979165, 'acc_vowel': 0.993321, 'acc_consonant': 0.993096, 'acc_word': 0.979588, 'loss_grapheme': 0.095346, 'loss_vowel': 0.052667, 'loss_consonant': 0.04656, 'loss_word': 0.086715}\n",
      "###>>>>> saved\n",
      "   28 | 0.000284 | 160640/160716 | 2.9909 | 10.5426 ||\n",
      "val: {'recall': 0.987106, 'recall_grapheme': 0.981721, 'recall_vowel': 0.991968, 'recall_consonant': 0.993012, 'recall_word': 0.980066, 'acc_grapheme': 0.979862, 'acc_vowel': 0.993445, 'acc_consonant': 0.993246, 'acc_word': 0.979613, 'loss_grapheme': 0.09446, 'loss_vowel': 0.049697, 'loss_consonant': 0.040288, 'loss_word': 0.0862}\n",
      "   29 | 0.000277 | 160640/160716 | 26.5238 | 10.4470 |\n",
      "val: {'recall': 0.98719, 'recall_grapheme': 0.981016, 'recall_vowel': 0.992433, 'recall_consonant': 0.994294, 'recall_word': 0.979436, 'acc_grapheme': 0.97894, 'acc_vowel': 0.993221, 'acc_consonant': 0.993545, 'acc_word': 0.97909, 'loss_grapheme': 0.120785, 'loss_vowel': 0.078446, 'loss_consonant': 0.056357, 'loss_word': 0.101193}\n",
      "###>>>>> saved\n",
      "   30 | 0.000269 | 160640/160716 | 2.4937 | 10.5037 ||\n",
      "val: {'recall': 0.986805, 'recall_grapheme': 0.980906, 'recall_vowel': 0.992669, 'recall_consonant': 0.99274, 'recall_word': 0.981356, 'acc_grapheme': 0.980087, 'acc_vowel': 0.993221, 'acc_consonant': 0.993171, 'acc_word': 0.981158, 'loss_grapheme': 0.113836, 'loss_vowel': 0.079982, 'loss_consonant': 0.062071, 'loss_word': 0.090754}\n",
      "   31 | 0.000262 | 160640/160716 | 26.2585 | 10.3484 |\n",
      "val: {'recall': 0.987074, 'recall_grapheme': 0.980559, 'recall_vowel': 0.992599, 'recall_consonant': 0.994577, 'recall_word': 0.980511, 'acc_grapheme': 0.979538, 'acc_vowel': 0.99347, 'acc_consonant': 0.993196, 'acc_word': 0.980261, 'loss_grapheme': 0.101045, 'loss_vowel': 0.065222, 'loss_consonant': 0.055219, 'loss_word': 0.086389}\n",
      "   32 | 0.000254 | 160640/160716 | 2.5381 | 9.7728 ||\n",
      "val: {'recall': 0.987602, 'recall_grapheme': 0.981621, 'recall_vowel': 0.993053, 'recall_consonant': 0.994115, 'recall_word': 0.980977, 'acc_grapheme': 0.980236, 'acc_vowel': 0.993695, 'acc_consonant': 0.993395, 'acc_word': 0.98066, 'loss_grapheme': 0.096315, 'loss_vowel': 0.056707, 'loss_consonant': 0.047004, 'loss_word': 0.084712}\n",
      "###>>>>> saved\n",
      "   33 | 0.000247 | 160640/160716 | 23.9817 | 9.5349 ||\n",
      "val: {'recall': 0.986815, 'recall_grapheme': 0.979963, 'recall_vowel': 0.992757, 'recall_consonant': 0.994576, 'recall_word': 0.978185, 'acc_grapheme': 0.978417, 'acc_vowel': 0.993246, 'acc_consonant': 0.993395, 'acc_word': 0.977968, 'loss_grapheme': 0.117934, 'loss_vowel': 0.071831, 'loss_consonant': 0.058289, 'loss_word': 0.107679}\n",
      "   34 | 0.000239 | 160640/160716 | 2.4345 | 9.8989 |||\n",
      "val: {'recall': 0.988145, 'recall_grapheme': 0.982761, 'recall_vowel': 0.992866, 'recall_consonant': 0.994191, 'recall_word': 0.982154, 'acc_grapheme': 0.98056, 'acc_vowel': 0.99347, 'acc_consonant': 0.99347, 'acc_word': 0.981881, 'loss_grapheme': 0.096178, 'loss_vowel': 0.061679, 'loss_consonant': 0.049402, 'loss_word': 0.080383}\n",
      "###>>>>> saved\n",
      "   35 | 0.000231 | 160640/160716 | 4.8172 | 10.5925 ||\n",
      "val: {'recall': 0.988971, 'recall_grapheme': 0.984294, 'recall_vowel': 0.994139, 'recall_consonant': 0.993155, 'recall_word': 0.983416, 'acc_grapheme': 0.982803, 'acc_vowel': 0.993994, 'acc_consonant': 0.993595, 'acc_word': 0.983252, 'loss_grapheme': 0.07998, 'loss_vowel': 0.044804, 'loss_consonant': 0.037834, 'loss_word': 0.072061}\n",
      "###>>>>> saved\n",
      "   36 | 0.000224 | 160640/160716 | 2.5328 | 10.3740 ||\n",
      "val: {'recall': 0.988425, 'recall_grapheme': 0.982959, 'recall_vowel': 0.993192, 'recall_consonant': 0.99459, 'recall_word': 0.982876, 'acc_grapheme': 0.981757, 'acc_vowel': 0.993969, 'acc_consonant': 0.993744, 'acc_word': 0.982579, 'loss_grapheme': 0.089994, 'loss_vowel': 0.05081, 'loss_consonant': 0.0444, 'loss_word': 0.076616}\n",
      "   37 | 0.000216 | 160640/160716 | 2.5730 | 10.0761 ||\n",
      "val: {'recall': 0.988347, 'recall_grapheme': 0.982642, 'recall_vowel': 0.994236, 'recall_consonant': 0.993867, 'recall_word': 0.982912, 'acc_grapheme': 0.982554, 'acc_vowel': 0.994343, 'acc_consonant': 0.994068, 'acc_word': 0.982778, 'loss_grapheme': 0.064827, 'loss_vowel': 0.030233, 'loss_consonant': 0.027166, 'loss_word': 0.064538}\n",
      "   38 | 0.000208 | 160640/160716 | 6.7609 | 10.6957 ||\n",
      "val: {'recall': 0.989119, 'recall_grapheme': 0.983713, 'recall_vowel': 0.993696, 'recall_consonant': 0.995353, 'recall_word': 0.982942, 'acc_grapheme': 0.982679, 'acc_vowel': 0.993969, 'acc_consonant': 0.993719, 'acc_word': 0.982554, 'loss_grapheme': 0.072092, 'loss_vowel': 0.040963, 'loss_consonant': 0.036738, 'loss_word': 0.069935}\n",
      "###>>>>> saved\n",
      "   39 | 0.000200 | 160640/160716 | 2.1546 | 10.1999 ||\n",
      "val: {'recall': 0.98822, 'recall_grapheme': 0.982337, 'recall_vowel': 0.993281, 'recall_consonant': 0.994925, 'recall_word': 0.98137, 'acc_grapheme': 0.980834, 'acc_vowel': 0.993794, 'acc_consonant': 0.99352, 'acc_word': 0.980984, 'loss_grapheme': 0.080808, 'loss_vowel': 0.045516, 'loss_consonant': 0.034674, 'loss_word': 0.078708}\n",
      "   40 | 0.000192 | 160640/160716 | 2.7974 | 10.3896 ||\n",
      "val: {'recall': 0.98837, 'recall_grapheme': 0.982178, 'recall_vowel': 0.993804, 'recall_consonant': 0.995319, 'recall_word': 0.983266, 'acc_grapheme': 0.982155, 'acc_vowel': 0.994318, 'acc_consonant': 0.993994, 'acc_word': 0.983028, 'loss_grapheme': 0.070327, 'loss_vowel': 0.036538, 'loss_consonant': 0.031454, 'loss_word': 0.068273}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   41 | 0.000184 | 160640/160716 | 2.1980 | 9.8575 |||\n",
      "val: {'recall': 0.988541, 'recall_grapheme': 0.982392, 'recall_vowel': 0.993822, 'recall_consonant': 0.995561, 'recall_word': 0.983101, 'acc_grapheme': 0.982255, 'acc_vowel': 0.993919, 'acc_consonant': 0.994268, 'acc_word': 0.982828, 'loss_grapheme': 0.073723, 'loss_vowel': 0.041669, 'loss_consonant': 0.033414, 'loss_word': 0.068638}\n",
      "   42 | 0.000177 | 160640/160716 | 12.1496 | 10.0625 |\n",
      "val: {'recall': 0.988676, 'recall_grapheme': 0.983335, 'recall_vowel': 0.992984, 'recall_consonant': 0.995049, 'recall_word': 0.982107, 'acc_grapheme': 0.981283, 'acc_vowel': 0.994093, 'acc_consonant': 0.993919, 'acc_word': 0.981831, 'loss_grapheme': 0.098801, 'loss_vowel': 0.061308, 'loss_consonant': 0.053519, 'loss_word': 0.08689}\n",
      "   43 | 0.000169 | 160640/160716 | 12.7108 | 10.4040 |\n",
      "val: {'recall': 0.989115, 'recall_grapheme': 0.984596, 'recall_vowel': 0.993529, 'recall_consonant': 0.993739, 'recall_word': 0.984719, 'acc_grapheme': 0.98385, 'acc_vowel': 0.994293, 'acc_consonant': 0.994567, 'acc_word': 0.984598, 'loss_grapheme': 0.071032, 'loss_vowel': 0.040757, 'loss_consonant': 0.034027, 'loss_word': 0.063372}\n",
      "   44 | 0.000161 | 160640/160716 | 2.0590 | 9.8019 |||\n",
      "val: {'recall': 0.990427, 'recall_grapheme': 0.986355, 'recall_vowel': 0.993778, 'recall_consonant': 0.995221, 'recall_word': 0.985632, 'acc_grapheme': 0.984473, 'acc_vowel': 0.994417, 'acc_consonant': 0.994517, 'acc_word': 0.98547, 'loss_grapheme': 0.062011, 'loss_vowel': 0.033849, 'loss_consonant': 0.02852, 'loss_word': 0.058847}\n",
      "###>>>>> saved\n",
      "   45 | 0.000153 | 160640/160716 | 7.7134 | 9.3121 |||\n",
      "val: {'recall': 0.99045, 'recall_grapheme': 0.985874, 'recall_vowel': 0.994136, 'recall_consonant': 0.995914, 'recall_word': 0.985113, 'acc_grapheme': 0.984299, 'acc_vowel': 0.994567, 'acc_consonant': 0.994467, 'acc_word': 0.984922, 'loss_grapheme': 0.062666, 'loss_vowel': 0.035347, 'loss_consonant': 0.029313, 'loss_word': 0.059584}\n",
      "###>>>>> saved\n",
      "   46 | 0.000146 | 160640/160716 | 12.4095 | 9.9293 ||\n",
      "val: {'recall': 0.989413, 'recall_grapheme': 0.984401, 'recall_vowel': 0.993841, 'recall_consonant': 0.99501, 'recall_word': 0.983405, 'acc_grapheme': 0.983501, 'acc_vowel': 0.994467, 'acc_consonant': 0.994218, 'acc_word': 0.983277, 'loss_grapheme': 0.088369, 'loss_vowel': 0.054684, 'loss_consonant': 0.046196, 'loss_word': 0.076832}\n",
      "   47 | 0.000138 | 160640/160716 | 22.1362 | 9.3190 ||\n",
      "val: {'recall': 0.989092, 'recall_grapheme': 0.984209, 'recall_vowel': 0.993556, 'recall_consonant': 0.994395, 'recall_word': 0.984401, 'acc_grapheme': 0.983227, 'acc_vowel': 0.994492, 'acc_consonant': 0.994143, 'acc_word': 0.984224, 'loss_grapheme': 0.071023, 'loss_vowel': 0.042299, 'loss_consonant': 0.034528, 'loss_word': 0.064837}\n",
      "   48 | 0.000131 | 160640/160716 | 24.2786 | 10.1685 |\n",
      "val: {'recall': 0.988775, 'recall_grapheme': 0.983434, 'recall_vowel': 0.993657, 'recall_consonant': 0.994574, 'recall_word': 0.984694, 'acc_grapheme': 0.983302, 'acc_vowel': 0.994417, 'acc_consonant': 0.994268, 'acc_word': 0.984548, 'loss_grapheme': 0.110732, 'loss_vowel': 0.09317, 'loss_consonant': 0.065088, 'loss_word': 0.082952}\n",
      "   49 | 0.000123 | 160640/160716 | 1.9987 | 9.9208 | |\n",
      "val: {'recall': 0.99048, 'recall_grapheme': 0.986607, 'recall_vowel': 0.994478, 'recall_consonant': 0.994227, 'recall_word': 0.986121, 'acc_grapheme': 0.985794, 'acc_vowel': 0.994617, 'acc_consonant': 0.994691, 'acc_word': 0.986118, 'loss_grapheme': 0.054437, 'loss_vowel': 0.028405, 'loss_consonant': 0.02411, 'loss_word': 0.051925}\n",
      "###>>>>> saved\n",
      "   50 | 0.000116 | 160640/160716 | 9.2060 | 9.1625 |||\n",
      "val: {'recall': 0.989433, 'recall_grapheme': 0.985018, 'recall_vowel': 0.993204, 'recall_consonant': 0.994494, 'recall_word': 0.984419, 'acc_grapheme': 0.983676, 'acc_vowel': 0.994592, 'acc_consonant': 0.994143, 'acc_word': 0.984224, 'loss_grapheme': 0.096364, 'loss_vowel': 0.071676, 'loss_consonant': 0.053318, 'loss_word': 0.079198}\n",
      "   51 | 0.000109 | 160640/160716 | 1.8879 | 9.7718 |||\n",
      "val: {'recall': 0.990288, 'recall_grapheme': 0.986452, 'recall_vowel': 0.993712, 'recall_consonant': 0.994535, 'recall_word': 0.985506, 'acc_grapheme': 0.985271, 'acc_vowel': 0.994642, 'acc_consonant': 0.994691, 'acc_word': 0.98537, 'loss_grapheme': 0.074381, 'loss_vowel': 0.048705, 'loss_consonant': 0.037357, 'loss_word': 0.065252}\n",
      "   52 | 0.000102 | 160640/160716 | 1.8686 | 9.3316 |||\n",
      "val: {'recall': 0.990485, 'recall_grapheme': 0.986089, 'recall_vowel': 0.993946, 'recall_consonant': 0.995816, 'recall_word': 0.984595, 'acc_grapheme': 0.984249, 'acc_vowel': 0.994691, 'acc_consonant': 0.994517, 'acc_word': 0.984498, 'loss_grapheme': 0.076122, 'loss_vowel': 0.048863, 'loss_consonant': 0.040405, 'loss_word': 0.068453}\n",
      "###>>>>> saved\n",
      "   53 | 0.000096 | 160640/160716 | 1.8940 | 9.2939 |||\n",
      "val: {'recall': 0.991202, 'recall_grapheme': 0.987034, 'recall_vowel': 0.994511, 'recall_consonant': 0.996229, 'recall_word': 0.987224, 'acc_grapheme': 0.986268, 'acc_vowel': 0.99504, 'acc_consonant': 0.99509, 'acc_word': 0.98719, 'loss_grapheme': 0.051097, 'loss_vowel': 0.025803, 'loss_consonant': 0.022492, 'loss_word': 0.049288}\n",
      "###>>>>> saved\n",
      "   54 | 0.000089 | 160640/160716 | 4.0124 | 8.9599 |||\n",
      "val: {'recall': 0.989134, 'recall_grapheme': 0.983677, 'recall_vowel': 0.993915, 'recall_consonant': 0.995266, 'recall_word': 0.982785, 'acc_grapheme': 0.982853, 'acc_vowel': 0.994766, 'acc_consonant': 0.994442, 'acc_word': 0.982604, 'loss_grapheme': 0.092865, 'loss_vowel': 0.060621, 'loss_consonant': 0.044202, 'loss_word': 0.083507}\n",
      "   55 | 0.000082 | 160640/160716 | 24.1633 | 9.9083 ||\n",
      "val: {'recall': 0.990148, 'recall_grapheme': 0.985149, 'recall_vowel': 0.994262, 'recall_consonant': 0.996031, 'recall_word': 0.985527, 'acc_grapheme': 0.984174, 'acc_vowel': 0.994642, 'acc_consonant': 0.994741, 'acc_word': 0.98542, 'loss_grapheme': 0.090474, 'loss_vowel': 0.0713, 'loss_consonant': 0.051129, 'loss_word': 0.073267}\n",
      "   56 | 0.000076 | 160640/160716 | 12.2525 | 9.4824 ||\n",
      "val: {'recall': 0.990688, 'recall_grapheme': 0.986425, 'recall_vowel': 0.994524, 'recall_consonant': 0.995379, 'recall_word': 0.985782, 'acc_grapheme': 0.985296, 'acc_vowel': 0.99504, 'acc_consonant': 0.994567, 'acc_word': 0.98562, 'loss_grapheme': 0.055987, 'loss_vowel': 0.027239, 'loss_consonant': 0.024916, 'loss_word': 0.054478}\n",
      "   57 | 0.000070 | 160640/160716 | 1.6659 | 9.4769 |||\n",
      "val: {'recall': 0.990614, 'recall_grapheme': 0.986902, 'recall_vowel': 0.994224, 'recall_consonant': 0.994428, 'recall_word': 0.986042, 'acc_grapheme': 0.98557, 'acc_vowel': 0.994791, 'acc_consonant': 0.994816, 'acc_word': 0.985968, 'loss_grapheme': 0.066202, 'loss_vowel': 0.046747, 'loss_consonant': 0.037303, 'loss_word': 0.057806}\n",
      "   58 | 0.000064 | 160640/160716 | 23.3762 | 10.2762 |\n",
      "val: {'recall': 0.9901, 'recall_grapheme': 0.986157, 'recall_vowel': 0.994165, 'recall_consonant': 0.99392, 'recall_word': 0.985931, 'acc_grapheme': 0.984797, 'acc_vowel': 0.994766, 'acc_consonant': 0.994716, 'acc_word': 0.985819, 'loss_grapheme': 0.08734, 'loss_vowel': 0.068897, 'loss_consonant': 0.052771, 'loss_word': 0.066828}\n",
      "   59 | 0.000059 | 160640/160716 | 1.7800 | 8.9940 |||\n",
      "val: {'recall': 0.990126, 'recall_grapheme': 0.985598, 'recall_vowel': 0.993947, 'recall_consonant': 0.995362, 'recall_word': 0.98566, 'acc_grapheme': 0.984747, 'acc_vowel': 0.994791, 'acc_consonant': 0.994791, 'acc_word': 0.98562, 'loss_grapheme': 0.066493, 'loss_vowel': 0.043666, 'loss_consonant': 0.03583, 'loss_word': 0.059025}\n",
      "   60 | 0.000053 | 160640/160716 | 20.1685 | 9.8402 ||\n",
      "val: {'recall': 0.989603, 'recall_grapheme': 0.984955, 'recall_vowel': 0.993814, 'recall_consonant': 0.994688, 'recall_word': 0.98427, 'acc_grapheme': 0.984074, 'acc_vowel': 0.994791, 'acc_consonant': 0.994816, 'acc_word': 0.984, 'loss_grapheme': 0.075711, 'loss_vowel': 0.04611, 'loss_consonant': 0.036431, 'loss_word': 0.070608}\n",
      "   61 | 0.000048 | 160640/160716 | 1.9327 | 8.9992 |||\n",
      "val: {'recall': 0.991297, 'recall_grapheme': 0.987962, 'recall_vowel': 0.994989, 'recall_consonant': 0.994277, 'recall_word': 0.986626, 'acc_grapheme': 0.986542, 'acc_vowel': 0.995165, 'acc_consonant': 0.995215, 'acc_word': 0.986542, 'loss_grapheme': 0.052072, 'loss_vowel': 0.026874, 'loss_consonant': 0.023086, 'loss_word': 0.051561}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###>>>>> saved\n",
      "   62 | 0.000043 | 160640/160716 | 14.2375 | 9.8167 ||\n",
      "val: {'recall': 0.990957, 'recall_grapheme': 0.987066, 'recall_vowel': 0.994248, 'recall_consonant': 0.995446, 'recall_word': 0.985634, 'acc_grapheme': 0.985545, 'acc_vowel': 0.995015, 'acc_consonant': 0.994841, 'acc_word': 0.985495, 'loss_grapheme': 0.061837, 'loss_vowel': 0.035317, 'loss_consonant': 0.029587, 'loss_word': 0.058413}\n",
      "   63 | 0.000038 | 160640/160716 | 2.2644 | 8.7743 |||\n",
      "val: {'recall': 0.990761, 'recall_grapheme': 0.987458, 'recall_vowel': 0.994194, 'recall_consonant': 0.993933, 'recall_word': 0.986092, 'acc_grapheme': 0.985719, 'acc_vowel': 0.994841, 'acc_consonant': 0.994916, 'acc_word': 0.986018, 'loss_grapheme': 0.065662, 'loss_vowel': 0.04173, 'loss_consonant': 0.033715, 'loss_word': 0.058626}\n",
      "   64 | 0.000034 | 160640/160716 | 1.5652 | 9.3138 ||\n",
      "val: {'recall': 0.990578, 'recall_grapheme': 0.986867, 'recall_vowel': 0.994158, 'recall_consonant': 0.99442, 'recall_word': 0.986565, 'acc_grapheme': 0.985719, 'acc_vowel': 0.994916, 'acc_consonant': 0.995065, 'acc_word': 0.986467, 'loss_grapheme': 0.068208, 'loss_vowel': 0.049161, 'loss_consonant': 0.038922, 'loss_word': 0.057621}\n",
      "   65 | 0.000029 | 160640/160716 | 5.4438 | 9.0691 ||\n",
      "val: {'recall': 0.989897, 'recall_grapheme': 0.985336, 'recall_vowel': 0.993847, 'recall_consonant': 0.995068, 'recall_word': 0.983789, 'acc_grapheme': 0.983925, 'acc_vowel': 0.994642, 'acc_consonant': 0.994592, 'acc_word': 0.983451, 'loss_grapheme': 0.080109, 'loss_vowel': 0.049874, 'loss_consonant': 0.039563, 'loss_word': 0.073546}\n",
      "   66 | 0.000026 | 160640/160716 | 1.7680 | 9.7913 |||\n",
      "val: {'recall': 0.991441, 'recall_grapheme': 0.988137, 'recall_vowel': 0.994579, 'recall_consonant': 0.994913, 'recall_word': 0.986227, 'acc_grapheme': 0.986442, 'acc_vowel': 0.99514, 'acc_consonant': 0.99509, 'acc_word': 0.986043, 'loss_grapheme': 0.05169, 'loss_vowel': 0.02563, 'loss_consonant': 0.022618, 'loss_word': 0.052093}\n",
      "###>>>>> saved\n",
      "   67 | 0.000022 | 160640/160716 | 1.8292 | 9.1545 |||\n",
      "val: {'recall': 0.99071, 'recall_grapheme': 0.986751, 'recall_vowel': 0.99374, 'recall_consonant': 0.995596, 'recall_word': 0.985633, 'acc_grapheme': 0.985345, 'acc_vowel': 0.994766, 'acc_consonant': 0.994941, 'acc_word': 0.98552, 'loss_grapheme': 0.070823, 'loss_vowel': 0.050678, 'loss_consonant': 0.039281, 'loss_word': 0.060614}\n",
      "   68 | 0.000018 | 160640/160716 | 1.7537 | 9.3453 |||\n",
      "val: {'recall': 0.990929, 'recall_grapheme': 0.987499, 'recall_vowel': 0.99436, 'recall_consonant': 0.994356, 'recall_word': 0.985948, 'acc_grapheme': 0.985819, 'acc_vowel': 0.995015, 'acc_consonant': 0.99504, 'acc_word': 0.985744, 'loss_grapheme': 0.057408, 'loss_vowel': 0.032028, 'loss_consonant': 0.026711, 'loss_word': 0.055513}\n",
      "   69 | 0.000015 | 160640/160716 | 24.2725 | 9.9105 ||\n",
      "val: {'recall': 0.989937, 'recall_grapheme': 0.985532, 'recall_vowel': 0.993882, 'recall_consonant': 0.994801, 'recall_word': 0.984041, 'acc_grapheme': 0.984074, 'acc_vowel': 0.994766, 'acc_consonant': 0.994691, 'acc_word': 0.983875, 'loss_grapheme': 0.100865, 'loss_vowel': 0.076121, 'loss_consonant': 0.056339, 'loss_word': 0.08187}\n",
      "   70 | 0.000012 | 160640/160716 | 1.4185 | 9.1058 |||\n",
      "val: {'recall': 0.990922, 'recall_grapheme': 0.986874, 'recall_vowel': 0.994358, 'recall_consonant': 0.99558, 'recall_word': 0.985362, 'acc_grapheme': 0.985221, 'acc_vowel': 0.994916, 'acc_consonant': 0.994891, 'acc_word': 0.985196, 'loss_grapheme': 0.061741, 'loss_vowel': 0.034865, 'loss_consonant': 0.028937, 'loss_word': 0.059516}\n",
      "   71 | 0.000010 | 160640/160716 | 9.2044 | 8.2380 |||\n",
      "val: {'recall': 0.990037, 'recall_grapheme': 0.986232, 'recall_vowel': 0.993841, 'recall_consonant': 0.993843, 'recall_word': 0.984982, 'acc_grapheme': 0.984722, 'acc_vowel': 0.994816, 'acc_consonant': 0.994816, 'acc_word': 0.984822, 'loss_grapheme': 0.070003, 'loss_vowel': 0.045819, 'loss_consonant': 0.035236, 'loss_word': 0.063963}\n",
      "   72 | 0.000008 | 160640/160716 | 1.8479 | 8.9904 ||\n",
      "val: {'recall': 0.991753, 'recall_grapheme': 0.988965, 'recall_vowel': 0.994685, 'recall_consonant': 0.994398, 'recall_word': 0.987545, 'acc_grapheme': 0.987663, 'acc_vowel': 0.995339, 'acc_consonant': 0.995589, 'acc_word': 0.987514, 'loss_grapheme': 0.045049, 'loss_vowel': 0.021482, 'loss_consonant': 0.018373, 'loss_word': 0.044927}\n",
      "###>>>>> saved\n",
      "   73 | 0.000006 | 160640/160716 | 9.1983 | 9.5933 |||\n",
      "val: {'recall': 0.989614, 'recall_grapheme': 0.984842, 'recall_vowel': 0.994057, 'recall_consonant': 0.994717, 'recall_word': 0.983531, 'acc_grapheme': 0.983651, 'acc_vowel': 0.994667, 'acc_consonant': 0.994467, 'acc_word': 0.983252, 'loss_grapheme': 0.092852, 'loss_vowel': 0.067253, 'loss_consonant': 0.050614, 'loss_word': 0.07904}\n",
      "   74 | 0.000004 | 160640/160716 | 1.4545 | 9.7739 |||\n",
      "val: {'recall': 0.990563, 'recall_grapheme': 0.98637, 'recall_vowel': 0.994102, 'recall_consonant': 0.99541, 'recall_word': 0.985126, 'acc_grapheme': 0.984897, 'acc_vowel': 0.994891, 'acc_consonant': 0.994941, 'acc_word': 0.984972, 'loss_grapheme': 0.0779, 'loss_vowel': 0.05669, 'loss_consonant': 0.04261, 'loss_word': 0.066337}\n",
      "   75 | 0.000002 | 160640/160716 | 9.7651 | 10.3642 ||\n",
      "val: {'recall': 0.990108, 'recall_grapheme': 0.985472, 'recall_vowel': 0.994129, 'recall_consonant': 0.99536, 'recall_word': 0.984336, 'acc_grapheme': 0.984349, 'acc_vowel': 0.994816, 'acc_consonant': 0.994691, 'acc_word': 0.984174, 'loss_grapheme': 0.083095, 'loss_vowel': 0.057395, 'loss_consonant': 0.043645, 'loss_word': 0.072501}\n",
      "   76 | 0.000001 | 160640/160716 | 4.3687 | 9.0087 ||\n",
      "val: {'recall': 0.991849, 'recall_grapheme': 0.988871, 'recall_vowel': 0.99453, 'recall_consonant': 0.995123, 'recall_word': 0.987341, 'acc_grapheme': 0.987489, 'acc_vowel': 0.995115, 'acc_consonant': 0.995589, 'acc_word': 0.987264, 'loss_grapheme': 0.04666, 'loss_vowel': 0.022652, 'loss_consonant': 0.019449, 'loss_word': 0.047008}\n",
      "###>>>>> saved\n",
      "   77 | 0.000001 | 160640/160716 | 1.6354 | 9.0776 |||\n",
      "val: {'recall': 0.99168, 'recall_grapheme': 0.988871, 'recall_vowel': 0.994311, 'recall_consonant': 0.994666, 'recall_word': 0.988053, 'acc_grapheme': 0.987539, 'acc_vowel': 0.99509, 'acc_consonant': 0.995663, 'acc_word': 0.987987, 'loss_grapheme': 0.049645, 'loss_vowel': 0.03046, 'loss_consonant': 0.02489, 'loss_word': 0.045618}\n",
      "   78 | 0.000000 | 160640/160716 | 17.4414 | 9.8139 ||\n",
      "val: {'recall': 0.990243, 'recall_grapheme': 0.986462, 'recall_vowel': 0.993881, 'recall_consonant': 0.994169, 'recall_word': 0.985778, 'acc_grapheme': 0.98537, 'acc_vowel': 0.994816, 'acc_consonant': 0.994966, 'acc_word': 0.985645, 'loss_grapheme': 0.06937, 'loss_vowel': 0.047443, 'loss_consonant': 0.036351, 'loss_word': 0.06158}\n",
      "   79 | 0.000000 | 160640/160716 | 20.8207 | 9.5504 ||\n",
      "val: {'recall': 0.99024, 'recall_grapheme': 0.986422, 'recall_vowel': 0.993899, 'recall_consonant': 0.994218, 'recall_word': 0.985278, 'acc_grapheme': 0.985096, 'acc_vowel': 0.994841, 'acc_consonant': 0.994866, 'acc_word': 0.985121, 'loss_grapheme': 0.071153, 'loss_vowel': 0.047183, 'loss_consonant': 0.036361, 'loss_word': 0.064101}\n",
      "CYCLE: 2\n",
      "{'recall': 0.99024, 'recall_grapheme': 0.986422, 'recall_vowel': 0.993899, 'recall_consonant': 0.994218, 'recall_word': 0.985278, 'acc_grapheme': 0.985096, 'acc_vowel': 0.994841, 'acc_consonant': 0.994866, 'acc_word': 0.985121, 'loss_grapheme': 0.071153, 'loss_vowel': 0.047183, 'loss_consonant': 0.036361, 'loss_word': 0.064101}\n",
      "    0 | 0.000040 | 160640/160716 | 0.9221 | 8.1568 |||\n",
      "val: {'recall': 0.991632, 'recall_grapheme': 0.988624, 'recall_vowel': 0.994396, 'recall_consonant': 0.994885, 'recall_word': 0.987832, 'acc_grapheme': 0.987414, 'acc_vowel': 0.995115, 'acc_consonant': 0.995589, 'acc_word': 0.987788, 'loss_grapheme': 0.046572, 'loss_vowel': 0.026259, 'loss_consonant': 0.021334, 'loss_word': 0.044475}\n",
      "    1 | 0.000080 | 160640/160716 | 8.4542 | 9.3899 |||\n",
      "val: {'recall': 0.988961, 'recall_grapheme': 0.983462, 'recall_vowel': 0.993351, 'recall_consonant': 0.995569, 'recall_word': 0.982735, 'acc_grapheme': 0.98228, 'acc_vowel': 0.994367, 'acc_consonant': 0.994143, 'acc_word': 0.982504, 'loss_grapheme': 0.124652, 'loss_vowel': 0.098291, 'loss_consonant': 0.068472, 'loss_word': 0.09825}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2 | 0.000119 | 160640/160716 | 1.3608 | 9.7764 |||\n",
      "val: {'recall': 0.990208, 'recall_grapheme': 0.985477, 'recall_vowel': 0.99368, 'recall_consonant': 0.996198, 'recall_word': 0.984568, 'acc_grapheme': 0.984049, 'acc_vowel': 0.994318, 'acc_consonant': 0.994467, 'acc_word': 0.984373, 'loss_grapheme': 0.097951, 'loss_vowel': 0.075754, 'loss_consonant': 0.055121, 'loss_word': 0.079259}\n",
      "    3 | 0.000159 | 160640/160716 | 1.9220 | 8.7770 |||\n",
      "val: {'recall': 0.990753, 'recall_grapheme': 0.986459, 'recall_vowel': 0.994259, 'recall_consonant': 0.995835, 'recall_word': 0.984751, 'acc_grapheme': 0.984573, 'acc_vowel': 0.994841, 'acc_consonant': 0.994791, 'acc_word': 0.984598, 'loss_grapheme': 0.058571, 'loss_vowel': 0.028828, 'loss_consonant': 0.025588, 'loss_word': 0.058152}\n",
      "    4 | 0.000198 | 160640/160716 | 11.5189 | 8.8218 |\n",
      "val: {'recall': 0.98848, 'recall_grapheme': 0.982883, 'recall_vowel': 0.993702, 'recall_consonant': 0.994454, 'recall_word': 0.982399, 'acc_grapheme': 0.982454, 'acc_vowel': 0.994293, 'acc_consonant': 0.994019, 'acc_word': 0.98223, 'loss_grapheme': 0.094207, 'loss_vowel': 0.058629, 'loss_consonant': 0.043358, 'loss_word': 0.085457}\n",
      "    5 | 0.000197 | 160640/160716 | 7.8223 | 9.3704 ||\n",
      "val: {'recall': 0.988848, 'recall_grapheme': 0.984437, 'recall_vowel': 0.99311, 'recall_consonant': 0.993407, 'recall_word': 0.983131, 'acc_grapheme': 0.982654, 'acc_vowel': 0.994442, 'acc_consonant': 0.994467, 'acc_word': 0.982803, 'loss_grapheme': 0.077996, 'loss_vowel': 0.042512, 'loss_consonant': 0.034775, 'loss_word': 0.074265}\n",
      "    6 | 0.000196 | 160640/160716 | 10.4453 | 10.4846 |\n",
      "val: {'recall': 0.988977, 'recall_grapheme': 0.98362, 'recall_vowel': 0.993081, 'recall_consonant': 0.995585, 'recall_word': 0.982876, 'acc_grapheme': 0.982729, 'acc_vowel': 0.994417, 'acc_consonant': 0.994193, 'acc_word': 0.982629, 'loss_grapheme': 0.107377, 'loss_vowel': 0.074248, 'loss_consonant': 0.053547, 'loss_word': 0.091108}\n",
      "    7 | 0.000195 | 160640/160716 | 10.8397 | 9.1072 |\n",
      "val: {'recall': 0.989624, 'recall_grapheme': 0.985033, 'recall_vowel': 0.993803, 'recall_consonant': 0.994629, 'recall_word': 0.983221, 'acc_grapheme': 0.983925, 'acc_vowel': 0.994318, 'acc_consonant': 0.994168, 'acc_word': 0.983127, 'loss_grapheme': 0.082004, 'loss_vowel': 0.048056, 'loss_consonant': 0.039483, 'loss_word': 0.077357}\n",
      "    8 | 0.000194 | 160640/160716 | 15.9374 | 9.2933 |\n",
      "val: {'recall': 0.990427, 'recall_grapheme': 0.986298, 'recall_vowel': 0.993111, 'recall_consonant': 0.996, 'recall_word': 0.985114, 'acc_grapheme': 0.984598, 'acc_vowel': 0.994442, 'acc_consonant': 0.994542, 'acc_word': 0.984897, 'loss_grapheme': 0.06936, 'loss_vowel': 0.042136, 'loss_consonant': 0.033181, 'loss_word': 0.065625}\n",
      "    9 | 0.000192 | 160640/160716 | 23.4438 | 9.5985 |\n",
      "val: {'recall': 0.988611, 'recall_grapheme': 0.983779, 'recall_vowel': 0.993571, 'recall_consonant': 0.993315, 'recall_word': 0.982971, 'acc_grapheme': 0.982978, 'acc_vowel': 0.994193, 'acc_consonant': 0.994417, 'acc_word': 0.982878, 'loss_grapheme': 0.09145, 'loss_vowel': 0.065904, 'loss_consonant': 0.048229, 'loss_word': 0.078327}\n",
      "   10 | 0.000191 | 160640/160716 | 11.8887 | 8.8878 ||\n",
      "val: {'recall': 0.989309, 'recall_grapheme': 0.983772, 'recall_vowel': 0.994063, 'recall_consonant': 0.995629, 'recall_word': 0.981777, 'acc_grapheme': 0.982579, 'acc_vowel': 0.994542, 'acc_consonant': 0.994118, 'acc_word': 0.981632, 'loss_grapheme': 0.098863, 'loss_vowel': 0.063417, 'loss_consonant': 0.047496, 'loss_word': 0.089852}\n",
      "   11 | 0.000189 | 160640/160716 | 2.6841 | 9.5044 |||\n",
      "val: {'recall': 0.990939, 'recall_grapheme': 0.987787, 'recall_vowel': 0.993868, 'recall_consonant': 0.994313, 'recall_word': 0.985869, 'acc_grapheme': 0.986392, 'acc_vowel': 0.994766, 'acc_consonant': 0.994891, 'acc_word': 0.985769, 'loss_grapheme': 0.058299, 'loss_vowel': 0.03126, 'loss_consonant': 0.026283, 'loss_word': 0.057554}\n",
      "   12 | 0.000187 | 160640/160716 | 6.2896 | 8.4127 ||\n",
      "val: {'recall': 0.987955, 'recall_grapheme': 0.981722, 'recall_vowel': 0.993719, 'recall_consonant': 0.994658, 'recall_word': 0.980891, 'acc_grapheme': 0.981507, 'acc_vowel': 0.994567, 'acc_consonant': 0.993944, 'acc_word': 0.98076, 'loss_grapheme': 0.084873, 'loss_vowel': 0.04429, 'loss_consonant': 0.038741, 'loss_word': 0.082481}\n",
      "   13 | 0.000185 | 160640/160716 | 17.5722 | 9.4327 ||\n",
      "val: {'recall': 0.989867, 'recall_grapheme': 0.985119, 'recall_vowel': 0.993989, 'recall_consonant': 0.995241, 'recall_word': 0.983924, 'acc_grapheme': 0.984049, 'acc_vowel': 0.994691, 'acc_consonant': 0.994816, 'acc_word': 0.9838, 'loss_grapheme': 0.078634, 'loss_vowel': 0.051283, 'loss_consonant': 0.039022, 'loss_word': 0.073052}\n",
      "   14 | 0.000183 | 160640/160716 | 2.1034 | 9.2215 |||\n",
      "val: {'recall': 0.990581, 'recall_grapheme': 0.986482, 'recall_vowel': 0.993305, 'recall_consonant': 0.996057, 'recall_word': 0.986122, 'acc_grapheme': 0.985694, 'acc_vowel': 0.994766, 'acc_consonant': 0.994716, 'acc_word': 0.985968, 'loss_grapheme': 0.069631, 'loss_vowel': 0.050291, 'loss_consonant': 0.039131, 'loss_word': 0.059784}\n",
      "   15 | 0.000182 | 067840/160716 | 10.4136 | 10.2898 |"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-6df1386d1bbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-c7bee7fc43c3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcycle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_cycles\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CYCLE:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtrain_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-c7bee7fc43c3>\u001b[0m in \u001b[0;36mtrain_cycle\u001b[0;34m(args, model, optimizer, lr_scheduler)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0;31m#loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(args, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_model(model, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
