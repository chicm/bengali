{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time, gc\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pretrainedmodels\n",
    "from argparse import Namespace\n",
    "from sklearn.utils import shuffle\n",
    "from apex import amp\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from cvcore.data.auto_augment import RandAugment\n",
    "from PIL import Image\n",
    "from utils import bn_update, moving_average, copy_model\n",
    "from augmix import RandomAugMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_map.csv\t\t       train.csv\r\n",
      "sample_submission.csv\t       train.csv.zip\r\n",
      "test.csv\t\t       train_image_data_0.parquet\r\n",
      "test_image_data_0.parquet      train_image_data_0.parquet.zip\r\n",
      "test_image_data_0.parquet.zip  train_image_data_1.parquet\r\n",
      "test_image_data_1.parquet      train_image_data_1.parquet.zip\r\n",
      "test_image_data_1.parquet.zip  train_image_data_2.parquet\r\n",
      "test_image_data_2.parquet      train_image_data_2.parquet.zip\r\n",
      "test_image_data_2.parquet.zip  train_image_data_3.parquet\r\n",
      "test_image_data_3.parquet      train_image_data_3.parquet.zip\r\n",
      "test_image_data_3.parquet.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/chec/data/bengali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /home/chec/data/bengali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/chec/data/bengali'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "class_map_df = pd.read_csv(f'{DATA_DIR}/class_map.csv')\n",
    "sample_sub_df = pd.read_csv(f'{DATA_DIR}/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>ক্ট্রো</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_1</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>হ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>খ্রী</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_3</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>র্টি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_4</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>থ্রো</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  grapheme_root  vowel_diacritic  consonant_diacritic grapheme\n",
       "0  Train_0             15                9                    5   ক্ট্রো\n",
       "1  Train_1            159                0                    0        হ\n",
       "2  Train_2             22                3                    5     খ্রী\n",
       "3  Train_3             53                2                    2     র্টি\n",
       "4  Train_4             71                9                    5     থ্রো"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5621946804610382"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 137\n",
    "WIDTH = 236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import albumentations as albu\n",
    "augmix = RandomAugMix(severity=3, width=3, alpha=1., p=1.)\n",
    "\n",
    "def get_train_augs():\n",
    "    #if np.random.rand() > 1.6:\n",
    "    #    return RandAugment(n=2, m=27), 1\n",
    "    #else:\n",
    "    #    return augmix, 2\n",
    "    return RandAugment(n=2, m=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.arange(10).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class BengaliDataset(Dataset):\n",
    "    def __init__(self, df, img_df, train_mode=True, test_mode=False):\n",
    "        self.df = df\n",
    "        self.img_df = img_df\n",
    "        self.train_mode = train_mode\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = self.get_img(row.image_id)\n",
    "        orig_img = img.copy()\n",
    "        #print(img.shape)\n",
    "        #if self.train_mode:\n",
    "        #    augs = get_train_augs()\n",
    "            #img = augs(image=img)['image']\n",
    "            #if aug_type == 1:\n",
    "        #    img = np.asarray(augs(Image.fromarray(img)))\n",
    "            #elif aug_type == 2:\n",
    "            #    img = augs(image=img)['image'].astype(np.uint8)\n",
    "            #else:\n",
    "            #    raise RuntimeError('error aug_type')\n",
    "        \n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "        orig_img = np.expand_dims(orig_img, axis=-1)\n",
    "        \n",
    "        #print('###', img.shape)\n",
    "        #img = np.concatenate([img, img, img], 2)\n",
    "        #print('>>>', img.shape)\n",
    "        \n",
    "        # taken from https://www.kaggle.com/iafoss/image-preprocessing-128x128\n",
    "        #MEAN = [ 0.06922848809290576,  0.06922848809290576,  0.06922848809290576]\n",
    "        #STD = [ 0.20515700083327537,  0.20515700083327537,  0.20515700083327537]\n",
    "        \n",
    "        img = transforms.functional.to_tensor(img)\n",
    "        orig_img = transforms.functional.to_tensor(orig_img)\n",
    "        \n",
    "        #img = transforms.functional.normalize(img, mean=MEAN, std=STD)\n",
    "        \n",
    "        if self.test_mode:\n",
    "            return img\n",
    "        elif self.train_mode:\n",
    "            return img, orig_img, torch.tensor([row.grapheme_root, row.vowel_diacritic, row.consonant_diacritic, row.word_label])\n",
    "        else:\n",
    "            return img, torch.tensor([row.grapheme_root, row.vowel_diacritic, row.consonant_diacritic, row.word_label])\n",
    "                    \n",
    "    def get_img(self, img_id):\n",
    "        return 255 - self.img_df.loc[img_id].values.reshape(HEIGHT, WIDTH).astype(np.uint8)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "def get_train_val_loaders(batch_size=4, val_batch_size=4, ifold=0, dev_mode=False):\n",
    "    train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "\n",
    "    train_df = shuffle(train_df, random_state=1234)\n",
    "\n",
    "    grapheme_words = np.unique(train_df.grapheme.values)\n",
    "    grapheme_words_dict = {grapheme: i for i, grapheme in enumerate(grapheme_words)}\n",
    "    train_df['word_label'] = train_df['grapheme'].map(lambda x: grapheme_words_dict[x])\n",
    "\n",
    "    print(train_df.shape)\n",
    "\n",
    "    if dev_mode:\n",
    "        img_df = pd.read_parquet(f'{DATA_DIR}/train_image_data_0.parquet').set_index('image_id')\n",
    "        train_df = train_df.iloc[:1000]\n",
    "    else:\n",
    "        img_dfs = [pd.read_parquet(f'{DATA_DIR}/train_image_data_{i}.parquet') for i in range(4)]\n",
    "        img_df = pd.concat(img_dfs, axis=0).set_index('image_id')\n",
    "    print(img_df.shape)\n",
    "    #split_index = int(len(train_df) * 0.9)\n",
    "    \n",
    "    #train = train_df.iloc[:split_index]\n",
    "    #val = train_df.iloc[split_index:]\n",
    "    \n",
    "    kf = StratifiedKFold(5, random_state=1234, shuffle=True)\n",
    "    for i, (train_idx, val_idx) in enumerate(kf.split(train_df, train_df['grapheme_root'].values)):\n",
    "        if i == ifold:\n",
    "            #print(val_idx)\n",
    "            train = train_df.iloc[train_idx]\n",
    "            val = train_df.iloc[val_idx]\n",
    "            break\n",
    "    assert i == ifold\n",
    "    print(train.shape, val.shape)\n",
    "    \n",
    "    train_ds = BengaliDataset(train, img_df, True, False)\n",
    "    val_ds = BengaliDataset(val, img_df, False, False)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "    train_loader.num = len(train_ds)\n",
    "\n",
    "    val_loader = DataLoader(val_ds, batch_size=val_batch_size, shuffle=False, num_workers=8, drop_last=False)\n",
    "    val_loader.num = len(val_ds)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader, val_loader = get_train_val_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x in train_loader:\n",
    "#    print(x)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'resnet50' # could be fbresnet152 or inceptionresnetv2\n",
    "#model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet').cuda()\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import timm\n",
    "from timm.models.activations import Swish, Mish\n",
    "from timm.models.adaptive_avgmax_pool import SelectAdaptivePool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = [ 0.06922848809290576 ]\n",
    "STD = [ 0.20515700083327537 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengaliNet4(nn.Module):\n",
    "    def __init__(self, backbone_name='se_resnext101_32x4d'):\n",
    "        super(BengaliNet4, self).__init__()\n",
    "        self.n_grapheme = 168\n",
    "        self.n_vowel = 11\n",
    "        self.n_consonant = 7\n",
    "        self.n_word = 1295\n",
    "        self.backbone_name = backbone_name\n",
    "        \n",
    "        self.num_classes = self.n_grapheme + self.n_vowel + self.n_consonant + self.n_word\n",
    "        \n",
    "        self.backbone = pretrainedmodels.__dict__[self.backbone_name](num_classes=1000, pretrained='imagenet')\n",
    "        self.fc = nn.Linear(self.backbone.last_linear.in_features, self.num_classes)\n",
    "        \n",
    "        self.num_p2_features = self.backbone.layer2[-1].se_module.fc2.out_channels\n",
    "        self.num_p3_features = self.backbone.layer3[-1].se_module.fc2.out_channels\n",
    "        self.p2_head = nn.Conv2d(self.num_p2_features, self.num_p2_features * 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.p3_head = nn.Conv2d(self.num_p3_features, self.num_p3_features * 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.num_p2_features * 4)\n",
    "        self.bn3 = nn.BatchNorm2d(self.num_p3_features * 4)\n",
    "        self.act2 = Swish()\n",
    "        self.act3 = Swish()\n",
    "        \n",
    "        self.fc_aux1 = nn.Linear(self.num_p3_features * 4, self.num_classes)\n",
    "        self.fc_aux2 = nn.Linear(self.num_p2_features * 4, self.num_classes)\n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        for fc in [self.fc, self.fc_aux1, self.fc_aux2]:\n",
    "            nn.init.zeros_(fc.bias.data)\n",
    "\n",
    "        print('init model4')\n",
    "        \n",
    "    def features(self, x):\n",
    "        x = self.backbone.layer0(x); #print(x.size())\n",
    "        x = self.backbone.layer1(x); #print(x.size())\n",
    "        x = self.backbone.layer2(x); p2 = x; p2 = self.p2_head(p2); p2 = self.bn2(p2); p2 = self.act2(p2) #print(x.size())\n",
    "        x = self.backbone.layer3(x); p3 = x; p3 = self.p3_head(p3); p3 = self.bn3(p3); p3 = self.act3(p3) #print(x.size())\n",
    "        x = self.backbone.layer4(x); #print(x.size())\n",
    "        return x, p2, p3\n",
    "        \n",
    "    def logits(self, x, p2, p3):\n",
    "        x = self.avg_pool(x)\n",
    "        #x = F.dropout2d(x, 0.2, self.training)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        p2 = self.avg_pool(p2)\n",
    "        p2 = torch.flatten(p2, 1)\n",
    "        \n",
    "        p3 = self.avg_pool(p3)\n",
    "        p3 = torch.flatten(p3, 1)\n",
    "        return self.fc(x), self.fc_aux1(p3), self.fc_aux2(p2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, size=(224,224), mode='bilinear', align_corners=False)\n",
    "        for i in range(len(x)):\n",
    "            transforms.functional.normalize(x[i], mean=MEAN, std=STD, inplace=True)\n",
    "        x = torch.cat([x,x,x], 1)\n",
    "        #x = self.conv0(x)\n",
    "        #print(x.size())\n",
    "        x, p2, p3 = self.features(x)\n",
    "        x, logits_aux1, logits_aux2 = self.logits(x, p2, p3)\n",
    "\n",
    "        return x, logits_aux1, logits_aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './model4-ckps'\n",
    "def create_model(args):\n",
    "    model = BengaliNet4(args.backbone)\n",
    "    model_file = os.path.join(MODEL_DIR, args.backbone, args.ckp_name)\n",
    "\n",
    "    parent_dir = os.path.dirname(model_file)\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "\n",
    "    print('model file: {}, exist: {}'.format(model_file, os.path.exists(model_file)))\n",
    "\n",
    "    if os.path.exists(model_file):\n",
    "        print('loading {}...'.format(model_file))\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "    \n",
    "    return model, model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bnet = BengaliNet('se_resnext50_32x4d').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bnet(torch.randn((2, 1, 137, 236)).cuda()).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.111111"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(1/9, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "\n",
    "\n",
    "def calc_metrics(preds0, preds1, preds2, preds3, y):\n",
    "    assert len(y) == len(preds0) == len(preds1) == len(preds2) == len(preds3)\n",
    "\n",
    "    recall_grapheme = sklearn.metrics.recall_score(preds0, y[:, 0], average='macro')\n",
    "    recall_vowel = sklearn.metrics.recall_score(preds1, y[:, 1], average='macro')\n",
    "    recall_consonant = sklearn.metrics.recall_score(preds2, y[:, 2], average='macro')\n",
    "    recall_word = sklearn.metrics.recall_score(preds3, y[:, 3], average='macro')\n",
    "    \n",
    "    scores = [recall_grapheme, recall_vowel, recall_consonant]\n",
    "    final_recall_score = np.average(scores, weights=[2, 1, 1])\n",
    "    \n",
    "    metrics = {}\n",
    "    metrics['recall'] = round(final_recall_score, 6)\n",
    "    metrics['recall_grapheme'] = round(recall_grapheme, 6)\n",
    "    metrics['recall_vowel'] = round(recall_vowel, 6)\n",
    "    metrics['recall_consonant'] = round(recall_consonant, 6)\n",
    "    metrics['recall_word'] = round(recall_word, 6)\n",
    "    \n",
    "    metrics['acc_grapheme'] = round((preds0 == y[:, 0]).sum() / len(y), 6)\n",
    "    metrics['acc_vowel'] = round((preds1 == y[:, 1]).sum() / len(y), 6)\n",
    "    metrics['acc_consonant'] = round((preds2 == y[:, 2]).sum() / len(y), 6)\n",
    "    metrics['acc_word'] = round((preds3 == y[:, 3]).sum() / len(y), 6)    \n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(outputs, y_true):\n",
    "    # outputs: (N, 182)\n",
    "    # y_true: (N, 3)\n",
    "    \n",
    "    outputs = torch.split(outputs, [168, 11, 7, 1295], dim=1)\n",
    "    loss0 = F.cross_entropy(outputs[0], y_true[:, 0], reduction='mean')\n",
    "    loss1 = F.cross_entropy(outputs[1], y_true[:, 1], reduction='mean')\n",
    "    loss2 = F.cross_entropy(outputs[2], y_true[:, 2], reduction='mean')\n",
    "    loss3 = F.cross_entropy(outputs[3], y_true[:, 3], reduction='mean')\n",
    "    \n",
    "    return loss0 + loss1 + loss2 + loss3 #, loss0.item(), loss1.item(), loss2.item()\n",
    "    #return loss3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    loss0, loss1, loss2, loss3 = 0., 0., 0., 0.\n",
    "    preds0, preds1, preds2, preds3 = [], [], [], []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            y_true.append(y)\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            outputs, outputs_aux1, outputs_aux2 = model(x)\n",
    "            #avg_outputs = torch.mean(torch.stack([outputs, outputs_aux1, outputs_aux2], 0), 0)\n",
    "            outputs = torch.split(outputs, [168, 11, 7, 1295], dim=1)\n",
    "            \n",
    "            preds0.append(torch.max(outputs[0], dim=1)[1])\n",
    "            preds1.append(torch.max(outputs[1], dim=1)[1])\n",
    "            preds2.append(torch.max(outputs[2], dim=1)[1])\n",
    "            preds3.append(torch.max(outputs[3], dim=1)[1])\n",
    "            loss0 += F.cross_entropy(outputs[0], y[:, 0], reduction='sum').item()\n",
    "            loss1 += F.cross_entropy(outputs[1], y[:, 1], reduction='sum').item()\n",
    "            loss2 += F.cross_entropy(outputs[2], y[:, 2], reduction='sum').item()\n",
    "            loss3 += F.cross_entropy(outputs[3], y[:, 3], reduction='sum').item()\n",
    "            \n",
    "            # for debug\n",
    "            #metrics = {}\n",
    "            #metrics['loss_grapheme'] =  F.cross_entropy(outputs[0], y[:, 0], reduction='mean').item()\n",
    "            #metrics['loss_vowel'] =  F.cross_entropy(outputs[1], y[:, 1], reduction='mean').item()\n",
    "            #metrics['loss_consonant'] =  F.cross_entropy(outputs[2], y[:, 2], reduction='mean').item()\n",
    "            #return metrics\n",
    "    \n",
    "    preds0 = torch.cat(preds0, 0).cpu().numpy()\n",
    "    preds1 = torch.cat(preds1, 0).cpu().numpy()\n",
    "    preds2 = torch.cat(preds2, 0).cpu().numpy()\n",
    "    preds3 = torch.cat(preds3, 0).cpu().numpy()\n",
    "    \n",
    "    y_true = torch.cat(y_true, 0).numpy()\n",
    "    \n",
    "    #print('y_true:', y_true.shape)\n",
    "    #print('preds0:', preds0.shape)\n",
    "    \n",
    "    metrics = calc_metrics(preds0, preds1, preds2, preds3, y_true)\n",
    "    metrics['loss_grapheme'] = round(loss0 / val_loader.num, 6)\n",
    "    metrics['loss_vowel'] = round(loss1 / val_loader.num, 6)\n",
    "    metrics['loss_consonant'] = round(loss2 / val_loader.num, 6)\n",
    "    metrics['loss_word'] = round(loss3 / val_loader.num, 6)\n",
    "    \n",
    "    return metrics\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lrs(optimizer):\n",
    "    lrs = []\n",
    "    for pgs in optimizer.state_dict()['param_groups']:\n",
    "        lrs.append(pgs['lr'])\n",
    "    lrs = ['{:.6f}'.format(x) for x in lrs]\n",
    "    return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_file):\n",
    "    parent_dir = os.path.dirname(model_file)\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        torch.save(model.module.state_dict(), model_file)\n",
    "    else:\n",
    "        torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(data, targets, alpha=1):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets = targets[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    data = data * lam + shuffled_data * (1 - lam)\n",
    "    targets = (targets, shuffled_targets, lam)\n",
    "\n",
    "    return data, targets\n",
    "\n",
    "\n",
    "def mixup_criterion(outputs, targets):\n",
    "    targets1, targets2, lam = targets\n",
    "    #criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    return lam * criterion(outputs, targets1) + (1 - lam) * criterion(outputs, targets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox_old(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    H = size[2]\n",
    "    W = size[3]\n",
    "\n",
    "    x_margin_rate = 0.2\n",
    "\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * (1-x_margin_rate*2) * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "    \n",
    "    min_x_center = np.int(W * x_margin_rate + cut_w / 2)\n",
    "    max_x_center = np.int(W * (1-x_margin_rate) - cut_w / 2)\n",
    "    #print(min_x_center, max_x_center, lam, cut_w)\n",
    "    min_y_center = cut_h // 2\n",
    "    max_y_center = H - cut_h // 2\n",
    "    if max_y_center == min_y_center:\n",
    "        max_y_center += 1\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(min_x_center, max_x_center)\n",
    "    cy = np.random.randint(min_y_center, max_y_center)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    #print(bbx1, bbx2, bby1, bby2)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48517551026158534"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from over9000.over9000 import Over9000\n",
    "from over9000.radam import RAdam\n",
    "from gridmask import GridMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvcore.solver import WarmupCyclicalLR\n",
    "def make_optimizer(model, base_lr=4e-4, weight_decay=0., weight_decay_bias=0., epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    Create optimizer with per-layer learning rate and weight decay.\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    for key, value in model.named_parameters():\n",
    "        if not value.requires_grad:\n",
    "            continue\n",
    "        lr = base_lr\n",
    "        params += [{\"params\": [value], \"lr\": lr, \"weight_decay\": weight_decay_bias if 'bias' in key else weight_decay}]\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(params, lr, eps=epsilon)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(args, model, train_loader, epoch, optimizer, lr_scheduler, grid):\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (img, orig_img, targets) in enumerate(train_loader):\n",
    "        img, orig_img, targets  = img.cuda(), orig_img.cuda(), targets.cuda()\n",
    "        batch_size = img.size(0)\n",
    "        r = np.random.rand()\n",
    "\n",
    "        if r < 0.3:\n",
    "            # generate mixed sample\n",
    "            lam = np.random.beta(args.beta, args.beta)\n",
    "            rand_index = torch.randperm(img.size()[0]).cuda()\n",
    "            target_a = targets\n",
    "            target_b = targets[rand_index]\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(img.size(), lam)\n",
    "            img[:, :, bby1:bby2, bbx1:bbx2] = img[rand_index, :, bby1:bby2, bbx1:bbx2] #for new cutmix\n",
    "            #img[:, :, bbx1:bbx2, bby1:bby2] = img[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            \n",
    "            # adjust lambda to exactly match pixel ratio\n",
    "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (img.size()[-1] * img.size()[-2]))\n",
    "            # compute output\n",
    "            outputs, outputs_aux1, outputs_aux2 = model(img)\n",
    "            loss_primary = criterion(outputs, target_a) * lam + criterion(outputs, target_b) * (1. - lam)\n",
    "            loss_aux1 = criterion(outputs_aux1, target_a) * lam + criterion(outputs_aux1, target_b) * (1. - lam)\n",
    "            loss_aux2 = criterion(outputs_aux2, target_a) * lam + criterion(outputs_aux2, target_b) * (1. - lam)\n",
    "            loss = loss_primary + (loss_aux1 + loss_aux2)*0.8\n",
    "        elif r > 0.7:\n",
    "            img = grid(img)\n",
    "            outputs, outputs_aux1, outputs_aux2 = model(img)\n",
    "            loss_primary = criterion(outputs, targets)\n",
    "            loss_aux1 = criterion(outputs_aux1, targets)\n",
    "            loss_aux2 = criterion(outputs_aux2, targets)\n",
    "            loss = loss_primary + (loss_aux1 + loss_aux2)*0.8\n",
    "        else:\n",
    "            img, targets = mixup(img, targets)\n",
    "            outputs, outputs_aux1, outputs_aux2 = model(img)\n",
    "            loss_primary = mixup_criterion(outputs, targets)\n",
    "            loss_aux1 = mixup_criterion(outputs_aux1, targets)\n",
    "            loss_aux2 = mixup_criterion(outputs_aux2, targets)\n",
    "            loss = loss_primary + (loss_aux1 + loss_aux2)*0.8\n",
    "            #loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        #loss.backward()\n",
    "        lr_scheduler(optimizer, batch_idx, epoch)\n",
    "        optimizer.step()            \n",
    "        \n",
    "        current_lr = get_lrs(optimizer)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        print('\\r {:4d} | {:.6f} | {:06d}/{} | {:.4f} | {:.4f} |'.format(\n",
    "            epoch, float(current_lr[0]), batch_size*(batch_idx+1), train_loader.num, \n",
    "            loss.item(), train_loss/(batch_idx+1)), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metrics = 0.\n",
    "\n",
    "def validate_and_save(model, model_file, val_loader, save=False):\n",
    "    global best_metrics\n",
    "    best_key = 'recall'\n",
    "    val_metrics = validate(model, val_loader)\n",
    "    print('\\nval:', val_metrics)\n",
    "    \n",
    "    if val_metrics[best_key] > best_metrics:\n",
    "        best_metrics = val_metrics[best_key]\n",
    "        if save:\n",
    "            save_model(model, model_file)\n",
    "            print('###>>>>> saved', model_file)\n",
    "    model.train()\n",
    "    \n",
    "\n",
    "def train(args):\n",
    "    model, model_file = create_model(args)\n",
    "    model = model.cuda()\n",
    "    \n",
    "    #for param in model.backbone.parameters():\n",
    "    #    param.requires_grad = False\n",
    "\n",
    "    swa_model, _ = create_model(args)\n",
    "    swa_model = swa_model.cuda()\n",
    "    swa_model_file = model_file\n",
    "\n",
    "    optimizer = make_optimizer(model)\n",
    "    lr_scheduler = WarmupCyclicalLR(\n",
    "        \"cos\", args.base_lr, args.num_epochs, iters_per_epoch=len(train_loader), warmup_epochs=args.warmup_epochs)\n",
    "    \n",
    "    [model, swa_model], optimizer = amp.initialize([model, swa_model], optimizer, opt_level=\"O1\",verbosity=0)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "        swa_model = nn.DataParallel(swa_model)\n",
    "    \n",
    "    validate_and_save(model, model_file, val_loader, save=False)\n",
    "    \n",
    "    for cycle in range(1, args.num_cycles+1):\n",
    "        print('CYCLE:', cycle)\n",
    "        grid = GridMask(64, 128, rotate=15, ratio=0.6, mode=1, prob=1.)\n",
    "\n",
    "        for epoch in range(args.start_epoch, args.num_epochs):\n",
    "            grid.set_prob(epoch, args.st_epochs)\n",
    "            train_epoch(args, model, train_loader, epoch, optimizer, lr_scheduler, grid)\n",
    "            validate_and_save(model, model_file, val_loader, save=True)\n",
    "            \n",
    "            if (epoch+1) == args.swa_start and cycle == 1:\n",
    "                copy_model(swa_model, model)\n",
    "                swa_n = 0\n",
    "            if (epoch+1) >= args.swa_start and (epoch+1) % args.swa_freq == 0:\n",
    "                print('SWA>>>:')\n",
    "                moving_average(swa_model, model, 1.0 / (swa_n + 1))\n",
    "                swa_n += 1\n",
    "                bn_update(train_loader, swa_model)\n",
    "                validate_and_save(swa_model, swa_model_file, val_loader, save=True)\n",
    "\n",
    "        # reset scheduler at each cycle\n",
    "        lr_scheduler = WarmupCyclicalLR(\n",
    "            \"cos\", args.base_lr, args.num_epochs, iters_per_epoch=len(train_loader), warmup_epochs=args.warmup_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.backbone = 'se_resnext101_32x4d'\n",
    "args.ckp_name = 'model4_se_resnext101_fold3_224.pth'\n",
    "\n",
    "args.base_lr = 1e-4\n",
    "\n",
    "args.num_epochs = 60\n",
    "args.warmup_epochs = 5\n",
    "args.start_epoch = 0\n",
    "\n",
    "args.swa_start = 1000\n",
    "args.swa_freq = 4\n",
    "\n",
    "args.num_cycles = 100\n",
    "args.batch_size = 440\n",
    "args.val_batch_size = 1024\n",
    "args.st_epochs = 10\n",
    "\n",
    "args.beta = 1.0\n",
    "args.cutmix_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200840, 6)\n",
      "(200840, 32332)\n",
      "(160716, 6) (40124, 6)\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_train_val_loaders(batch_size=args.batch_size, val_batch_size=args.val_batch_size, ifold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init model4\n",
      "model file: ./model4-ckps/se_resnext101_32x4d/model4_se_resnext101_fold3_224.pth, exist: True\n",
      "loading ./model4-ckps/se_resnext101_32x4d/model4_se_resnext101_fold3_224.pth...\n",
      "init model4\n",
      "model file: ./model4-ckps/se_resnext101_32x4d/model4_se_resnext101_fold3_224.pth, exist: True\n",
      "loading ./model4-ckps/se_resnext101_32x4d/model4_se_resnext101_fold3_224.pth...\n",
      "\n",
      "val: {'recall': 0.99568, 'recall_grapheme': 0.993924, 'recall_vowel': 0.996616, 'recall_consonant': 0.998254, 'recall_word': 0.992653, 'acc_grapheme': 0.993146, 'acc_vowel': 0.99686, 'acc_consonant': 0.997582, 'acc_word': 0.992498, 'loss_grapheme': 0.027386, 'loss_vowel': 0.017612, 'loss_consonant': 0.013515, 'loss_word': 0.027576}\n",
      "CYCLE: 1\n",
      "    0 | 0.000020 | 160600/160716 | 8.4128 | 6.9039 ||\n",
      "val: {'recall': 0.99351, 'recall_grapheme': 0.990305, 'recall_vowel': 0.996005, 'recall_consonant': 0.997425, 'recall_word': 0.988899, 'acc_grapheme': 0.990255, 'acc_vowel': 0.996212, 'acc_consonant': 0.996287, 'acc_word': 0.988735, 'loss_grapheme': 0.050464, 'loss_vowel': 0.03887, 'loss_consonant': 0.03115, 'loss_word': 0.047321}\n",
      "    1 | 0.000040 | 160600/160716 | 18.5634 | 7.1712 |\n",
      "val: {'recall': 0.994793, 'recall_grapheme': 0.992449, 'recall_vowel': 0.996427, 'recall_consonant': 0.997846, 'recall_word': 0.990756, 'acc_grapheme': 0.991277, 'acc_vowel': 0.99671, 'acc_consonant': 0.99676, 'acc_word': 0.990604, 'loss_grapheme': 0.043481, 'loss_vowel': 0.041341, 'loss_consonant': 0.032901, 'loss_word': 0.038103}\n",
      "    2 | 0.000060 | 160600/160716 | 0.3005 | 7.0857 |||\n",
      "val: {'recall': 0.994352, 'recall_grapheme': 0.991543, 'recall_vowel': 0.996517, 'recall_consonant': 0.997805, 'recall_word': 0.990356, 'acc_grapheme': 0.991501, 'acc_vowel': 0.996635, 'acc_consonant': 0.99671, 'acc_word': 0.990255, 'loss_grapheme': 0.034082, 'loss_vowel': 0.020498, 'loss_consonant': 0.016199, 'loss_word': 0.035757}\n",
      "    3 | 0.000079 | 160600/160716 | 0.1237 | 6.6708 ||\n",
      "val: {'recall': 0.994455, 'recall_grapheme': 0.9916, 'recall_vowel': 0.996748, 'recall_consonant': 0.997874, 'recall_word': 0.990516, 'acc_grapheme': 0.991526, 'acc_vowel': 0.99691, 'acc_consonant': 0.99686, 'acc_word': 0.99038, 'loss_grapheme': 0.034001, 'loss_vowel': 0.017812, 'loss_consonant': 0.013932, 'loss_word': 0.036893}\n",
      "    4 | 0.000098 | 160600/160716 | 17.1989 | 6.9011 ||\n",
      "val: {'recall': 0.994136, 'recall_grapheme': 0.991725, 'recall_vowel': 0.995973, 'recall_consonant': 0.99712, 'recall_word': 0.990253, 'acc_grapheme': 0.991252, 'acc_vowel': 0.996262, 'acc_consonant': 0.99686, 'acc_word': 0.990106, 'loss_grapheme': 0.041814, 'loss_vowel': 0.035679, 'loss_consonant': 0.028316, 'loss_word': 0.038574}\n",
      "    5 | 0.000098 | 160600/160716 | 0.2174 | 6.2583 ||\n",
      "val: {'recall': 0.992963, 'recall_grapheme': 0.989912, 'recall_vowel': 0.996077, 'recall_consonant': 0.995952, 'recall_word': 0.988773, 'acc_grapheme': 0.989557, 'acc_vowel': 0.995987, 'acc_consonant': 0.996062, 'acc_word': 0.98866, 'loss_grapheme': 0.045564, 'loss_vowel': 0.0318, 'loss_consonant': 0.023765, 'loss_word': 0.045031}\n",
      "    6 | 0.000097 | 160600/160716 | 6.5118 | 6.8627 ||\n",
      "val: {'recall': 0.994284, 'recall_grapheme': 0.991468, 'recall_vowel': 0.996429, 'recall_consonant': 0.997771, 'recall_word': 0.990516, 'acc_grapheme': 0.991302, 'acc_vowel': 0.996561, 'acc_consonant': 0.99676, 'acc_word': 0.99028, 'loss_grapheme': 0.035398, 'loss_vowel': 0.021452, 'loss_consonant': 0.017654, 'loss_word': 0.037029}\n",
      "    7 | 0.000096 | 160600/160716 | 17.6806 | 7.0248 ||\n",
      "val: {'recall': 0.993458, 'recall_grapheme': 0.99018, 'recall_vowel': 0.996121, 'recall_consonant': 0.99735, 'recall_word': 0.989553, 'acc_grapheme': 0.990529, 'acc_vowel': 0.996411, 'acc_consonant': 0.996411, 'acc_word': 0.989508, 'loss_grapheme': 0.044525, 'loss_vowel': 0.036782, 'loss_consonant': 0.02923, 'loss_word': 0.041888}\n",
      "    8 | 0.000095 | 160600/160716 | 11.2660 | 6.9152 |\n",
      "val: {'recall': 0.993618, 'recall_grapheme': 0.990884, 'recall_vowel': 0.995523, 'recall_consonant': 0.997183, 'recall_word': 0.988133, 'acc_grapheme': 0.989408, 'acc_vowel': 0.995938, 'acc_consonant': 0.995987, 'acc_word': 0.987987, 'loss_grapheme': 0.060551, 'loss_vowel': 0.047237, 'loss_consonant': 0.037143, 'loss_word': 0.054414}\n",
      "    9 | 0.000093 | 160600/160716 | 3.2401 | 7.3139 |||\n",
      "val: {'recall': 0.994206, 'recall_grapheme': 0.991207, 'recall_vowel': 0.996466, 'recall_consonant': 0.997944, 'recall_word': 0.990011, 'acc_grapheme': 0.990779, 'acc_vowel': 0.996386, 'acc_consonant': 0.996959, 'acc_word': 0.989807, 'loss_grapheme': 0.036407, 'loss_vowel': 0.019765, 'loss_consonant': 0.015331, 'loss_word': 0.038361}\n",
      "   10 | 0.000092 | 160600/160716 | 13.0096 | 6.5915 ||\n",
      "val: {'recall': 0.993475, 'recall_grapheme': 0.990088, 'recall_vowel': 0.996022, 'recall_consonant': 0.997702, 'recall_word': 0.988838, 'acc_grapheme': 0.990056, 'acc_vowel': 0.996262, 'acc_consonant': 0.996287, 'acc_word': 0.988735, 'loss_grapheme': 0.044851, 'loss_vowel': 0.03516, 'loss_consonant': 0.02829, 'loss_word': 0.043048}\n",
      "   11 | 0.000090 | 160600/160716 | 7.8266 | 6.8572 ||\n",
      "val: {'recall': 0.994299, 'recall_grapheme': 0.991777, 'recall_vowel': 0.99614, 'recall_consonant': 0.997503, 'recall_word': 0.99098, 'acc_grapheme': 0.991452, 'acc_vowel': 0.996237, 'acc_consonant': 0.99676, 'acc_word': 0.990779, 'loss_grapheme': 0.037071, 'loss_vowel': 0.030224, 'loss_consonant': 0.022595, 'loss_word': 0.035848}\n",
      "   12 | 0.000089 | 160600/160716 | 1.3016 | 6.8101 |||\n",
      "val: {'recall': 0.993847, 'recall_grapheme': 0.990638, 'recall_vowel': 0.996444, 'recall_consonant': 0.997667, 'recall_word': 0.9894, 'acc_grapheme': 0.99043, 'acc_vowel': 0.996112, 'acc_consonant': 0.996511, 'acc_word': 0.989208, 'loss_grapheme': 0.039467, 'loss_vowel': 0.023589, 'loss_consonant': 0.019514, 'loss_word': 0.041018}\n",
      "   13 | 0.000087 | 160600/160716 | 8.5587 | 6.7982 ||\n",
      "val: {'recall': 0.993626, 'recall_grapheme': 0.990412, 'recall_vowel': 0.996088, 'recall_consonant': 0.99759, 'recall_word': 0.988074, 'acc_grapheme': 0.989807, 'acc_vowel': 0.996112, 'acc_consonant': 0.996187, 'acc_word': 0.987962, 'loss_grapheme': 0.051286, 'loss_vowel': 0.034238, 'loss_consonant': 0.027806, 'loss_word': 0.049454}\n",
      "   14 | 0.000085 | 160600/160716 | 0.2892 | 7.0050 |||\n",
      "val: {'recall': 0.994503, 'recall_grapheme': 0.991903, 'recall_vowel': 0.996682, 'recall_consonant': 0.997522, 'recall_word': 0.989878, 'acc_grapheme': 0.990529, 'acc_vowel': 0.996436, 'acc_consonant': 0.996735, 'acc_word': 0.989657, 'loss_grapheme': 0.038402, 'loss_vowel': 0.024403, 'loss_consonant': 0.01888, 'loss_word': 0.039106}\n",
      "   15 | 0.000083 | 160600/160716 | 4.7655 | 6.9629 |||\n",
      "val: {'recall': 0.993784, 'recall_grapheme': 0.990904, 'recall_vowel': 0.996032, 'recall_consonant': 0.997295, 'recall_word': 0.988604, 'acc_grapheme': 0.990255, 'acc_vowel': 0.996262, 'acc_consonant': 0.996237, 'acc_word': 0.988486, 'loss_grapheme': 0.041513, 'loss_vowel': 0.025886, 'loss_consonant': 0.0214, 'loss_word': 0.042732}\n",
      "   16 | 0.000081 | 160600/160716 | 20.4664 | 6.8610 ||\n",
      "val: {'recall': 0.994268, 'recall_grapheme': 0.991635, 'recall_vowel': 0.996146, 'recall_consonant': 0.997657, 'recall_word': 0.990173, 'acc_grapheme': 0.991152, 'acc_vowel': 0.996411, 'acc_consonant': 0.996586, 'acc_word': 0.990031, 'loss_grapheme': 0.039006, 'loss_vowel': 0.031189, 'loss_consonant': 0.024553, 'loss_word': 0.037719}\n",
      "   17 | 0.000079 | 160600/160716 | 18.6770 | 6.5279 ||\n",
      "val: {'recall': 0.994619, 'recall_grapheme': 0.992478, 'recall_vowel': 0.996387, 'recall_consonant': 0.997135, 'recall_word': 0.991444, 'acc_grapheme': 0.991601, 'acc_vowel': 0.996611, 'acc_consonant': 0.996935, 'acc_word': 0.991327, 'loss_grapheme': 0.033382, 'loss_vowel': 0.026484, 'loss_consonant': 0.020473, 'loss_word': 0.031822}\n",
      "   18 | 0.000077 | 160600/160716 | 17.0161 | 7.7312 |\n",
      "val: {'recall': 0.994234, 'recall_grapheme': 0.991857, 'recall_vowel': 0.996372, 'recall_consonant': 0.99685, 'recall_word': 0.990242, 'acc_grapheme': 0.991028, 'acc_vowel': 0.996486, 'acc_consonant': 0.996212, 'acc_word': 0.990106, 'loss_grapheme': 0.041856, 'loss_vowel': 0.03585, 'loss_consonant': 0.030371, 'loss_word': 0.037974}\n",
      "   19 | 0.000075 | 160600/160716 | 0.2203 | 6.7285 ||\n",
      "val: {'recall': 0.993474, 'recall_grapheme': 0.990276, 'recall_vowel': 0.996483, 'recall_consonant': 0.996863, 'recall_word': 0.989087, 'acc_grapheme': 0.990056, 'acc_vowel': 0.996411, 'acc_consonant': 0.996062, 'acc_word': 0.988909, 'loss_grapheme': 0.044689, 'loss_vowel': 0.031013, 'loss_consonant': 0.026437, 'loss_word': 0.042783}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20 | 0.000073 | 160600/160716 | 12.6315 | 7.3443 ||\n",
      "val: {'recall': 0.993016, 'recall_grapheme': 0.989521, 'recall_vowel': 0.996378, 'recall_consonant': 0.996646, 'recall_word': 0.98837, 'acc_grapheme': 0.989208, 'acc_vowel': 0.996361, 'acc_consonant': 0.995888, 'acc_word': 0.988187, 'loss_grapheme': 0.047492, 'loss_vowel': 0.033285, 'loss_consonant': 0.028356, 'loss_word': 0.046543}\n",
      "   21 | 0.000070 | 160600/160716 | 17.0000 | 6.9909 ||\n",
      "val: {'recall': 0.994627, 'recall_grapheme': 0.992146, 'recall_vowel': 0.996667, 'recall_consonant': 0.997549, 'recall_word': 0.991286, 'acc_grapheme': 0.991551, 'acc_vowel': 0.99676, 'acc_consonant': 0.996536, 'acc_word': 0.991152, 'loss_grapheme': 0.035714, 'loss_vowel': 0.028704, 'loss_consonant': 0.023212, 'loss_word': 0.034285}\n",
      "   22 | 0.000068 | 160600/160716 | 8.0090 | 7.4144 |||\n",
      "val: {'recall': 0.994571, 'recall_grapheme': 0.992037, 'recall_vowel': 0.996608, 'recall_consonant': 0.997601, 'recall_word': 0.990367, 'acc_grapheme': 0.991427, 'acc_vowel': 0.99681, 'acc_consonant': 0.996586, 'acc_word': 0.99028, 'loss_grapheme': 0.036369, 'loss_vowel': 0.02314, 'loss_consonant': 0.019746, 'loss_word': 0.036964}\n",
      "   23 | 0.000065 | 160600/160716 | 18.6258 | 6.7661 |\n",
      "val: {'recall': 0.993599, 'recall_grapheme': 0.990714, 'recall_vowel': 0.996009, 'recall_consonant': 0.996959, 'recall_word': 0.989205, 'acc_grapheme': 0.989981, 'acc_vowel': 0.996311, 'acc_consonant': 0.995963, 'acc_word': 0.988959, 'loss_grapheme': 0.057048, 'loss_vowel': 0.049442, 'loss_consonant': 0.038046, 'loss_word': 0.049262}\n",
      "   24 | 0.000063 | 160600/160716 | 6.4073 | 6.0020 ||\n",
      "val: {'recall': 0.994115, 'recall_grapheme': 0.991438, 'recall_vowel': 0.995908, 'recall_consonant': 0.997674, 'recall_word': 0.989544, 'acc_grapheme': 0.990779, 'acc_vowel': 0.996561, 'acc_consonant': 0.996511, 'acc_word': 0.989408, 'loss_grapheme': 0.040647, 'loss_vowel': 0.028894, 'loss_consonant': 0.023575, 'loss_word': 0.040079}\n",
      "   25 | 0.000060 | 160600/160716 | 3.8846 | 6.9740 ||\n",
      "val: {'recall': 0.99397, 'recall_grapheme': 0.991171, 'recall_vowel': 0.996325, 'recall_consonant': 0.997214, 'recall_word': 0.990167, 'acc_grapheme': 0.991227, 'acc_vowel': 0.996486, 'acc_consonant': 0.996386, 'acc_word': 0.989931, 'loss_grapheme': 0.036349, 'loss_vowel': 0.02271, 'loss_consonant': 0.018875, 'loss_word': 0.037507}\n",
      "   26 | 0.000058 | 160600/160716 | 0.2624 | 6.8175 ||\n",
      "val: {'recall': 0.994728, 'recall_grapheme': 0.99234, 'recall_vowel': 0.996331, 'recall_consonant': 0.9979, 'recall_word': 0.990907, 'acc_grapheme': 0.9919, 'acc_vowel': 0.996785, 'acc_consonant': 0.99666, 'acc_word': 0.990704, 'loss_grapheme': 0.034258, 'loss_vowel': 0.025651, 'loss_consonant': 0.021047, 'loss_word': 0.034738}\n",
      "   27 | 0.000055 | 160600/160716 | 8.2227 | 6.7982 ||\n",
      "val: {'recall': 0.994426, 'recall_grapheme': 0.9921, 'recall_vowel': 0.996353, 'recall_consonant': 0.997151, 'recall_word': 0.990401, 'acc_grapheme': 0.991452, 'acc_vowel': 0.996561, 'acc_consonant': 0.99671, 'acc_word': 0.99018, 'loss_grapheme': 0.036808, 'loss_vowel': 0.023631, 'loss_consonant': 0.018783, 'loss_word': 0.037485}\n",
      "   28 | 0.000053 | 160600/160716 | 9.3940 | 6.3708 |||\n",
      "val: {'recall': 0.994326, 'recall_grapheme': 0.991891, 'recall_vowel': 0.996244, 'recall_consonant': 0.997277, 'recall_word': 0.9902, 'acc_grapheme': 0.991202, 'acc_vowel': 0.996511, 'acc_consonant': 0.996411, 'acc_word': 0.989981, 'loss_grapheme': 0.040037, 'loss_vowel': 0.027974, 'loss_consonant': 0.022982, 'loss_word': 0.039005}\n",
      "   29 | 0.000050 | 160600/160716 | 0.1906 | 6.2987 ||\n",
      "val: {'recall': 0.995104, 'recall_grapheme': 0.992901, 'recall_vowel': 0.996636, 'recall_consonant': 0.997977, 'recall_word': 0.991718, 'acc_grapheme': 0.992399, 'acc_vowel': 0.996984, 'acc_consonant': 0.997234, 'acc_word': 0.991651, 'loss_grapheme': 0.029102, 'loss_vowel': 0.01548, 'loss_consonant': 0.012156, 'loss_word': 0.030994}\n",
      "   30 | 0.000047 | 160600/160716 | 0.2089 | 6.3716 |||\n",
      "val: {'recall': 0.995682, 'recall_grapheme': 0.993966, 'recall_vowel': 0.996726, 'recall_consonant': 0.998072, 'recall_word': 0.992634, 'acc_grapheme': 0.993146, 'acc_vowel': 0.997084, 'acc_consonant': 0.997358, 'acc_word': 0.992523, 'loss_grapheme': 0.026802, 'loss_vowel': 0.015368, 'loss_consonant': 0.011964, 'loss_word': 0.027188}\n",
      "###>>>>> saved ./model4-ckps/se_resnext101_32x4d/model4_se_resnext101_fold3_224.pth\n",
      "   31 | 0.000045 | 160600/160716 | 6.5123 | 6.6507 |||\n",
      "val: {'recall': 0.992196, 'recall_grapheme': 0.989326, 'recall_vowel': 0.995551, 'recall_consonant': 0.994582, 'recall_word': 0.986991, 'acc_grapheme': 0.988735, 'acc_vowel': 0.996212, 'acc_consonant': 0.995639, 'acc_word': 0.986816, 'loss_grapheme': 0.058742, 'loss_vowel': 0.038284, 'loss_consonant': 0.031135, 'loss_word': 0.057533}\n",
      "   32 | 0.000042 | 160600/160716 | 4.8396 | 6.8036 ||\n",
      "val: {'recall': 0.994233, 'recall_grapheme': 0.991595, 'recall_vowel': 0.996454, 'recall_consonant': 0.99729, 'recall_word': 0.989604, 'acc_grapheme': 0.990978, 'acc_vowel': 0.996611, 'acc_consonant': 0.996561, 'acc_word': 0.989458, 'loss_grapheme': 0.03708, 'loss_vowel': 0.02066, 'loss_consonant': 0.017305, 'loss_word': 0.039964}\n",
      "   33 | 0.000040 | 160600/160716 | 6.9215 | 6.7601 ||\n",
      "val: {'recall': 0.994478, 'recall_grapheme': 0.991817, 'recall_vowel': 0.996751, 'recall_consonant': 0.997524, 'recall_word': 0.990505, 'acc_grapheme': 0.991302, 'acc_vowel': 0.99676, 'acc_consonant': 0.99671, 'acc_word': 0.990405, 'loss_grapheme': 0.035717, 'loss_vowel': 0.026832, 'loss_consonant': 0.022403, 'loss_word': 0.034785}\n",
      "   34 | 0.000037 | 160600/160716 | 5.1803 | 6.7206 ||\n",
      "val: {'recall': 0.993943, 'recall_grapheme': 0.991094, 'recall_vowel': 0.99633, 'recall_consonant': 0.997254, 'recall_word': 0.989085, 'acc_grapheme': 0.990355, 'acc_vowel': 0.996511, 'acc_consonant': 0.996262, 'acc_word': 0.988984, 'loss_grapheme': 0.042891, 'loss_vowel': 0.029358, 'loss_consonant': 0.023552, 'loss_word': 0.043323}\n",
      "   35 | 0.000035 | 160600/160716 | 7.8167 | 6.3666 |||\n",
      "val: {'recall': 0.994244, 'recall_grapheme': 0.99148, 'recall_vowel': 0.996761, 'recall_consonant': 0.997254, 'recall_word': 0.989688, 'acc_grapheme': 0.991053, 'acc_vowel': 0.996635, 'acc_consonant': 0.99686, 'acc_word': 0.989557, 'loss_grapheme': 0.035938, 'loss_vowel': 0.018536, 'loss_consonant': 0.014642, 'loss_word': 0.038989}\n",
      "   36 | 0.000032 | 160600/160716 | 0.2182 | 6.7470 ||\n",
      "val: {'recall': 0.995058, 'recall_grapheme': 0.992796, 'recall_vowel': 0.996724, 'recall_consonant': 0.997916, 'recall_word': 0.99113, 'acc_grapheme': 0.992124, 'acc_vowel': 0.99691, 'acc_consonant': 0.996935, 'acc_word': 0.991003, 'loss_grapheme': 0.03189, 'loss_vowel': 0.018566, 'loss_consonant': 0.015307, 'loss_word': 0.033803}\n",
      "   37 | 0.000030 | 160600/160716 | 0.2385 | 6.2900 |||\n",
      "val: {'recall': 0.993927, 'recall_grapheme': 0.990929, 'recall_vowel': 0.996489, 'recall_consonant': 0.997362, 'recall_word': 0.98892, 'acc_grapheme': 0.99033, 'acc_vowel': 0.996561, 'acc_consonant': 0.996511, 'acc_word': 0.988735, 'loss_grapheme': 0.041079, 'loss_vowel': 0.026477, 'loss_consonant': 0.019462, 'loss_word': 0.04378}\n",
      "   38 | 0.000027 | 160600/160716 | 17.6726 | 6.0815 |\n",
      "val: {'recall': 0.993777, 'recall_grapheme': 0.990922, 'recall_vowel': 0.996119, 'recall_consonant': 0.997146, 'recall_word': 0.988838, 'acc_grapheme': 0.990305, 'acc_vowel': 0.996586, 'acc_consonant': 0.996237, 'acc_word': 0.988685, 'loss_grapheme': 0.048712, 'loss_vowel': 0.03932, 'loss_consonant': 0.03166, 'loss_word': 0.046208}\n",
      "   39 | 0.000025 | 160600/160716 | 7.4340 | 6.5007 ||\n",
      "val: {'recall': 0.994091, 'recall_grapheme': 0.991221, 'recall_vowel': 0.996471, 'recall_consonant': 0.997453, 'recall_word': 0.989774, 'acc_grapheme': 0.990754, 'acc_vowel': 0.996536, 'acc_consonant': 0.996461, 'acc_word': 0.989607, 'loss_grapheme': 0.044024, 'loss_vowel': 0.037195, 'loss_consonant': 0.029801, 'loss_word': 0.040783}\n",
      "   40 | 0.000023 | 160600/160716 | 5.8505 | 6.4252 |||\n",
      "val: {'recall': 0.994463, 'recall_grapheme': 0.991958, 'recall_vowel': 0.996242, 'recall_consonant': 0.997694, 'recall_word': 0.990577, 'acc_grapheme': 0.991726, 'acc_vowel': 0.996685, 'acc_consonant': 0.99676, 'acc_word': 0.99038, 'loss_grapheme': 0.035018, 'loss_vowel': 0.025626, 'loss_consonant': 0.020295, 'loss_word': 0.035793}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   41 | 0.000021 | 160600/160716 | 3.2032 | 5.6600 |||\n",
      "val: {'recall': 0.994841, 'recall_grapheme': 0.992356, 'recall_vowel': 0.99655, 'recall_consonant': 0.998104, 'recall_word': 0.991304, 'acc_grapheme': 0.992174, 'acc_vowel': 0.99691, 'acc_consonant': 0.997184, 'acc_word': 0.991078, 'loss_grapheme': 0.030183, 'loss_vowel': 0.017648, 'loss_consonant': 0.013031, 'loss_word': 0.032839}\n",
      "   42 | 0.000019 | 160600/160716 | 3.7560 | 7.0704 ||\n",
      "val: {'recall': 0.994494, 'recall_grapheme': 0.992099, 'recall_vowel': 0.996181, 'recall_consonant': 0.997596, 'recall_word': 0.990653, 'acc_grapheme': 0.991277, 'acc_vowel': 0.996411, 'acc_consonant': 0.996436, 'acc_word': 0.990405, 'loss_grapheme': 0.036638, 'loss_vowel': 0.029291, 'loss_consonant': 0.022599, 'loss_word': 0.036438}\n",
      "   43 | 0.000017 | 160600/160716 | 7.5272 | 6.1906 ||\n",
      "val: {'recall': 0.994669, 'recall_grapheme': 0.992279, 'recall_vowel': 0.996462, 'recall_consonant': 0.997658, 'recall_word': 0.99046, 'acc_grapheme': 0.991601, 'acc_vowel': 0.99666, 'acc_consonant': 0.996785, 'acc_word': 0.990305, 'loss_grapheme': 0.034709, 'loss_vowel': 0.02405, 'loss_consonant': 0.018991, 'loss_word': 0.035925}\n",
      "   44 | 0.000015 | 160600/160716 | 0.2670 | 6.0559 ||\n",
      "val: {'recall': 0.99396, 'recall_grapheme': 0.99114, 'recall_vowel': 0.995943, 'recall_consonant': 0.997616, 'recall_word': 0.989254, 'acc_grapheme': 0.990679, 'acc_vowel': 0.996187, 'acc_consonant': 0.996536, 'acc_word': 0.989009, 'loss_grapheme': 0.039199, 'loss_vowel': 0.026294, 'loss_consonant': 0.020139, 'loss_word': 0.041759}\n",
      "   45 | 0.000013 | 160600/160716 | 0.1829 | 6.4687 |||\n",
      "val: {'recall': 0.994464, 'recall_grapheme': 0.99187, 'recall_vowel': 0.996319, 'recall_consonant': 0.997797, 'recall_word': 0.990167, 'acc_grapheme': 0.991402, 'acc_vowel': 0.996561, 'acc_consonant': 0.99676, 'acc_word': 0.989956, 'loss_grapheme': 0.035368, 'loss_vowel': 0.020774, 'loss_consonant': 0.017052, 'loss_word': 0.037924}\n",
      "   46 | 0.000011 | 160600/160716 | 5.6415 | 6.0713 ||\n",
      "val: {'recall': 0.992875, 'recall_grapheme': 0.989631, 'recall_vowel': 0.995869, 'recall_consonant': 0.996371, 'recall_word': 0.986576, 'acc_grapheme': 0.988511, 'acc_vowel': 0.996087, 'acc_consonant': 0.995788, 'acc_word': 0.986392, 'loss_grapheme': 0.072556, 'loss_vowel': 0.052767, 'loss_consonant': 0.041221, 'loss_word': 0.064589}\n",
      "   47 | 0.000010 | 160600/160716 | 8.0633 | 6.2877 ||\n",
      "val: {'recall': 0.993911, 'recall_grapheme': 0.991239, 'recall_vowel': 0.995894, 'recall_consonant': 0.997271, 'recall_word': 0.989162, 'acc_grapheme': 0.99043, 'acc_vowel': 0.996311, 'acc_consonant': 0.996262, 'acc_word': 0.988934, 'loss_grapheme': 0.04721, 'loss_vowel': 0.03843, 'loss_consonant': 0.030972, 'loss_word': 0.044316}\n",
      "   48 | 0.000008 | 160600/160716 | 1.2396 | 6.2184 ||\n",
      "val: {'recall': 0.995157, 'recall_grapheme': 0.993147, 'recall_vowel': 0.996457, 'recall_consonant': 0.997878, 'recall_word': 0.991564, 'acc_grapheme': 0.992648, 'acc_vowel': 0.996959, 'acc_consonant': 0.997059, 'acc_word': 0.991352, 'loss_grapheme': 0.029901, 'loss_vowel': 0.017876, 'loss_consonant': 0.013788, 'loss_word': 0.032208}\n",
      "   49 | 0.000007 | 160600/160716 | 0.2589 | 6.6253 ||\n",
      "val: {'recall': 0.993852, 'recall_grapheme': 0.991074, 'recall_vowel': 0.996102, 'recall_consonant': 0.997158, 'recall_word': 0.989041, 'acc_grapheme': 0.99043, 'acc_vowel': 0.996386, 'acc_consonant': 0.996237, 'acc_word': 0.98886, 'loss_grapheme': 0.048297, 'loss_vowel': 0.041511, 'loss_consonant': 0.033018, 'loss_word': 0.044422}\n",
      "   50 | 0.000005 | 160600/160716 | 0.1918 | 6.2067 ||\n",
      "val: {'recall': 0.995026, 'recall_grapheme': 0.992878, 'recall_vowel': 0.996604, 'recall_consonant': 0.997743, 'recall_word': 0.991087, 'acc_grapheme': 0.992124, 'acc_vowel': 0.99671, 'acc_consonant': 0.996935, 'acc_word': 0.990903, 'loss_grapheme': 0.032535, 'loss_vowel': 0.022791, 'loss_consonant': 0.018343, 'loss_word': 0.033366}\n",
      "   51 | 0.000004 | 160600/160716 | 17.8214 | 6.1578 ||\n",
      "val: {'recall': 0.994952, 'recall_grapheme': 0.992919, 'recall_vowel': 0.996261, 'recall_consonant': 0.997709, 'recall_word': 0.991385, 'acc_grapheme': 0.992349, 'acc_vowel': 0.99686, 'acc_consonant': 0.99691, 'acc_word': 0.991152, 'loss_grapheme': 0.031056, 'loss_vowel': 0.019961, 'loss_consonant': 0.015507, 'loss_word': 0.033214}\n",
      "   52 | 0.000003 | 160600/160716 | 5.9719 | 6.4231 ||\n",
      "val: {'recall': 0.993728, 'recall_grapheme': 0.990633, 'recall_vowel': 0.996293, 'recall_consonant': 0.997355, 'recall_word': 0.988696, 'acc_grapheme': 0.990205, 'acc_vowel': 0.996461, 'acc_consonant': 0.996386, 'acc_word': 0.988511, 'loss_grapheme': 0.043395, 'loss_vowel': 0.031154, 'loss_consonant': 0.024593, 'loss_word': 0.044151}\n",
      "   53 | 0.000002 | 160600/160716 | 0.2530 | 6.7804 |||\n",
      "val: {'recall': 0.995851, 'recall_grapheme': 0.994137, 'recall_vowel': 0.996884, 'recall_consonant': 0.998247, 'recall_word': 0.993031, 'acc_grapheme': 0.993371, 'acc_vowel': 0.997209, 'acc_consonant': 0.997558, 'acc_word': 0.992847, 'loss_grapheme': 0.025273, 'loss_vowel': 0.019727, 'loss_consonant': 0.01492, 'loss_word': 0.025393}\n",
      "###>>>>> saved ./model4-ckps/se_resnext101_32x4d/model4_se_resnext101_fold3_224.pth\n",
      "   54 | 0.000002 | 160600/160716 | 8.5016 | 6.2574 ||\n",
      "val: {'recall': 0.994321, 'recall_grapheme': 0.991643, 'recall_vowel': 0.996308, 'recall_consonant': 0.997691, 'recall_word': 0.989703, 'acc_grapheme': 0.990928, 'acc_vowel': 0.996511, 'acc_consonant': 0.99666, 'acc_word': 0.989532, 'loss_grapheme': 0.036319, 'loss_vowel': 0.022308, 'loss_consonant': 0.018389, 'loss_word': 0.03968}\n",
      "   55 | 0.000001 | 160600/160716 | 0.1989 | 6.7100 |||\n",
      "val: {'recall': 0.994461, 'recall_grapheme': 0.991997, 'recall_vowel': 0.996425, 'recall_consonant': 0.997424, 'recall_word': 0.990164, 'acc_grapheme': 0.991003, 'acc_vowel': 0.996561, 'acc_consonant': 0.996461, 'acc_word': 0.989981, 'loss_grapheme': 0.038813, 'loss_vowel': 0.029514, 'loss_consonant': 0.024157, 'loss_word': 0.038301}\n",
      "   56 | 0.000001 | 160600/160716 | 16.7551 | 6.7216 ||\n",
      "val: {'recall': 0.995406, 'recall_grapheme': 0.993573, 'recall_vowel': 0.996509, 'recall_consonant': 0.99797, 'recall_word': 0.99239, 'acc_grapheme': 0.992822, 'acc_vowel': 0.996835, 'acc_consonant': 0.997234, 'acc_word': 0.992174, 'loss_grapheme': 0.028352, 'loss_vowel': 0.022925, 'loss_consonant': 0.017556, 'loss_word': 0.028287}\n",
      "   57 | 0.000000 | 160600/160716 | 0.2269 | 6.3152 ||\n",
      "val: {'recall': 0.99406, 'recall_grapheme': 0.991136, 'recall_vowel': 0.996436, 'recall_consonant': 0.997531, 'recall_word': 0.989419, 'acc_grapheme': 0.990554, 'acc_vowel': 0.996536, 'acc_consonant': 0.996586, 'acc_word': 0.989208, 'loss_grapheme': 0.039283, 'loss_vowel': 0.023294, 'loss_consonant': 0.018772, 'loss_word': 0.042166}\n",
      "   58 | 0.000000 | 160600/160716 | 0.2192 | 6.6810 |||\n",
      "val: {'recall': 0.995089, 'recall_grapheme': 0.993093, 'recall_vowel': 0.996345, 'recall_consonant': 0.997825, 'recall_word': 0.99186, 'acc_grapheme': 0.992623, 'acc_vowel': 0.99686, 'acc_consonant': 0.997184, 'acc_word': 0.991676, 'loss_grapheme': 0.029067, 'loss_vowel': 0.017051, 'loss_consonant': 0.013285, 'loss_word': 0.03121}\n",
      "   59 | 0.000000 | 160600/160716 | 16.5665 | 6.3929 ||\n",
      "val: {'recall': 0.994691, 'recall_grapheme': 0.992302, 'recall_vowel': 0.996496, 'recall_consonant': 0.997662, 'recall_word': 0.990302, 'acc_grapheme': 0.991501, 'acc_vowel': 0.996586, 'acc_consonant': 0.996685, 'acc_word': 0.99018, 'loss_grapheme': 0.03618, 'loss_vowel': 0.025052, 'loss_consonant': 0.0214, 'loss_word': 0.036819}\n",
      "CYCLE: 2\n",
      "    0 | 0.000020 | 160600/160716 | 1.4181 | 6.3544 ||\n",
      "val: {'recall': 0.993853, 'recall_grapheme': 0.99096, 'recall_vowel': 0.996166, 'recall_consonant': 0.997326, 'recall_word': 0.988707, 'acc_grapheme': 0.990579, 'acc_vowel': 0.996486, 'acc_consonant': 0.996386, 'acc_word': 0.988585, 'loss_grapheme': 0.046903, 'loss_vowel': 0.034018, 'loss_consonant': 0.026651, 'loss_word': 0.046565}\n",
      "    1 | 0.000040 | 160600/160716 | 10.8782 | 6.3749 ||\n",
      "val: {'recall': 0.994734, 'recall_grapheme': 0.992627, 'recall_vowel': 0.996343, 'recall_consonant': 0.997339, 'recall_word': 0.991503, 'acc_grapheme': 0.992199, 'acc_vowel': 0.99681, 'acc_consonant': 0.997059, 'acc_word': 0.991377, 'loss_grapheme': 0.030014, 'loss_vowel': 0.01963, 'loss_consonant': 0.015124, 'loss_word': 0.031773}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2 | 0.000060 | 160600/160716 | 16.9800 | 6.1457 |\n",
      "val: {'recall': 0.993546, 'recall_grapheme': 0.990673, 'recall_vowel': 0.995647, 'recall_consonant': 0.997188, 'recall_word': 0.989191, 'acc_grapheme': 0.990131, 'acc_vowel': 0.996162, 'acc_consonant': 0.996037, 'acc_word': 0.988984, 'loss_grapheme': 0.050674, 'loss_vowel': 0.043321, 'loss_consonant': 0.033717, 'loss_word': 0.046422}\n",
      "    3 | 0.000079 | 160600/160716 | 0.1082 | 5.7816 ||\n",
      "val: {'recall': 0.994508, 'recall_grapheme': 0.99202, 'recall_vowel': 0.996274, 'recall_consonant': 0.997719, 'recall_word': 0.990077, 'acc_grapheme': 0.991427, 'acc_vowel': 0.99666, 'acc_consonant': 0.99681, 'acc_word': 0.989881, 'loss_grapheme': 0.034263, 'loss_vowel': 0.016656, 'loss_consonant': 0.01309, 'loss_word': 0.038544}\n",
      "    4 | 0.000098 | 160600/160716 | 7.7661 | 6.3727 ||\n",
      "val: {'recall': 0.993118, 'recall_grapheme': 0.990082, 'recall_vowel': 0.996214, 'recall_consonant': 0.996093, 'recall_word': 0.98825, 'acc_grapheme': 0.989433, 'acc_vowel': 0.996436, 'acc_consonant': 0.996037, 'acc_word': 0.988087, 'loss_grapheme': 0.046725, 'loss_vowel': 0.031355, 'loss_consonant': 0.025006, 'loss_word': 0.047733}\n",
      "    5 | 0.000098 | 160600/160716 | 10.4481 | 6.9085 |\n",
      "val: {'recall': 0.994257, 'recall_grapheme': 0.991779, 'recall_vowel': 0.996188, 'recall_consonant': 0.997281, 'recall_word': 0.989716, 'acc_grapheme': 0.990704, 'acc_vowel': 0.996386, 'acc_consonant': 0.996237, 'acc_word': 0.989582, 'loss_grapheme': 0.052452, 'loss_vowel': 0.049335, 'loss_consonant': 0.037202, 'loss_word': 0.046612}\n",
      "    6 | 0.000097 | 160600/160716 | 15.1729 | 6.4517 |\n",
      "val: {'recall': 0.993516, 'recall_grapheme': 0.990564, 'recall_vowel': 0.996159, 'recall_consonant': 0.996779, 'recall_word': 0.989057, 'acc_grapheme': 0.990031, 'acc_vowel': 0.996461, 'acc_consonant': 0.996187, 'acc_word': 0.98881, 'loss_grapheme': 0.046926, 'loss_vowel': 0.036447, 'loss_consonant': 0.030145, 'loss_word': 0.044352}\n",
      "    7 | 0.000096 | 160600/160716 | 0.2610 | 6.7500 |||\n",
      "val: {'recall': 0.994139, 'recall_grapheme': 0.991399, 'recall_vowel': 0.996118, 'recall_consonant': 0.99764, 'recall_word': 0.990461, 'acc_grapheme': 0.991252, 'acc_vowel': 0.996411, 'acc_consonant': 0.996785, 'acc_word': 0.990305, 'loss_grapheme': 0.036751, 'loss_vowel': 0.0275, 'loss_consonant': 0.021972, 'loss_word': 0.03647}\n",
      "    8 | 0.000095 | 160600/160716 | 0.1911 | 6.4128 |||\n",
      "val: {'recall': 0.99442, 'recall_grapheme': 0.991769, 'recall_vowel': 0.996459, 'recall_consonant': 0.997682, 'recall_word': 0.990245, 'acc_grapheme': 0.991227, 'acc_vowel': 0.99676, 'acc_consonant': 0.99691, 'acc_word': 0.990056, 'loss_grapheme': 0.034273, 'loss_vowel': 0.019366, 'loss_consonant': 0.014973, 'loss_word': 0.036409}\n",
      "    9 | 0.000093 | 160600/160716 | 5.5470 | 7.0508 ||\n",
      "val: {'recall': 0.995193, 'recall_grapheme': 0.993038, 'recall_vowel': 0.996951, 'recall_consonant': 0.997748, 'recall_word': 0.99196, 'acc_grapheme': 0.992623, 'acc_vowel': 0.996885, 'acc_consonant': 0.997134, 'acc_word': 0.9918, 'loss_grapheme': 0.02872, 'loss_vowel': 0.016546, 'loss_consonant': 0.012685, 'loss_word': 0.03042}\n",
      "   10 | 0.000092 | 160600/160716 | 18.2765 | 7.4803 |\n",
      "val: {'recall': 0.994046, 'recall_grapheme': 0.991817, 'recall_vowel': 0.995738, 'recall_consonant': 0.996813, 'recall_word': 0.990399, 'acc_grapheme': 0.991003, 'acc_vowel': 0.996386, 'acc_consonant': 0.996262, 'acc_word': 0.99018, 'loss_grapheme': 0.043042, 'loss_vowel': 0.038946, 'loss_consonant': 0.0301, 'loss_word': 0.03987}\n",
      "   11 | 0.000090 | 160600/160716 | 0.2641 | 6.5620 ||\n",
      "val: {'recall': 0.994456, 'recall_grapheme': 0.991997, 'recall_vowel': 0.996306, 'recall_consonant': 0.997525, 'recall_word': 0.990791, 'acc_grapheme': 0.991377, 'acc_vowel': 0.99671, 'acc_consonant': 0.99671, 'acc_word': 0.990654, 'loss_grapheme': 0.033762, 'loss_vowel': 0.018846, 'loss_consonant': 0.01536, 'loss_word': 0.035497}\n",
      "   12 | 0.000089 | 160600/160716 | 0.2915 | 6.7683 |||\n",
      "val: {'recall': 0.994569, 'recall_grapheme': 0.992169, 'recall_vowel': 0.996384, 'recall_consonant': 0.997554, 'recall_word': 0.991135, 'acc_grapheme': 0.991751, 'acc_vowel': 0.996586, 'acc_consonant': 0.996586, 'acc_word': 0.990903, 'loss_grapheme': 0.034408, 'loss_vowel': 0.024153, 'loss_consonant': 0.018997, 'loss_word': 0.034932}\n",
      "   13 | 0.000087 | 160600/160716 | 17.0303 | 7.0195 ||\n",
      "val: {'recall': 0.994728, 'recall_grapheme': 0.992377, 'recall_vowel': 0.996466, 'recall_consonant': 0.997692, 'recall_word': 0.991103, 'acc_grapheme': 0.991875, 'acc_vowel': 0.996735, 'acc_consonant': 0.996635, 'acc_word': 0.990953, 'loss_grapheme': 0.033343, 'loss_vowel': 0.026286, 'loss_consonant': 0.021379, 'loss_word': 0.032806}\n",
      "   14 | 0.000085 | 160600/160716 | 14.2794 | 6.4677 ||\n",
      "val: {'recall': 0.992567, 'recall_grapheme': 0.989273, 'recall_vowel': 0.995837, 'recall_consonant': 0.995887, 'recall_word': 0.987212, 'acc_grapheme': 0.98866, 'acc_vowel': 0.996037, 'acc_consonant': 0.995987, 'acc_word': 0.986916, 'loss_grapheme': 0.054371, 'loss_vowel': 0.036011, 'loss_consonant': 0.029989, 'loss_word': 0.054719}\n",
      "   15 | 0.000083 | 160600/160716 | 0.3219 | 6.4058 |||\n",
      "val: {'recall': 0.994214, 'recall_grapheme': 0.992191, 'recall_vowel': 0.996335, 'recall_consonant': 0.996139, 'recall_word': 0.990843, 'acc_grapheme': 0.991751, 'acc_vowel': 0.996511, 'acc_consonant': 0.996586, 'acc_word': 0.990579, 'loss_grapheme': 0.034846, 'loss_vowel': 0.022708, 'loss_consonant': 0.017379, 'loss_word': 0.036906}\n",
      "   16 | 0.000081 | 160600/160716 | 0.2405 | 5.9640 ||\n",
      "val: {'recall': 0.993881, 'recall_grapheme': 0.991453, 'recall_vowel': 0.995845, 'recall_consonant': 0.996773, 'recall_word': 0.989355, 'acc_grapheme': 0.990455, 'acc_vowel': 0.996586, 'acc_consonant': 0.996087, 'acc_word': 0.989109, 'loss_grapheme': 0.036957, 'loss_vowel': 0.021367, 'loss_consonant': 0.01867, 'loss_word': 0.039498}\n",
      "   17 | 0.000079 | 160600/160716 | 0.3857 | 6.8793 ||\n",
      "val: {'recall': 0.994133, 'recall_grapheme': 0.991657, 'recall_vowel': 0.996386, 'recall_consonant': 0.996833, 'recall_word': 0.989727, 'acc_grapheme': 0.991177, 'acc_vowel': 0.996611, 'acc_consonant': 0.996262, 'acc_word': 0.989483, 'loss_grapheme': 0.037117, 'loss_vowel': 0.022548, 'loss_consonant': 0.017963, 'loss_word': 0.04021}\n",
      "   18 | 0.000077 | 160600/160716 | 16.2773 | 6.6025 |\n",
      "val: {'recall': 0.994576, 'recall_grapheme': 0.992399, 'recall_vowel': 0.99607, 'recall_consonant': 0.997437, 'recall_word': 0.990805, 'acc_grapheme': 0.991476, 'acc_vowel': 0.99681, 'acc_consonant': 0.996536, 'acc_word': 0.990579, 'loss_grapheme': 0.035925, 'loss_vowel': 0.02411, 'loss_consonant': 0.020851, 'loss_word': 0.035509}\n",
      "   19 | 0.000075 | 160600/160716 | 6.4040 | 6.2806 |||\n",
      "val: {'recall': 0.993024, 'recall_grapheme': 0.990198, 'recall_vowel': 0.995755, 'recall_consonant': 0.995945, 'recall_word': 0.988104, 'acc_grapheme': 0.989532, 'acc_vowel': 0.996311, 'acc_consonant': 0.995938, 'acc_word': 0.987962, 'loss_grapheme': 0.05158, 'loss_vowel': 0.037476, 'loss_consonant': 0.031621, 'loss_word': 0.049668}\n",
      "   20 | 0.000073 | 160600/160716 | 8.6111 | 6.2675 |||\n",
      "val: {'recall': 0.992836, 'recall_grapheme': 0.989581, 'recall_vowel': 0.996267, 'recall_consonant': 0.995916, 'recall_word': 0.987982, 'acc_grapheme': 0.989084, 'acc_vowel': 0.996287, 'acc_consonant': 0.996037, 'acc_word': 0.987763, 'loss_grapheme': 0.046259, 'loss_vowel': 0.029064, 'loss_consonant': 0.023169, 'loss_word': 0.049616}\n",
      "   21 | 0.000070 | 160600/160716 | 15.9547 | 7.1612 |\n",
      "val: {'recall': 0.993471, 'recall_grapheme': 0.9905, 'recall_vowel': 0.99618, 'recall_consonant': 0.996702, 'recall_word': 0.988848, 'acc_grapheme': 0.990205, 'acc_vowel': 0.996336, 'acc_consonant': 0.995888, 'acc_word': 0.98861, 'loss_grapheme': 0.056713, 'loss_vowel': 0.051201, 'loss_consonant': 0.042265, 'loss_word': 0.048677}\n",
      "   22 | 0.000068 | 160600/160716 | 12.7102 | 6.5408 ||\n",
      "val: {'recall': 0.994231, 'recall_grapheme': 0.991825, 'recall_vowel': 0.996332, 'recall_consonant': 0.996943, 'recall_word': 0.990595, 'acc_grapheme': 0.991327, 'acc_vowel': 0.996561, 'acc_consonant': 0.996461, 'acc_word': 0.990355, 'loss_grapheme': 0.037826, 'loss_vowel': 0.029779, 'loss_consonant': 0.024177, 'loss_word': 0.036805}\n",
      "   23 | 0.000065 | 160600/160716 | 8.3714 | 6.4507 |||\n",
      "val: {'recall': 0.994762, 'recall_grapheme': 0.99248, 'recall_vowel': 0.996377, 'recall_consonant': 0.997712, 'recall_word': 0.990757, 'acc_grapheme': 0.991925, 'acc_vowel': 0.996785, 'acc_consonant': 0.99666, 'acc_word': 0.990604, 'loss_grapheme': 0.033187, 'loss_vowel': 0.021031, 'loss_consonant': 0.017843, 'loss_word': 0.035585}\n",
      "   24 | 0.000063 | 160600/160716 | 9.9337 | 6.6830 ||\n",
      "val: {'recall': 0.995155, 'recall_grapheme': 0.993263, 'recall_vowel': 0.996392, 'recall_consonant': 0.9977, 'recall_word': 0.990926, 'acc_grapheme': 0.992174, 'acc_vowel': 0.996685, 'acc_consonant': 0.99671, 'acc_word': 0.990679, 'loss_grapheme': 0.034389, 'loss_vowel': 0.027241, 'loss_consonant': 0.021309, 'loss_word': 0.035105}\n",
      "   25 | 0.000060 | 160600/160716 | 0.2707 | 6.8616 ||\n",
      "val: {'recall': 0.994735, 'recall_grapheme': 0.992566, 'recall_vowel': 0.996131, 'recall_consonant': 0.997677, 'recall_word': 0.990884, 'acc_grapheme': 0.991676, 'acc_vowel': 0.996511, 'acc_consonant': 0.996685, 'acc_word': 0.990654, 'loss_grapheme': 0.035888, 'loss_vowel': 0.028028, 'loss_consonant': 0.023087, 'loss_word': 0.035646}\n",
      "   26 | 0.000058 | 160600/160716 | 18.2354 | 6.3437 ||\n",
      "val: {'recall': 0.995487, 'recall_grapheme': 0.993861, 'recall_vowel': 0.996505, 'recall_consonant': 0.997724, 'recall_word': 0.992128, 'acc_grapheme': 0.992723, 'acc_vowel': 0.996885, 'acc_consonant': 0.996685, 'acc_word': 0.9919, 'loss_grapheme': 0.030646, 'loss_vowel': 0.023594, 'loss_consonant': 0.01904, 'loss_word': 0.030786}\n",
      "   27 | 0.000055 | 160600/160716 | 9.0643 | 6.4582 ||\n",
      "val: {'recall': 0.994081, 'recall_grapheme': 0.991381, 'recall_vowel': 0.996199, 'recall_consonant': 0.997363, 'recall_word': 0.989503, 'acc_grapheme': 0.990953, 'acc_vowel': 0.996586, 'acc_consonant': 0.996287, 'acc_word': 0.989358, 'loss_grapheme': 0.037909, 'loss_vowel': 0.025382, 'loss_consonant': 0.020779, 'loss_word': 0.040896}\n",
      "   28 | 0.000053 | 160600/160716 | 13.7071 | 6.9289 ||\n",
      "val: {'recall': 0.994753, 'recall_grapheme': 0.992304, 'recall_vowel': 0.99672, 'recall_consonant': 0.997682, 'recall_word': 0.990459, 'acc_grapheme': 0.991452, 'acc_vowel': 0.99671, 'acc_consonant': 0.996536, 'acc_word': 0.99023, 'loss_grapheme': 0.037302, 'loss_vowel': 0.027105, 'loss_consonant': 0.023307, 'loss_word': 0.036591}\n",
      "   29 | 0.000050 | 160600/160716 | 6.9224 | 6.4990 ||\n",
      "val: {'recall': 0.993305, 'recall_grapheme': 0.990564, 'recall_vowel': 0.995937, 'recall_consonant': 0.996158, 'recall_word': 0.988377, 'acc_grapheme': 0.990056, 'acc_vowel': 0.996237, 'acc_consonant': 0.996237, 'acc_word': 0.988212, 'loss_grapheme': 0.052327, 'loss_vowel': 0.036381, 'loss_consonant': 0.031162, 'loss_word': 0.049445}\n",
      "   30 | 0.000047 | 160600/160716 | 0.2419 | 5.9982 ||\n",
      "val: {'recall': 0.995225, 'recall_grapheme': 0.993137, 'recall_vowel': 0.996634, 'recall_consonant': 0.997991, 'recall_word': 0.991517, 'acc_grapheme': 0.992573, 'acc_vowel': 0.99691, 'acc_consonant': 0.996959, 'acc_word': 0.991327, 'loss_grapheme': 0.030613, 'loss_vowel': 0.020518, 'loss_consonant': 0.017535, 'loss_word': 0.031963}\n",
      "   31 | 0.000045 | 160600/160716 | 9.3630 | 6.3403 |||\n",
      "val: {'recall': 0.994045, 'recall_grapheme': 0.991609, 'recall_vowel': 0.996148, 'recall_consonant': 0.996814, 'recall_word': 0.98979, 'acc_grapheme': 0.990903, 'acc_vowel': 0.996586, 'acc_consonant': 0.996237, 'acc_word': 0.989582, 'loss_grapheme': 0.043261, 'loss_vowel': 0.036123, 'loss_consonant': 0.02897, 'loss_word': 0.041104}\n",
      "   32 | 0.000042 | 160600/160716 | 5.6051 | 6.3691 ||\n",
      "val: {'recall': 0.994688, 'recall_grapheme': 0.992403, 'recall_vowel': 0.996318, 'recall_consonant': 0.997628, 'recall_word': 0.990196, 'acc_grapheme': 0.991402, 'acc_vowel': 0.99671, 'acc_consonant': 0.996635, 'acc_word': 0.990056, 'loss_grapheme': 0.037687, 'loss_vowel': 0.027815, 'loss_consonant': 0.022062, 'loss_word': 0.037994}\n",
      "   33 | 0.000040 | 160600/160716 | 0.2492 | 5.7392 |||\n",
      "val: {'recall': 0.995196, 'recall_grapheme': 0.993256, 'recall_vowel': 0.996468, 'recall_consonant': 0.997805, 'recall_word': 0.990995, 'acc_grapheme': 0.992149, 'acc_vowel': 0.996885, 'acc_consonant': 0.996935, 'acc_word': 0.990804, 'loss_grapheme': 0.033247, 'loss_vowel': 0.022151, 'loss_consonant': 0.017706, 'loss_word': 0.035477}\n",
      "   34 | 0.000037 | 160600/160716 | 2.7455 | 6.4913 ||\n",
      "val: {'recall': 0.995563, 'recall_grapheme': 0.993819, 'recall_vowel': 0.996498, 'recall_consonant': 0.998117, 'recall_word': 0.992324, 'acc_grapheme': 0.992972, 'acc_vowel': 0.996935, 'acc_consonant': 0.997134, 'acc_word': 0.992174, 'loss_grapheme': 0.027142, 'loss_vowel': 0.017579, 'loss_consonant': 0.013496, 'loss_word': 0.028341}\n",
      "   35 | 0.000035 | 160600/160716 | 16.5843 | 6.3495 ||\n",
      "val: {'recall': 0.995325, 'recall_grapheme': 0.993455, 'recall_vowel': 0.996612, 'recall_consonant': 0.997778, 'recall_word': 0.99176, 'acc_grapheme': 0.992498, 'acc_vowel': 0.996984, 'acc_consonant': 0.99676, 'acc_word': 0.991626, 'loss_grapheme': 0.031825, 'loss_vowel': 0.027512, 'loss_consonant': 0.022814, 'loss_word': 0.031047}\n",
      "   36 | 0.000032 | 160600/160716 | 8.1615 | 6.5386 ||\n",
      "val: {'recall': 0.992918, 'recall_grapheme': 0.989446, 'recall_vowel': 0.995664, 'recall_consonant': 0.997116, 'recall_word': 0.987586, 'acc_grapheme': 0.989184, 'acc_vowel': 0.996112, 'acc_consonant': 0.995888, 'acc_word': 0.987389, 'loss_grapheme': 0.052577, 'loss_vowel': 0.039481, 'loss_consonant': 0.031163, 'loss_word': 0.050976}\n",
      "   37 | 0.000030 | 160600/160716 | 6.7740 | 6.0314 ||\n",
      "val: {'recall': 0.994223, 'recall_grapheme': 0.991736, 'recall_vowel': 0.996108, 'recall_consonant': 0.997313, 'recall_word': 0.98927, 'acc_grapheme': 0.990579, 'acc_vowel': 0.996336, 'acc_consonant': 0.996237, 'acc_word': 0.989059, 'loss_grapheme': 0.043618, 'loss_vowel': 0.03355, 'loss_consonant': 0.025701, 'loss_word': 0.043224}\n",
      "   38 | 0.000027 | 160600/160716 | 5.7769 | 6.2841 |||\n",
      "val: {'recall': 0.995577, 'recall_grapheme': 0.993752, 'recall_vowel': 0.996807, 'recall_consonant': 0.997998, 'recall_word': 0.992238, 'acc_grapheme': 0.993096, 'acc_vowel': 0.996959, 'acc_consonant': 0.997234, 'acc_word': 0.992099, 'loss_grapheme': 0.02794, 'loss_vowel': 0.01858, 'loss_consonant': 0.014148, 'loss_word': 0.030056}\n",
      "   39 | 0.000026 | 113960/160716 | 6.3742 | 7.0734 |||"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   41 | 0.000021 | 160600/160716 | 0.1711 | 6.4507 ||\n",
      "val: {'recall': 0.995452, 'recall_grapheme': 0.993582, 'recall_vowel': 0.996698, 'recall_consonant': 0.997947, 'recall_word': 0.991536, 'acc_grapheme': 0.992648, 'acc_vowel': 0.996885, 'acc_consonant': 0.997109, 'acc_word': 0.991327, 'loss_grapheme': 0.030605, 'loss_vowel': 0.020579, 'loss_consonant': 0.016235, 'loss_word': 0.032595}\n",
      "   42 | 0.000019 | 160600/160716 | 13.7321 | 6.5739 |\n",
      "val: {'recall': 0.994493, 'recall_grapheme': 0.991957, 'recall_vowel': 0.996472, 'recall_consonant': 0.997586, 'recall_word': 0.989771, 'acc_grapheme': 0.991177, 'acc_vowel': 0.996685, 'acc_consonant': 0.996486, 'acc_word': 0.989557, 'loss_grapheme': 0.040118, 'loss_vowel': 0.028852, 'loss_consonant': 0.024275, 'loss_word': 0.040446}\n",
      "   43 | 0.000017 | 160600/160716 | 0.1625 | 6.3324 ||\n",
      "val: {'recall': 0.995106, 'recall_grapheme': 0.993205, 'recall_vowel': 0.996701, 'recall_consonant': 0.997314, 'recall_word': 0.991316, 'acc_grapheme': 0.992099, 'acc_vowel': 0.996885, 'acc_consonant': 0.99681, 'acc_word': 0.991103, 'loss_grapheme': 0.03146, 'loss_vowel': 0.021539, 'loss_consonant': 0.018109, 'loss_word': 0.03319}\n",
      "   44 | 0.000015 | 160600/160716 | 8.6813 | 6.1437 ||\n",
      "val: {'recall': 0.994889, 'recall_grapheme': 0.992531, 'recall_vowel': 0.996772, 'recall_consonant': 0.997724, 'recall_word': 0.990864, 'acc_grapheme': 0.991875, 'acc_vowel': 0.996835, 'acc_consonant': 0.99676, 'acc_word': 0.990654, 'loss_grapheme': 0.033569, 'loss_vowel': 0.023577, 'loss_consonant': 0.018717, 'loss_word': 0.035526}\n",
      "   45 | 0.000013 | 160600/160716 | 0.2136 | 6.7786 ||\n",
      "val: {'recall': 0.9947, 'recall_grapheme': 0.992359, 'recall_vowel': 0.99645, 'recall_consonant': 0.997631, 'recall_word': 0.990424, 'acc_grapheme': 0.991551, 'acc_vowel': 0.996785, 'acc_consonant': 0.996586, 'acc_word': 0.990205, 'loss_grapheme': 0.03674, 'loss_vowel': 0.027226, 'loss_consonant': 0.022561, 'loss_word': 0.03709}\n",
      "   46 | 0.000011 | 160600/160716 | 0.2970 | 6.3673 ||\n",
      "val: {'recall': 0.995246, 'recall_grapheme': 0.993478, 'recall_vowel': 0.996587, 'recall_consonant': 0.997442, 'recall_word': 0.991675, 'acc_grapheme': 0.992648, 'acc_vowel': 0.997009, 'acc_consonant': 0.997134, 'acc_word': 0.991452, 'loss_grapheme': 0.02961, 'loss_vowel': 0.017955, 'loss_consonant': 0.014085, 'loss_word': 0.03229}\n",
      "   47 | 0.000010 | 160600/160716 | 8.0915 | 6.5106 |||\n",
      "val: {'recall': 0.994606, 'recall_grapheme': 0.992106, 'recall_vowel': 0.996581, 'recall_consonant': 0.997632, 'recall_word': 0.99025, 'acc_grapheme': 0.991202, 'acc_vowel': 0.99666, 'acc_consonant': 0.996486, 'acc_word': 0.990006, 'loss_grapheme': 0.037631, 'loss_vowel': 0.025974, 'loss_consonant': 0.02177, 'loss_word': 0.03904}\n",
      "   48 | 0.000008 | 160600/160716 | 9.0673 | 6.3366 |||\n",
      "val: {'recall': 0.993941, 'recall_grapheme': 0.990985, 'recall_vowel': 0.996378, 'recall_consonant': 0.997415, 'recall_word': 0.989144, 'acc_grapheme': 0.990255, 'acc_vowel': 0.996511, 'acc_consonant': 0.996237, 'acc_word': 0.988959, 'loss_grapheme': 0.047414, 'loss_vowel': 0.037483, 'loss_consonant': 0.030567, 'loss_word': 0.045315}\n",
      "   49 | 0.000007 | 160600/160716 | 4.3795 | 6.5251 |||\n",
      "val: {'recall': 0.99408, 'recall_grapheme': 0.991337, 'recall_vowel': 0.996196, 'recall_consonant': 0.99745, 'recall_word': 0.989485, 'acc_grapheme': 0.990654, 'acc_vowel': 0.996586, 'acc_consonant': 0.996262, 'acc_word': 0.989233, 'loss_grapheme': 0.042464, 'loss_vowel': 0.030367, 'loss_consonant': 0.024907, 'loss_word': 0.04272}\n",
      "   50 | 0.000005 | 160600/160716 | 8.1194 | 6.0456 |||\n",
      "val: {'recall': 0.995008, 'recall_grapheme': 0.992805, 'recall_vowel': 0.996692, 'recall_consonant': 0.997728, 'recall_word': 0.990863, 'acc_grapheme': 0.992, 'acc_vowel': 0.99681, 'acc_consonant': 0.996785, 'acc_word': 0.990629, 'loss_grapheme': 0.03344, 'loss_vowel': 0.023145, 'loss_consonant': 0.019271, 'loss_word': 0.034612}\n",
      "   51 | 0.000004 | 160600/160716 | 0.2878 | 6.4701 |||\n",
      "val: {'recall': 0.995332, 'recall_grapheme': 0.993431, 'recall_vowel': 0.996663, 'recall_consonant': 0.997802, 'recall_word': 0.991879, 'acc_grapheme': 0.992623, 'acc_vowel': 0.996885, 'acc_consonant': 0.996984, 'acc_word': 0.991626, 'loss_grapheme': 0.029993, 'loss_vowel': 0.018882, 'loss_consonant': 0.015433, 'loss_word': 0.031931}\n",
      "   52 | 0.000003 | 160600/160716 | 8.3214 | 6.9223 |||\n",
      "val: {'recall': 0.994178, 'recall_grapheme': 0.991529, 'recall_vowel': 0.996237, 'recall_consonant': 0.997418, 'recall_word': 0.989736, 'acc_grapheme': 0.990704, 'acc_vowel': 0.996536, 'acc_consonant': 0.996262, 'acc_word': 0.989483, 'loss_grapheme': 0.044756, 'loss_vowel': 0.035303, 'loss_consonant': 0.028916, 'loss_word': 0.042797}\n",
      "   53 | 0.000002 | 160600/160716 | 7.8986 | 6.5621 |||\n",
      "val: {'recall': 0.995169, 'recall_grapheme': 0.99335, 'recall_vowel': 0.996648, 'recall_consonant': 0.997329, 'recall_word': 0.991165, 'acc_grapheme': 0.992648, 'acc_vowel': 0.996959, 'acc_consonant': 0.996984, 'acc_word': 0.990903, 'loss_grapheme': 0.031207, 'loss_vowel': 0.019483, 'loss_consonant': 0.015602, 'loss_word': 0.034097}\n",
      "   54 | 0.000002 | 160600/160716 | 8.0389 | 6.1329 ||\n",
      "val: {'recall': 0.993768, 'recall_grapheme': 0.990746, 'recall_vowel': 0.995986, 'recall_consonant': 0.997593, 'recall_word': 0.989124, 'acc_grapheme': 0.99033, 'acc_vowel': 0.996411, 'acc_consonant': 0.996536, 'acc_word': 0.989034, 'loss_grapheme': 0.041608, 'loss_vowel': 0.027592, 'loss_consonant': 0.022134, 'loss_word': 0.044491}\n",
      "   55 | 0.000001 | 160600/160716 | 0.2141 | 6.6081 ||\n",
      "val: {'recall': 0.994863, 'recall_grapheme': 0.992512, 'recall_vowel': 0.996786, 'recall_consonant': 0.997642, 'recall_word': 0.990691, 'acc_grapheme': 0.991576, 'acc_vowel': 0.99686, 'acc_consonant': 0.99666, 'acc_word': 0.990455, 'loss_grapheme': 0.035313, 'loss_vowel': 0.024137, 'loss_consonant': 0.020697, 'loss_word': 0.036395}\n",
      "   56 | 0.000001 | 160600/160716 | 15.2286 | 6.8788 |\n",
      "val: {'recall': 0.995785, 'recall_grapheme': 0.994107, 'recall_vowel': 0.996942, 'recall_consonant': 0.997984, 'recall_word': 0.992536, 'acc_grapheme': 0.993146, 'acc_vowel': 0.997159, 'acc_consonant': 0.997283, 'acc_word': 0.992324, 'loss_grapheme': 0.027458, 'loss_vowel': 0.02081, 'loss_consonant': 0.015793, 'loss_word': 0.027966}\n",
      "   57 | 0.000000 | 160600/160716 | 0.7548 | 5.9674 ||\n",
      "val: {'recall': 0.995163, 'recall_grapheme': 0.993122, 'recall_vowel': 0.996699, 'recall_consonant': 0.997706, 'recall_word': 0.990968, 'acc_grapheme': 0.99205, 'acc_vowel': 0.996885, 'acc_consonant': 0.99681, 'acc_word': 0.990704, 'loss_grapheme': 0.034462, 'loss_vowel': 0.024702, 'loss_consonant': 0.019842, 'loss_word': 0.035838}\n",
      "   58 | 0.000000 | 160600/160716 | 0.1758 | 6.5124 ||\n",
      "val: {'recall': 0.995604, 'recall_grapheme': 0.993846, 'recall_vowel': 0.996904, 'recall_consonant': 0.997819, 'recall_word': 0.992048, 'acc_grapheme': 0.992872, 'acc_vowel': 0.997134, 'acc_consonant': 0.997034, 'acc_word': 0.9918, 'loss_grapheme': 0.030398, 'loss_vowel': 0.025652, 'loss_consonant': 0.019974, 'loss_word': 0.030414}\n",
      "   59 | 0.000000 | 160600/160716 | 1.4738 | 5.7327 ||\n",
      "val: {'recall': 0.99536, 'recall_grapheme': 0.993351, 'recall_vowel': 0.996779, 'recall_consonant': 0.99796, 'recall_word': 0.991347, 'acc_grapheme': 0.992448, 'acc_vowel': 0.997034, 'acc_consonant': 0.997184, 'acc_word': 0.991103, 'loss_grapheme': 0.031396, 'loss_vowel': 0.016997, 'loss_consonant': 0.013332, 'loss_word': 0.034868}\n",
      "CYCLE: 3\n",
      "    0 | 0.000020 | 160600/160716 | 4.1819 | 6.7042 ||\n",
      "val: {'recall': 0.994607, 'recall_grapheme': 0.992002, 'recall_vowel': 0.996676, 'recall_consonant': 0.997749, 'recall_word': 0.99033, 'acc_grapheme': 0.991252, 'acc_vowel': 0.996735, 'acc_consonant': 0.996461, 'acc_word': 0.990131, 'loss_grapheme': 0.041988, 'loss_vowel': 0.037142, 'loss_consonant': 0.030338, 'loss_word': 0.039458}\n",
      "    1 | 0.000040 | 160600/160716 | 6.9873 | 6.2584 ||\n",
      "val: {'recall': 0.993171, 'recall_grapheme': 0.98969, 'recall_vowel': 0.99598, 'recall_consonant': 0.997324, 'recall_word': 0.987152, 'acc_grapheme': 0.988934, 'acc_vowel': 0.996062, 'acc_consonant': 0.996112, 'acc_word': 0.986916, 'loss_grapheme': 0.058535, 'loss_vowel': 0.043977, 'loss_consonant': 0.034508, 'loss_word': 0.056091}\n",
      "    2 | 0.000060 | 160600/160716 | 7.8918 | 6.1830 |||\n",
      "val: {'recall': 0.994198, 'recall_grapheme': 0.991334, 'recall_vowel': 0.996557, 'recall_consonant': 0.997567, 'recall_word': 0.989828, 'acc_grapheme': 0.990978, 'acc_vowel': 0.996685, 'acc_consonant': 0.996361, 'acc_word': 0.989632, 'loss_grapheme': 0.038972, 'loss_vowel': 0.026409, 'loss_consonant': 0.020353, 'loss_word': 0.041028}\n",
      "    3 | 0.000079 | 160600/160716 | 0.1521 | 5.9602 ||\n",
      "val: {'recall': 0.995083, 'recall_grapheme': 0.992907, 'recall_vowel': 0.996648, 'recall_consonant': 0.997868, 'recall_word': 0.991324, 'acc_grapheme': 0.992149, 'acc_vowel': 0.99686, 'acc_consonant': 0.997084, 'acc_word': 0.991177, 'loss_grapheme': 0.031634, 'loss_vowel': 0.019129, 'loss_consonant': 0.01602, 'loss_word': 0.033555}\n",
      "    4 | 0.000098 | 160600/160716 | 16.2739 | 6.2299 ||\n",
      "val: {'recall': 0.994442, 'recall_grapheme': 0.992096, 'recall_vowel': 0.996545, 'recall_consonant': 0.997031, 'recall_word': 0.990607, 'acc_grapheme': 0.991252, 'acc_vowel': 0.996436, 'acc_consonant': 0.996511, 'acc_word': 0.990455, 'loss_grapheme': 0.04053, 'loss_vowel': 0.036945, 'loss_consonant': 0.028749, 'loss_word': 0.037475}\n",
      "    5 | 0.000098 | 160600/160716 | 6.8101 | 6.4388 ||\n",
      "val: {'recall': 0.993034, 'recall_grapheme': 0.990014, 'recall_vowel': 0.996213, 'recall_consonant': 0.995896, 'recall_word': 0.987928, 'acc_grapheme': 0.988909, 'acc_vowel': 0.996137, 'acc_consonant': 0.996112, 'acc_word': 0.987713, 'loss_grapheme': 0.050686, 'loss_vowel': 0.034982, 'loss_consonant': 0.026995, 'loss_word': 0.050267}\n",
      "    6 | 0.000097 | 160600/160716 | 0.1915 | 6.4500 |||\n",
      "val: {'recall': 0.993765, 'recall_grapheme': 0.990772, 'recall_vowel': 0.995982, 'recall_consonant': 0.997534, 'recall_word': 0.989201, 'acc_grapheme': 0.990255, 'acc_vowel': 0.996237, 'acc_consonant': 0.996336, 'acc_word': 0.989084, 'loss_grapheme': 0.042382, 'loss_vowel': 0.028614, 'loss_consonant': 0.024669, 'loss_word': 0.042387}\n",
      "    7 | 0.000096 | 160600/160716 | 0.1842 | 6.8159 ||\n",
      "val: {'recall': 0.995185, 'recall_grapheme': 0.992936, 'recall_vowel': 0.996707, 'recall_consonant': 0.99816, 'recall_word': 0.99118, 'acc_grapheme': 0.992423, 'acc_vowel': 0.99671, 'acc_consonant': 0.997134, 'acc_word': 0.991078, 'loss_grapheme': 0.031679, 'loss_vowel': 0.017027, 'loss_consonant': 0.012635, 'loss_word': 0.035271}\n",
      "    8 | 0.000095 | 160600/160716 | 16.8392 | 6.3035 ||\n",
      "val: {'recall': 0.993522, 'recall_grapheme': 0.990468, 'recall_vowel': 0.995884, 'recall_consonant': 0.997269, 'recall_word': 0.989048, 'acc_grapheme': 0.990131, 'acc_vowel': 0.996287, 'acc_consonant': 0.996311, 'acc_word': 0.988884, 'loss_grapheme': 0.05237, 'loss_vowel': 0.040775, 'loss_consonant': 0.033667, 'loss_word': 0.047854}\n",
      "    9 | 0.000093 | 160600/160716 | 17.4574 | 6.3662 ||\n",
      "val: {'recall': 0.995342, 'recall_grapheme': 0.993399, 'recall_vowel': 0.996325, 'recall_consonant': 0.998247, 'recall_word': 0.992258, 'acc_grapheme': 0.992797, 'acc_vowel': 0.996735, 'acc_consonant': 0.997184, 'acc_word': 0.992124, 'loss_grapheme': 0.02967, 'loss_vowel': 0.021788, 'loss_consonant': 0.01698, 'loss_word': 0.02986}\n",
      "   10 | 0.000092 | 160600/160716 | 5.2954 | 6.6608 ||\n",
      "val: {'recall': 0.995284, 'recall_grapheme': 0.993336, 'recall_vowel': 0.996757, 'recall_consonant': 0.997707, 'recall_word': 0.991466, 'acc_grapheme': 0.992174, 'acc_vowel': 0.99671, 'acc_consonant': 0.996835, 'acc_word': 0.991402, 'loss_grapheme': 0.032203, 'loss_vowel': 0.024394, 'loss_consonant': 0.01804, 'loss_word': 0.032363}\n",
      "   11 | 0.000090 | 160600/160716 | 7.1697 | 6.3320 ||\n",
      "val: {'recall': 0.993702, 'recall_grapheme': 0.991452, 'recall_vowel': 0.995768, 'recall_consonant': 0.996137, 'recall_word': 0.989789, 'acc_grapheme': 0.990804, 'acc_vowel': 0.996262, 'acc_consonant': 0.996212, 'acc_word': 0.989682, 'loss_grapheme': 0.04059, 'loss_vowel': 0.030692, 'loss_consonant': 0.024687, 'loss_word': 0.040353}\n",
      "   12 | 0.000089 | 160600/160716 | 12.7059 | 6.4564 |\n",
      "val: {'recall': 0.99487, 'recall_grapheme': 0.992786, 'recall_vowel': 0.996261, 'recall_consonant': 0.997648, 'recall_word': 0.991074, 'acc_grapheme': 0.991975, 'acc_vowel': 0.996511, 'acc_consonant': 0.99691, 'acc_word': 0.990953, 'loss_grapheme': 0.036887, 'loss_vowel': 0.031757, 'loss_consonant': 0.02479, 'loss_word': 0.035243}\n",
      "   13 | 0.000087 | 160600/160716 | 16.3252 | 6.5952 ||\n",
      "val: {'recall': 0.994781, 'recall_grapheme': 0.992356, 'recall_vowel': 0.996626, 'recall_consonant': 0.997786, 'recall_word': 0.991914, 'acc_grapheme': 0.992299, 'acc_vowel': 0.996935, 'acc_consonant': 0.997084, 'acc_word': 0.991825, 'loss_grapheme': 0.031578, 'loss_vowel': 0.02381, 'loss_consonant': 0.01892, 'loss_word': 0.030894}\n",
      "   14 | 0.000085 | 160600/160716 | 17.9967 | 6.5251 ||\n",
      "val: {'recall': 0.993621, 'recall_grapheme': 0.990887, 'recall_vowel': 0.996263, 'recall_consonant': 0.996446, 'recall_word': 0.988425, 'acc_grapheme': 0.990081, 'acc_vowel': 0.996311, 'acc_consonant': 0.996336, 'acc_word': 0.988261, 'loss_grapheme': 0.055908, 'loss_vowel': 0.042565, 'loss_consonant': 0.035621, 'loss_word': 0.05123}\n",
      "   15 | 0.000083 | 160600/160716 | 11.7742 | 6.6820 ||\n",
      "val: {'recall': 0.994532, 'recall_grapheme': 0.992418, 'recall_vowel': 0.996257, 'recall_consonant': 0.997036, 'recall_word': 0.99024, 'acc_grapheme': 0.991177, 'acc_vowel': 0.996536, 'acc_consonant': 0.996237, 'acc_word': 0.990131, 'loss_grapheme': 0.042878, 'loss_vowel': 0.035554, 'loss_consonant': 0.029961, 'loss_word': 0.040423}\n",
      "   16 | 0.000081 | 160600/160716 | 0.2147 | 6.5747 ||\n",
      "val: {'recall': 0.993826, 'recall_grapheme': 0.99079, 'recall_vowel': 0.996057, 'recall_consonant': 0.997667, 'recall_word': 0.989866, 'acc_grapheme': 0.99038, 'acc_vowel': 0.996561, 'acc_consonant': 0.99666, 'acc_word': 0.989657, 'loss_grapheme': 0.040538, 'loss_vowel': 0.026234, 'loss_consonant': 0.021252, 'loss_word': 0.039919}\n",
      "   17 | 0.000079 | 160600/160716 | 0.2578 | 6.5043 |||\n",
      "val: {'recall': 0.994133, 'recall_grapheme': 0.9912, 'recall_vowel': 0.996269, 'recall_consonant': 0.997865, 'recall_word': 0.990417, 'acc_grapheme': 0.991103, 'acc_vowel': 0.996411, 'acc_consonant': 0.996611, 'acc_word': 0.990205, 'loss_grapheme': 0.037946, 'loss_vowel': 0.02836, 'loss_consonant': 0.023945, 'loss_word': 0.037583}\n",
      "   18 | 0.000077 | 160600/160716 | 0.2016 | 6.6860 ||\n",
      "val: {'recall': 0.994552, 'recall_grapheme': 0.992118, 'recall_vowel': 0.996226, 'recall_consonant': 0.997744, 'recall_word': 0.990911, 'acc_grapheme': 0.991726, 'acc_vowel': 0.996586, 'acc_consonant': 0.996486, 'acc_word': 0.990729, 'loss_grapheme': 0.036634, 'loss_vowel': 0.026958, 'loss_consonant': 0.023051, 'loss_word': 0.035791}\n",
      "   19 | 0.000075 | 160600/160716 | 0.1628 | 6.2748 ||\n",
      "val: {'recall': 0.995002, 'recall_grapheme': 0.992949, 'recall_vowel': 0.996443, 'recall_consonant': 0.997667, 'recall_word': 0.990733, 'acc_grapheme': 0.991975, 'acc_vowel': 0.996935, 'acc_consonant': 0.996685, 'acc_word': 0.990604, 'loss_grapheme': 0.033308, 'loss_vowel': 0.018244, 'loss_consonant': 0.016359, 'loss_word': 0.035212}\n",
      "   20 | 0.000073 | 160600/160716 | 9.0990 | 5.7793 |||\n",
      "val: {'recall': 0.994614, 'recall_grapheme': 0.992023, 'recall_vowel': 0.996552, 'recall_consonant': 0.99786, 'recall_word': 0.990797, 'acc_grapheme': 0.991551, 'acc_vowel': 0.996835, 'acc_consonant': 0.996885, 'acc_word': 0.990654, 'loss_grapheme': 0.034567, 'loss_vowel': 0.0204, 'loss_consonant': 0.017855, 'loss_word': 0.035745}\n",
      "   22 | 0.000068 | 160600/160716 | 8.8319 | 6.1099 ||\n",
      "val: {'recall': 0.993247, 'recall_grapheme': 0.989922, 'recall_vowel': 0.995754, 'recall_consonant': 0.99739, 'recall_word': 0.987807, 'acc_grapheme': 0.989109, 'acc_vowel': 0.996187, 'acc_consonant': 0.996287, 'acc_word': 0.987489, 'loss_grapheme': 0.04791, 'loss_vowel': 0.02921, 'loss_consonant': 0.024012, 'loss_word': 0.050102}\n",
      "   23 | 0.000065 | 160600/160716 | 6.7603 | 6.1738 |||\n",
      "val: {'recall': 0.994194, 'recall_grapheme': 0.991241, 'recall_vowel': 0.996415, 'recall_consonant': 0.997877, 'recall_word': 0.990163, 'acc_grapheme': 0.990903, 'acc_vowel': 0.996635, 'acc_consonant': 0.99686, 'acc_word': 0.989956, 'loss_grapheme': 0.036226, 'loss_vowel': 0.019766, 'loss_consonant': 0.016232, 'loss_word': 0.03854}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   24 | 0.000063 | 160600/160716 | 0.2151 | 6.5917 ||\n",
      "val: {'recall': 0.995077, 'recall_grapheme': 0.992798, 'recall_vowel': 0.996663, 'recall_consonant': 0.998048, 'recall_word': 0.991815, 'acc_grapheme': 0.992349, 'acc_vowel': 0.99676, 'acc_consonant': 0.997059, 'acc_word': 0.991676, 'loss_grapheme': 0.031272, 'loss_vowel': 0.022871, 'loss_consonant': 0.018231, 'loss_word': 0.030701}\n",
      "   25 | 0.000060 | 160600/160716 | 0.2318 | 6.5634 |||\n",
      "val: {'recall': 0.994484, 'recall_grapheme': 0.991988, 'recall_vowel': 0.996017, 'recall_consonant': 0.997941, 'recall_word': 0.990986, 'acc_grapheme': 0.991751, 'acc_vowel': 0.996611, 'acc_consonant': 0.99691, 'acc_word': 0.990853, 'loss_grapheme': 0.033002, 'loss_vowel': 0.019112, 'loss_consonant': 0.015692, 'loss_word': 0.035463}\n",
      "   26 | 0.000058 | 160600/160716 | 0.2077 | 6.2640 ||\n",
      "val: {'recall': 0.993863, 'recall_grapheme': 0.99135, 'recall_vowel': 0.995571, 'recall_consonant': 0.997181, 'recall_word': 0.989498, 'acc_grapheme': 0.990255, 'acc_vowel': 0.996162, 'acc_consonant': 0.996386, 'acc_word': 0.989308, 'loss_grapheme': 0.044807, 'loss_vowel': 0.035907, 'loss_consonant': 0.026513, 'loss_word': 0.043151}\n",
      "   27 | 0.000055 | 160600/160716 | 0.1721 | 6.4803 ||\n",
      "val: {'recall': 0.995386, 'recall_grapheme': 0.993633, 'recall_vowel': 0.996587, 'recall_consonant': 0.997692, 'recall_word': 0.99215, 'acc_grapheme': 0.992548, 'acc_vowel': 0.99691, 'acc_consonant': 0.997059, 'acc_word': 0.99205, 'loss_grapheme': 0.029804, 'loss_vowel': 0.020688, 'loss_consonant': 0.015994, 'loss_word': 0.030235}\n",
      "   28 | 0.000053 | 160600/160716 | 0.1999 | 6.3017 ||\n",
      "val: {'recall': 0.995249, 'recall_grapheme': 0.993257, 'recall_vowel': 0.996294, 'recall_consonant': 0.998186, 'recall_word': 0.992059, 'acc_grapheme': 0.992673, 'acc_vowel': 0.996935, 'acc_consonant': 0.997433, 'acc_word': 0.991925, 'loss_grapheme': 0.02879, 'loss_vowel': 0.01505, 'loss_consonant': 0.011443, 'loss_word': 0.03123}\n",
      "   29 | 0.000050 | 160600/160716 | 9.3958 | 6.4239 |||\n",
      "val: {'recall': 0.994676, 'recall_grapheme': 0.992567, 'recall_vowel': 0.996403, 'recall_consonant': 0.997169, 'recall_word': 0.991447, 'acc_grapheme': 0.99195, 'acc_vowel': 0.996785, 'acc_consonant': 0.996959, 'acc_word': 0.991227, 'loss_grapheme': 0.032617, 'loss_vowel': 0.021298, 'loss_consonant': 0.017722, 'loss_word': 0.033672}\n",
      "   30 | 0.000047 | 160600/160716 | 9.6693 | 6.2145 ||\n",
      "val: {'recall': 0.995345, 'recall_grapheme': 0.9934, 'recall_vowel': 0.996461, 'recall_consonant': 0.998118, 'recall_word': 0.992258, 'acc_grapheme': 0.993022, 'acc_vowel': 0.996984, 'acc_consonant': 0.997134, 'acc_word': 0.992124, 'loss_grapheme': 0.027499, 'loss_vowel': 0.01595, 'loss_consonant': 0.012948, 'loss_word': 0.029473}\n",
      "   31 | 0.000045 | 160600/160716 | 15.5089 | 5.9716 ||\n",
      "val: {'recall': 0.994598, 'recall_grapheme': 0.992123, 'recall_vowel': 0.996501, 'recall_consonant': 0.997644, 'recall_word': 0.990884, 'acc_grapheme': 0.991302, 'acc_vowel': 0.99666, 'acc_consonant': 0.996611, 'acc_word': 0.990679, 'loss_grapheme': 0.03533, 'loss_vowel': 0.024351, 'loss_consonant': 0.020811, 'loss_word': 0.035382}\n",
      "   32 | 0.000042 | 160600/160716 | 0.2633 | 6.0388 |||\n",
      "val: {'recall': 0.99424, 'recall_grapheme': 0.991792, 'recall_vowel': 0.995871, 'recall_consonant': 0.997507, 'recall_word': 0.98989, 'acc_grapheme': 0.990953, 'acc_vowel': 0.996411, 'acc_consonant': 0.996536, 'acc_word': 0.989682, 'loss_grapheme': 0.03733, 'loss_vowel': 0.022318, 'loss_consonant': 0.018682, 'loss_word': 0.039616}\n",
      "   33 | 0.000040 | 160600/160716 | 8.8202 | 5.9595 |||\n",
      "val: {'recall': 0.994939, 'recall_grapheme': 0.992865, 'recall_vowel': 0.996376, 'recall_consonant': 0.99765, 'recall_word': 0.990954, 'acc_grapheme': 0.992075, 'acc_vowel': 0.99676, 'acc_consonant': 0.996785, 'acc_word': 0.990804, 'loss_grapheme': 0.033161, 'loss_vowel': 0.020525, 'loss_consonant': 0.017708, 'loss_word': 0.034236}\n",
      "   34 | 0.000037 | 160600/160716 | 15.7377 | 5.8966 |\n",
      "val: {'recall': 0.994539, 'recall_grapheme': 0.991948, 'recall_vowel': 0.996231, 'recall_consonant': 0.998027, 'recall_word': 0.990462, 'acc_grapheme': 0.991302, 'acc_vowel': 0.996536, 'acc_consonant': 0.99686, 'acc_word': 0.990205, 'loss_grapheme': 0.036494, 'loss_vowel': 0.02495, 'loss_consonant': 0.021443, 'loss_word': 0.037173}\n",
      "   35 | 0.000035 | 160600/160716 | 9.5604 | 6.2726 ||\n",
      "val: {'recall': 0.994252, 'recall_grapheme': 0.991708, 'recall_vowel': 0.996092, 'recall_consonant': 0.9975, 'recall_word': 0.988861, 'acc_grapheme': 0.99048, 'acc_vowel': 0.996511, 'acc_consonant': 0.996187, 'acc_word': 0.988685, 'loss_grapheme': 0.047148, 'loss_vowel': 0.034356, 'loss_consonant': 0.029634, 'loss_word': 0.046667}\n",
      "   36 | 0.000032 | 160600/160716 | 6.6171 | 5.8094 |||\n",
      "val: {'recall': 0.993169, 'recall_grapheme': 0.990158, 'recall_vowel': 0.995509, 'recall_consonant': 0.99685, 'recall_word': 0.988177, 'acc_grapheme': 0.989557, 'acc_vowel': 0.996212, 'acc_consonant': 0.995813, 'acc_word': 0.988012, 'loss_grapheme': 0.052182, 'loss_vowel': 0.037943, 'loss_consonant': 0.030466, 'loss_word': 0.052057}\n",
      "   37 | 0.000030 | 160600/160716 | 0.2602 | 6.1744 ||\n",
      "val: {'recall': 0.994986, 'recall_grapheme': 0.992659, 'recall_vowel': 0.996726, 'recall_consonant': 0.997901, 'recall_word': 0.990912, 'acc_grapheme': 0.992124, 'acc_vowel': 0.99681, 'acc_consonant': 0.996885, 'acc_word': 0.990804, 'loss_grapheme': 0.032375, 'loss_vowel': 0.016639, 'loss_consonant': 0.013619, 'loss_word': 0.035992}\n",
      "   38 | 0.000027 | 160600/160716 | 15.5352 | 6.1242 |\n",
      "val: {'recall': 0.995714, 'recall_grapheme': 0.994029, 'recall_vowel': 0.996904, 'recall_consonant': 0.997895, 'recall_word': 0.992342, 'acc_grapheme': 0.993221, 'acc_vowel': 0.996959, 'acc_consonant': 0.997184, 'acc_word': 0.992174, 'loss_grapheme': 0.028416, 'loss_vowel': 0.01941, 'loss_consonant': 0.016207, 'loss_word': 0.029375}\n",
      "   39 | 0.000025 | 160600/160716 | 13.9728 | 5.7660 ||\n",
      "val: {'recall': 0.994831, 'recall_grapheme': 0.992376, 'recall_vowel': 0.996753, 'recall_consonant': 0.997816, 'recall_word': 0.990956, 'acc_grapheme': 0.99185, 'acc_vowel': 0.99681, 'acc_consonant': 0.99681, 'acc_word': 0.990754, 'loss_grapheme': 0.033091, 'loss_vowel': 0.020082, 'loss_consonant': 0.017768, 'loss_word': 0.03486}\n",
      "   40 | 0.000023 | 160600/160716 | 2.9377 | 5.4944 |||\n",
      "val: {'recall': 0.994865, 'recall_grapheme': 0.992548, 'recall_vowel': 0.996653, 'recall_consonant': 0.997713, 'recall_word': 0.990842, 'acc_grapheme': 0.99185, 'acc_vowel': 0.99681, 'acc_consonant': 0.99681, 'acc_word': 0.990604, 'loss_grapheme': 0.03361, 'loss_vowel': 0.021751, 'loss_consonant': 0.018253, 'loss_word': 0.035124}\n",
      "   41 | 0.000021 | 160600/160716 | 8.7579 | 6.3978 |||\n",
      "val: {'recall': 0.995227, 'recall_grapheme': 0.993157, 'recall_vowel': 0.996769, 'recall_consonant': 0.997822, 'recall_word': 0.991607, 'acc_grapheme': 0.992598, 'acc_vowel': 0.996885, 'acc_consonant': 0.997059, 'acc_word': 0.991427, 'loss_grapheme': 0.030311, 'loss_vowel': 0.017357, 'loss_consonant': 0.013784, 'loss_word': 0.033261}\n",
      "   42 | 0.000019 | 160600/160716 | 8.9385 | 5.7880 ||\n",
      "val: {'recall': 0.994812, 'recall_grapheme': 0.992456, 'recall_vowel': 0.99649, 'recall_consonant': 0.997847, 'recall_word': 0.99058, 'acc_grapheme': 0.991751, 'acc_vowel': 0.99671, 'acc_consonant': 0.996785, 'acc_word': 0.990455, 'loss_grapheme': 0.03482, 'loss_vowel': 0.018701, 'loss_consonant': 0.01536, 'loss_word': 0.038633}\n",
      "   43 | 0.000017 | 160600/160716 | 0.1883 | 5.6816 ||\n",
      "val: {'recall': 0.995159, 'recall_grapheme': 0.992955, 'recall_vowel': 0.996731, 'recall_consonant': 0.997994, 'recall_word': 0.991268, 'acc_grapheme': 0.992224, 'acc_vowel': 0.99681, 'acc_consonant': 0.996959, 'acc_word': 0.991078, 'loss_grapheme': 0.033369, 'loss_vowel': 0.020848, 'loss_consonant': 0.017486, 'loss_word': 0.034844}\n",
      "   44 | 0.000015 | 160600/160716 | 16.5565 | 6.0888 |\n",
      "val: {'recall': 0.994133, 'recall_grapheme': 0.991293, 'recall_vowel': 0.996473, 'recall_consonant': 0.997474, 'recall_word': 0.989421, 'acc_grapheme': 0.990704, 'acc_vowel': 0.996361, 'acc_consonant': 0.996262, 'acc_word': 0.989358, 'loss_grapheme': 0.04266, 'loss_vowel': 0.033103, 'loss_consonant': 0.026574, 'loss_word': 0.042156}\n",
      "   45 | 0.000013 | 160600/160716 | 14.9055 | 6.4455 |\n",
      "val: {'recall': 0.993944, 'recall_grapheme': 0.991186, 'recall_vowel': 0.996273, 'recall_consonant': 0.99713, 'recall_word': 0.989308, 'acc_grapheme': 0.990554, 'acc_vowel': 0.996311, 'acc_consonant': 0.996037, 'acc_word': 0.989208, 'loss_grapheme': 0.046117, 'loss_vowel': 0.040817, 'loss_consonant': 0.033281, 'loss_word': 0.043132}\n",
      "   46 | 0.000011 | 160600/160716 | 11.8676 | 5.8841 |\n",
      "val: {'recall': 0.995131, 'recall_grapheme': 0.993029, 'recall_vowel': 0.996669, 'recall_consonant': 0.997797, 'recall_word': 0.990767, 'acc_grapheme': 0.99205, 'acc_vowel': 0.996835, 'acc_consonant': 0.997009, 'acc_word': 0.990579, 'loss_grapheme': 0.03278, 'loss_vowel': 0.018063, 'loss_consonant': 0.015047, 'loss_word': 0.036223}\n",
      "   47 | 0.000010 | 160600/160716 | 0.7889 | 6.0845 ||\n",
      "val: {'recall': 0.995875, 'recall_grapheme': 0.994303, 'recall_vowel': 0.996769, 'recall_consonant': 0.998126, 'recall_word': 0.992575, 'acc_grapheme': 0.993395, 'acc_vowel': 0.996935, 'acc_consonant': 0.997458, 'acc_word': 0.992399, 'loss_grapheme': 0.027303, 'loss_vowel': 0.019547, 'loss_consonant': 0.015128, 'loss_word': 0.027597}\n",
      "###>>>>> saved ./model4-ckps/se_resnext101_32x4d/model4_se_resnext101_fold3_224.pth\n",
      "   48 | 0.000008 | 160600/160716 | 0.1322 | 6.2294 |||\n",
      "val: {'recall': 0.995506, 'recall_grapheme': 0.993608, 'recall_vowel': 0.996777, 'recall_consonant': 0.99803, 'recall_word': 0.992523, 'acc_grapheme': 0.993047, 'acc_vowel': 0.997134, 'acc_consonant': 0.997308, 'acc_word': 0.992324, 'loss_grapheme': 0.027353, 'loss_vowel': 0.017666, 'loss_consonant': 0.014507, 'loss_word': 0.028866}\n",
      "   49 | 0.000007 | 160600/160716 | 8.4294 | 6.0272 ||\n",
      "val: {'recall': 0.993235, 'recall_grapheme': 0.990224, 'recall_vowel': 0.995639, 'recall_consonant': 0.996853, 'recall_word': 0.988213, 'acc_grapheme': 0.989582, 'acc_vowel': 0.996037, 'acc_consonant': 0.995639, 'acc_word': 0.988062, 'loss_grapheme': 0.059396, 'loss_vowel': 0.049274, 'loss_consonant': 0.0399, 'loss_word': 0.053944}\n",
      "   50 | 0.000005 | 160600/160716 | 8.1289 | 5.8627 ||\n",
      "val: {'recall': 0.994654, 'recall_grapheme': 0.992123, 'recall_vowel': 0.996722, 'recall_consonant': 0.997647, 'recall_word': 0.989985, 'acc_grapheme': 0.991227, 'acc_vowel': 0.996511, 'acc_consonant': 0.996561, 'acc_word': 0.989832, 'loss_grapheme': 0.03694, 'loss_vowel': 0.023976, 'loss_consonant': 0.019332, 'loss_word': 0.039316}\n",
      "   51 | 0.000004 | 160600/160716 | 15.4145 | 5.7124 |\n",
      "val: {'recall': 0.995445, 'recall_grapheme': 0.993693, 'recall_vowel': 0.996361, 'recall_consonant': 0.998033, 'recall_word': 0.992658, 'acc_grapheme': 0.992922, 'acc_vowel': 0.996785, 'acc_consonant': 0.997258, 'acc_word': 0.992448, 'loss_grapheme': 0.028471, 'loss_vowel': 0.019426, 'loss_consonant': 0.015913, 'loss_word': 0.029431}\n",
      "   52 | 0.000003 | 160600/160716 | 0.2509 | 6.0602 ||\n",
      "val: {'recall': 0.995547, 'recall_grapheme': 0.993714, 'recall_vowel': 0.996694, 'recall_consonant': 0.998067, 'recall_word': 0.992095, 'acc_grapheme': 0.992922, 'acc_vowel': 0.996835, 'acc_consonant': 0.997333, 'acc_word': 0.991925, 'loss_grapheme': 0.029294, 'loss_vowel': 0.015529, 'loss_consonant': 0.012308, 'loss_word': 0.031994}\n",
      "   53 | 0.000002 | 160600/160716 | 8.6890 | 5.5696 |||\n",
      "val: {'recall': 0.994114, 'recall_grapheme': 0.99109, 'recall_vowel': 0.996718, 'recall_consonant': 0.997556, 'recall_word': 0.989013, 'acc_grapheme': 0.990554, 'acc_vowel': 0.996511, 'acc_consonant': 0.996287, 'acc_word': 0.98886, 'loss_grapheme': 0.040756, 'loss_vowel': 0.026921, 'loss_consonant': 0.0221, 'loss_word': 0.044031}\n",
      "   54 | 0.000002 | 160600/160716 | 5.9178 | 5.8852 |||\n",
      "val: {'recall': 0.994781, 'recall_grapheme': 0.992347, 'recall_vowel': 0.996565, 'recall_consonant': 0.997864, 'recall_word': 0.990486, 'acc_grapheme': 0.991651, 'acc_vowel': 0.996735, 'acc_consonant': 0.99691, 'acc_word': 0.99028, 'loss_grapheme': 0.034598, 'loss_vowel': 0.017982, 'loss_consonant': 0.014911, 'loss_word': 0.038966}\n",
      "   55 | 0.000001 | 160600/160716 | 8.1357 | 5.6122 ||\n",
      "val: {'recall': 0.995029, 'recall_grapheme': 0.99282, 'recall_vowel': 0.996627, 'recall_consonant': 0.997848, 'recall_word': 0.991068, 'acc_grapheme': 0.992174, 'acc_vowel': 0.996635, 'acc_consonant': 0.996835, 'acc_word': 0.990928, 'loss_grapheme': 0.032508, 'loss_vowel': 0.020919, 'loss_consonant': 0.01783, 'loss_word': 0.034322}\n",
      "   56 | 0.000001 | 137720/160716 | 5.5808 | 5.7590 ||"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-aad596905035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-b2c1bf117c1f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mvalidate_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-ebc31c90964f>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(args, model, train_loader, epoch, optimizer, lr_scheduler, grid)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_aux1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_aux2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mloss_primary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mloss_aux1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_aux1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0m_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_tup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for param in model.backbone.parameters():\n",
    "#    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validate(nn.DataParallel(model), val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init model4\n",
      "model file: ./model4-ckps/se_resnext101_32x4d/model4_se_resnext101_fold3_224.pth, exist: True\n",
      "loading ./model4-ckps/se_resnext101_32x4d/model4_se_resnext101_fold3_224.pth...\n",
      "init model4\n",
      "model file: ./model4-ckps/se_resnext101_32x4d/model4_se_resnext101_fold3_224.pth, exist: True\n",
      "loading ./model4-ckps/se_resnext101_32x4d/model4_se_resnext101_fold3_224.pth...\n",
      "\n",
      "val: {'recall': 0.995777, 'recall_grapheme': 0.994077, 'recall_vowel': 0.996846, 'recall_consonant': 0.99811, 'recall_word': 0.99307, 'acc_grapheme': 0.99347, 'acc_vowel': 0.997059, 'acc_consonant': 0.997682, 'acc_word': 0.993047, 'loss_grapheme': 0.0246, 'loss_vowel': 0.012441, 'loss_consonant': 0.008668, 'loss_word': 0.02692}\n",
      "CYCLE: 1\n",
      "    0 | 0.000015 | 160600/160716 | 1.9527 | 12.0129 ||\n",
      "val: {'recall': 0.992213, 'recall_grapheme': 0.988496, 'recall_vowel': 0.995465, 'recall_consonant': 0.996394, 'recall_word': 0.98645, 'acc_grapheme': 0.988037, 'acc_vowel': 0.996012, 'acc_consonant': 0.995763, 'acc_word': 0.986317, 'loss_grapheme': 0.082455, 'loss_vowel': 0.06918, 'loss_consonant': 0.053365, 'loss_word': 0.073632}\n",
      "    1 | 0.000030 | 160600/160716 | 17.9707 | 11.5853 |\n",
      "val: {'recall': 0.992857, 'recall_grapheme': 0.989524, 'recall_vowel': 0.995875, 'recall_consonant': 0.996505, 'recall_word': 0.987725, 'acc_grapheme': 0.98876, 'acc_vowel': 0.995987, 'acc_consonant': 0.995913, 'acc_word': 0.987539, 'loss_grapheme': 0.086435, 'loss_vowel': 0.07847, 'loss_consonant': 0.059214, 'loss_word': 0.074878}\n",
      "    2 | 0.000045 | 160600/160716 | 20.7364 | 12.4012 |\n",
      "val: {'recall': 0.992697, 'recall_grapheme': 0.989479, 'recall_vowel': 0.995378, 'recall_consonant': 0.996453, 'recall_word': 0.987398, 'acc_grapheme': 0.988386, 'acc_vowel': 0.995913, 'acc_consonant': 0.996062, 'acc_word': 0.987289, 'loss_grapheme': 0.083874, 'loss_vowel': 0.075178, 'loss_consonant': 0.056502, 'loss_word': 0.073393}\n",
      "    3 | 0.000059 | 160600/160716 | 18.6038 | 12.3818 |\n",
      "val: {'recall': 0.992324, 'recall_grapheme': 0.988672, 'recall_vowel': 0.995846, 'recall_consonant': 0.996105, 'recall_word': 0.987234, 'acc_grapheme': 0.988112, 'acc_vowel': 0.995938, 'acc_consonant': 0.996037, 'acc_word': 0.98709, 'loss_grapheme': 0.093457, 'loss_vowel': 0.086879, 'loss_consonant': 0.062944, 'loss_word': 0.08069}\n",
      "    4 | 0.000074 | 160600/160716 | 0.8492 | 12.1740 ||\n",
      "val: {'recall': 0.993083, 'recall_grapheme': 0.98949, 'recall_vowel': 0.996097, 'recall_consonant': 0.997258, 'recall_word': 0.988482, 'acc_grapheme': 0.98876, 'acc_vowel': 0.996212, 'acc_consonant': 0.996311, 'acc_word': 0.988336, 'loss_grapheme': 0.067174, 'loss_vowel': 0.059875, 'loss_consonant': 0.044093, 'loss_word': 0.06132}\n",
      "    5 | 0.000088 | 160600/160716 | 3.8895 | 12.5137 ||\n",
      "val: {'recall': 0.99195, 'recall_grapheme': 0.98818, 'recall_vowel': 0.995371, 'recall_consonant': 0.996069, 'recall_word': 0.986257, 'acc_grapheme': 0.987763, 'acc_vowel': 0.995863, 'acc_consonant': 0.995987, 'acc_word': 0.986118, 'loss_grapheme': 0.076243, 'loss_vowel': 0.063291, 'loss_consonant': 0.047558, 'loss_word': 0.071323}\n",
      "    6 | 0.000101 | 160600/160716 | 20.5675 | 12.3185 |\n",
      "val: {'recall': 0.992479, 'recall_grapheme': 0.988624, 'recall_vowel': 0.995984, 'recall_consonant': 0.996686, 'recall_word': 0.988529, 'acc_grapheme': 0.988386, 'acc_vowel': 0.995938, 'acc_consonant': 0.996336, 'acc_word': 0.988461, 'loss_grapheme': 0.096668, 'loss_vowel': 0.092671, 'loss_consonant': 0.067271, 'loss_word': 0.076673}\n",
      "    7 | 0.000115 | 160600/160716 | 20.5747 | 12.5078 |\n",
      "val: {'recall': 0.990866, 'recall_grapheme': 0.986348, 'recall_vowel': 0.994824, 'recall_consonant': 0.995946, 'recall_word': 0.984891, 'acc_grapheme': 0.985919, 'acc_vowel': 0.99529, 'acc_consonant': 0.99524, 'acc_word': 0.984847, 'loss_grapheme': 0.104864, 'loss_vowel': 0.08627, 'loss_consonant': 0.066961, 'loss_word': 0.08749}\n",
      "    8 | 0.000128 | 160600/160716 | 16.8590 | 13.0703 |\n",
      "val: {'recall': 0.992898, 'recall_grapheme': 0.989631, 'recall_vowel': 0.995346, 'recall_consonant': 0.996984, 'recall_word': 0.988591, 'acc_grapheme': 0.98886, 'acc_vowel': 0.995838, 'acc_consonant': 0.995738, 'acc_word': 0.988461, 'loss_grapheme': 0.097666, 'loss_vowel': 0.087932, 'loss_consonant': 0.061648, 'loss_word': 0.080636}\n",
      "    9 | 0.000140 | 160600/160716 | 20.2234 | 12.7743 |\n",
      "val: {'recall': 0.991839, 'recall_grapheme': 0.987923, 'recall_vowel': 0.995469, 'recall_consonant': 0.996043, 'recall_word': 0.985905, 'acc_grapheme': 0.986766, 'acc_vowel': 0.995614, 'acc_consonant': 0.995738, 'acc_word': 0.985819, 'loss_grapheme': 0.101851, 'loss_vowel': 0.084429, 'loss_consonant': 0.063496, 'loss_word': 0.082784}\n",
      "   10 | 0.000138 | 160600/160716 | 3.5801 | 12.3786 ||\n",
      "val: {'recall': 0.99179, 'recall_grapheme': 0.988031, 'recall_vowel': 0.994414, 'recall_consonant': 0.996682, 'recall_word': 0.987235, 'acc_grapheme': 0.98714, 'acc_vowel': 0.995738, 'acc_consonant': 0.996012, 'acc_word': 0.987065, 'loss_grapheme': 0.088611, 'loss_vowel': 0.076509, 'loss_consonant': 0.060397, 'loss_word': 0.075894}\n",
      "   11 | 0.000136 | 160600/160716 | 6.8641 | 12.4698 ||\n",
      "val: {'recall': 0.993449, 'recall_grapheme': 0.990103, 'recall_vowel': 0.996278, 'recall_consonant': 0.997313, 'recall_word': 0.988556, 'acc_grapheme': 0.989383, 'acc_vowel': 0.996237, 'acc_consonant': 0.995913, 'acc_word': 0.988486, 'loss_grapheme': 0.088643, 'loss_vowel': 0.087421, 'loss_consonant': 0.062573, 'loss_word': 0.074454}\n",
      "   12 | 0.000133 | 160600/160716 | 20.9748 | 12.5303 |\n",
      "val: {'recall': 0.99278, 'recall_grapheme': 0.989581, 'recall_vowel': 0.99505, 'recall_consonant': 0.996907, 'recall_word': 0.988327, 'acc_grapheme': 0.98861, 'acc_vowel': 0.995938, 'acc_consonant': 0.996137, 'acc_word': 0.988162, 'loss_grapheme': 0.091454, 'loss_vowel': 0.090356, 'loss_consonant': 0.067038, 'loss_word': 0.076784}\n",
      "   13 | 0.000131 | 160600/160716 | 21.3200 | 12.5804 |\n",
      "val: {'recall': 0.992159, 'recall_grapheme': 0.988521, 'recall_vowel': 0.995207, 'recall_consonant': 0.996388, 'recall_word': 0.987412, 'acc_grapheme': 0.987314, 'acc_vowel': 0.995614, 'acc_consonant': 0.995713, 'acc_word': 0.98714, 'loss_grapheme': 0.10445, 'loss_vowel': 0.10299, 'loss_consonant': 0.074005, 'loss_word': 0.083693}\n",
      "   14 | 0.000128 | 160600/160716 | 7.2501 | 11.8166 ||\n",
      "val: {'recall': 0.991823, 'recall_grapheme': 0.988202, 'recall_vowel': 0.995221, 'recall_consonant': 0.995667, 'recall_word': 0.986356, 'acc_grapheme': 0.987613, 'acc_vowel': 0.995838, 'acc_consonant': 0.996012, 'acc_word': 0.986218, 'loss_grapheme': 0.083338, 'loss_vowel': 0.068276, 'loss_consonant': 0.053221, 'loss_word': 0.075108}\n",
      "   15 | 0.000125 | 160600/160716 | 17.0834 | 11.9679 |\n",
      "val: {'recall': 0.993086, 'recall_grapheme': 0.989856, 'recall_vowel': 0.995566, 'recall_consonant': 0.997066, 'recall_word': 0.988762, 'acc_grapheme': 0.988884, 'acc_vowel': 0.995888, 'acc_consonant': 0.996037, 'acc_word': 0.988635, 'loss_grapheme': 0.098301, 'loss_vowel': 0.099606, 'loss_consonant': 0.069307, 'loss_word': 0.076475}\n",
      "   16 | 0.000122 | 160600/160716 | 20.9062 | 12.1304 |\n",
      "val: {'recall': 0.992324, 'recall_grapheme': 0.988888, 'recall_vowel': 0.994895, 'recall_consonant': 0.996626, 'recall_word': 0.98748, 'acc_grapheme': 0.988286, 'acc_vowel': 0.995938, 'acc_consonant': 0.996112, 'acc_word': 0.987464, 'loss_grapheme': 0.074621, 'loss_vowel': 0.068201, 'loss_consonant': 0.053853, 'loss_word': 0.065039}\n",
      "   17 | 0.000119 | 160600/160716 | 20.9323 | 12.3977 |\n",
      "val: {'recall': 0.993135, 'recall_grapheme': 0.989852, 'recall_vowel': 0.995591, 'recall_consonant': 0.997244, 'recall_word': 0.989205, 'acc_grapheme': 0.989333, 'acc_vowel': 0.996012, 'acc_consonant': 0.996561, 'acc_word': 0.989134, 'loss_grapheme': 0.080396, 'loss_vowel': 0.079534, 'loss_consonant': 0.058093, 'loss_word': 0.065033}\n",
      "   18 | 0.000116 | 160600/160716 | 20.7899 | 11.9689 |\n",
      "val: {'recall': 0.992433, 'recall_grapheme': 0.988852, 'recall_vowel': 0.994755, 'recall_consonant': 0.997274, 'recall_word': 0.987955, 'acc_grapheme': 0.988635, 'acc_vowel': 0.995738, 'acc_consonant': 0.996386, 'acc_word': 0.987863, 'loss_grapheme': 0.081173, 'loss_vowel': 0.073517, 'loss_consonant': 0.05322, 'loss_word': 0.072305}\n",
      "   19 | 0.000113 | 160600/160716 | 8.6902 | 12.1632 ||\n",
      "val: {'recall': 0.992599, 'recall_grapheme': 0.988573, 'recall_vowel': 0.996077, 'recall_consonant': 0.997173, 'recall_word': 0.986991, 'acc_grapheme': 0.988212, 'acc_vowel': 0.996037, 'acc_consonant': 0.996062, 'acc_word': 0.986841, 'loss_grapheme': 0.086934, 'loss_vowel': 0.074505, 'loss_consonant': 0.055405, 'loss_word': 0.076862}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20 | 0.000109 | 160600/160716 | 12.8580 | 12.4901 |\n",
      "val: {'recall': 0.992932, 'recall_grapheme': 0.989504, 'recall_vowel': 0.99572, 'recall_consonant': 0.996999, 'recall_word': 0.98799, 'acc_grapheme': 0.988486, 'acc_vowel': 0.996237, 'acc_consonant': 0.996137, 'acc_word': 0.987863, 'loss_grapheme': 0.091197, 'loss_vowel': 0.081313, 'loss_consonant': 0.059179, 'loss_word': 0.077822}\n",
      "   21 | 0.000106 | 160600/160716 | 21.4262 | 11.8378 |\n",
      "val: {'recall': 0.991404, 'recall_grapheme': 0.987282, 'recall_vowel': 0.995096, 'recall_consonant': 0.995958, 'recall_word': 0.984981, 'acc_grapheme': 0.986367, 'acc_vowel': 0.995788, 'acc_consonant': 0.995514, 'acc_word': 0.984772, 'loss_grapheme': 0.08523, 'loss_vowel': 0.068181, 'loss_consonant': 0.052766, 'loss_word': 0.078215}\n",
      "   22 | 0.000102 | 160600/160716 | 18.5419 | 11.5148 |\n",
      "val: {'recall': 0.992849, 'recall_grapheme': 0.98917, 'recall_vowel': 0.996185, 'recall_consonant': 0.996869, 'recall_word': 0.987154, 'acc_grapheme': 0.98861, 'acc_vowel': 0.996361, 'acc_consonant': 0.996411, 'acc_word': 0.987065, 'loss_grapheme': 0.067574, 'loss_vowel': 0.051863, 'loss_consonant': 0.040811, 'loss_word': 0.0661}\n",
      "   23 | 0.000098 | 160600/160716 | 15.9050 | 12.2798 |\n",
      "val: {'recall': 0.992218, 'recall_grapheme': 0.988567, 'recall_vowel': 0.995422, 'recall_consonant': 0.996314, 'recall_word': 0.987413, 'acc_grapheme': 0.988685, 'acc_vowel': 0.996012, 'acc_consonant': 0.996012, 'acc_word': 0.987289, 'loss_grapheme': 0.077397, 'loss_vowel': 0.067708, 'loss_consonant': 0.050019, 'loss_word': 0.068686}\n",
      "   24 | 0.000094 | 160600/160716 | 21.3232 | 12.0582 |\n",
      "val: {'recall': 0.99276, 'recall_grapheme': 0.98983, 'recall_vowel': 0.995059, 'recall_consonant': 0.996323, 'recall_word': 0.987946, 'acc_grapheme': 0.989184, 'acc_vowel': 0.995738, 'acc_consonant': 0.996212, 'acc_word': 0.987838, 'loss_grapheme': 0.078572, 'loss_vowel': 0.073213, 'loss_consonant': 0.053275, 'loss_word': 0.069531}\n",
      "   25 | 0.000091 | 160600/160716 | 6.9535 | 11.7301 ||\n",
      "val: {'recall': 0.992013, 'recall_grapheme': 0.988017, 'recall_vowel': 0.9951, 'recall_consonant': 0.996919, 'recall_word': 0.986262, 'acc_grapheme': 0.987738, 'acc_vowel': 0.995639, 'acc_consonant': 0.996087, 'acc_word': 0.986193, 'loss_grapheme': 0.076189, 'loss_vowel': 0.066543, 'loss_consonant': 0.049435, 'loss_word': 0.070078}\n",
      "   26 | 0.000087 | 160600/160716 | 19.6532 | 12.4137 |\n",
      "val: {'recall': 0.994103, 'recall_grapheme': 0.991619, 'recall_vowel': 0.995561, 'recall_consonant': 0.997611, 'recall_word': 0.989106, 'acc_grapheme': 0.99043, 'acc_vowel': 0.996137, 'acc_consonant': 0.996411, 'acc_word': 0.988909, 'loss_grapheme': 0.074174, 'loss_vowel': 0.072768, 'loss_consonant': 0.05343, 'loss_word': 0.064007}\n",
      "   27 | 0.000083 | 160600/160716 | 1.7071 | 11.8384 ||\n",
      "val: {'recall': 0.992512, 'recall_grapheme': 0.989315, 'recall_vowel': 0.994986, 'recall_consonant': 0.99643, 'recall_word': 0.987186, 'acc_grapheme': 0.988461, 'acc_vowel': 0.995813, 'acc_consonant': 0.996262, 'acc_word': 0.987015, 'loss_grapheme': 0.079611, 'loss_vowel': 0.068266, 'loss_consonant': 0.051436, 'loss_word': 0.074334}\n",
      "   28 | 0.000079 | 160600/160716 | 6.7468 | 11.6960 ||\n",
      "val: {'recall': 0.992833, 'recall_grapheme': 0.989211, 'recall_vowel': 0.995687, 'recall_consonant': 0.997222, 'recall_word': 0.98668, 'acc_grapheme': 0.988112, 'acc_vowel': 0.995987, 'acc_consonant': 0.995987, 'acc_word': 0.986592, 'loss_grapheme': 0.075919, 'loss_vowel': 0.063291, 'loss_consonant': 0.049908, 'loss_word': 0.06992}\n",
      "   29 | 0.000075 | 160600/160716 | 21.3569 | 12.3102 |\n",
      "val: {'recall': 0.992252, 'recall_grapheme': 0.988255, 'recall_vowel': 0.995424, 'recall_consonant': 0.997076, 'recall_word': 0.986225, 'acc_grapheme': 0.987688, 'acc_vowel': 0.995863, 'acc_consonant': 0.995888, 'acc_word': 0.986143, 'loss_grapheme': 0.077506, 'loss_vowel': 0.061731, 'loss_consonant': 0.049565, 'loss_word': 0.07051}\n",
      "   30 | 0.000071 | 160600/160716 | 18.8522 | 12.3704 |\n",
      "val: {'recall': 0.993287, 'recall_grapheme': 0.990033, 'recall_vowel': 0.995658, 'recall_consonant': 0.997424, 'recall_word': 0.989241, 'acc_grapheme': 0.989358, 'acc_vowel': 0.996212, 'acc_consonant': 0.996685, 'acc_word': 0.989134, 'loss_grapheme': 0.083029, 'loss_vowel': 0.083824, 'loss_consonant': 0.060434, 'loss_word': 0.069779}\n",
      "   31 | 0.000067 | 160600/160716 | 6.9992 | 12.3257 ||\n",
      "val: {'recall': 0.99112, 'recall_grapheme': 0.987209, 'recall_vowel': 0.995528, 'recall_consonant': 0.994536, 'recall_word': 0.985203, 'acc_grapheme': 0.986691, 'acc_vowel': 0.995813, 'acc_consonant': 0.995464, 'acc_word': 0.985121, 'loss_grapheme': 0.088964, 'loss_vowel': 0.0705, 'loss_consonant': 0.054817, 'loss_word': 0.076908}\n",
      "   32 | 0.000063 | 160600/160716 | 7.6967 | 12.4723 ||\n",
      "val: {'recall': 0.993555, 'recall_grapheme': 0.990648, 'recall_vowel': 0.995514, 'recall_consonant': 0.997412, 'recall_word': 0.988841, 'acc_grapheme': 0.989856, 'acc_vowel': 0.996162, 'acc_consonant': 0.996735, 'acc_word': 0.988785, 'loss_grapheme': 0.06662, 'loss_vowel': 0.060738, 'loss_consonant': 0.044713, 'loss_word': 0.06125}\n",
      "   33 | 0.000059 | 160600/160716 | 11.1515 | 11.7923 |\n",
      "val: {'recall': 0.993976, 'recall_grapheme': 0.991093, 'recall_vowel': 0.996192, 'recall_consonant': 0.997527, 'recall_word': 0.989324, 'acc_grapheme': 0.99018, 'acc_vowel': 0.996212, 'acc_consonant': 0.99671, 'acc_word': 0.989159, 'loss_grapheme': 0.065067, 'loss_vowel': 0.059514, 'loss_consonant': 0.043087, 'loss_word': 0.060665}\n",
      "   34 | 0.000056 | 160600/160716 | 9.0273 | 11.9409 ||\n",
      "val: {'recall': 0.992394, 'recall_grapheme': 0.989265, 'recall_vowel': 0.995475, 'recall_consonant': 0.99557, 'recall_word': 0.986626, 'acc_grapheme': 0.987962, 'acc_vowel': 0.995863, 'acc_consonant': 0.996287, 'acc_word': 0.986542, 'loss_grapheme': 0.077369, 'loss_vowel': 0.06813, 'loss_consonant': 0.052525, 'loss_word': 0.067972}\n",
      "   35 | 0.000052 | 160600/160716 | 7.6230 | 11.8212 ||\n",
      "val: {'recall': 0.992989, 'recall_grapheme': 0.989554, 'recall_vowel': 0.995318, 'recall_consonant': 0.997532, 'recall_word': 0.987776, 'acc_grapheme': 0.988735, 'acc_vowel': 0.995938, 'acc_consonant': 0.996237, 'acc_word': 0.987564, 'loss_grapheme': 0.071791, 'loss_vowel': 0.062897, 'loss_consonant': 0.048328, 'loss_word': 0.066209}\n",
      "   36 | 0.000048 | 160600/160716 | 19.1467 | 12.0890 |\n",
      "val: {'recall': 0.993211, 'recall_grapheme': 0.990281, 'recall_vowel': 0.996235, 'recall_consonant': 0.996045, 'recall_word': 0.988318, 'acc_grapheme': 0.989532, 'acc_vowel': 0.996411, 'acc_consonant': 0.99676, 'acc_word': 0.988187, 'loss_grapheme': 0.069127, 'loss_vowel': 0.059535, 'loss_consonant': 0.045633, 'loss_word': 0.064343}\n",
      "   37 | 0.000045 | 160600/160716 | 20.3674 | 11.5136 |\n",
      "val: {'recall': 0.992573, 'recall_grapheme': 0.989061, 'recall_vowel': 0.995746, 'recall_consonant': 0.996423, 'recall_word': 0.986994, 'acc_grapheme': 0.988012, 'acc_vowel': 0.996112, 'acc_consonant': 0.996511, 'acc_word': 0.986866, 'loss_grapheme': 0.076973, 'loss_vowel': 0.065886, 'loss_consonant': 0.051253, 'loss_word': 0.069056}\n",
      "   38 | 0.000041 | 160600/160716 | 19.8976 | 12.2497 |\n",
      "val: {'recall': 0.993436, 'recall_grapheme': 0.990447, 'recall_vowel': 0.996316, 'recall_consonant': 0.996533, 'recall_word': 0.988296, 'acc_grapheme': 0.989682, 'acc_vowel': 0.996436, 'acc_consonant': 0.996586, 'acc_word': 0.988112, 'loss_grapheme': 0.071324, 'loss_vowel': 0.064818, 'loss_consonant': 0.049926, 'loss_word': 0.06484}\n",
      "   39 | 0.000038 | 160600/160716 | 13.4699 | 11.8916 |\n",
      "val: {'recall': 0.992928, 'recall_grapheme': 0.989668, 'recall_vowel': 0.995916, 'recall_consonant': 0.996462, 'recall_word': 0.987528, 'acc_grapheme': 0.988884, 'acc_vowel': 0.996212, 'acc_consonant': 0.996336, 'acc_word': 0.987364, 'loss_grapheme': 0.069106, 'loss_vowel': 0.056199, 'loss_consonant': 0.044899, 'loss_word': 0.0644}\n",
      "   40 | 0.000034 | 160600/160716 | 17.5395 | 11.9140 |\n",
      "val: {'recall': 0.99405, 'recall_grapheme': 0.9911, 'recall_vowel': 0.996251, 'recall_consonant': 0.99775, 'recall_word': 0.988802, 'acc_grapheme': 0.989807, 'acc_vowel': 0.996336, 'acc_consonant': 0.99676, 'acc_word': 0.988585, 'loss_grapheme': 0.076408, 'loss_vowel': 0.073745, 'loss_consonant': 0.054509, 'loss_word': 0.067081}\n",
      "   41 | 0.000031 | 160600/160716 | 4.2393 | 11.7289 ||\n",
      "val: {'recall': 0.992444, 'recall_grapheme': 0.988987, 'recall_vowel': 0.995932, 'recall_consonant': 0.99587, 'recall_word': 0.986379, 'acc_grapheme': 0.987937, 'acc_vowel': 0.996162, 'acc_consonant': 0.996062, 'acc_word': 0.986218, 'loss_grapheme': 0.075943, 'loss_vowel': 0.053815, 'loss_consonant': 0.042673, 'loss_word': 0.06999}\n",
      "   42 | 0.000028 | 160600/160716 | 20.6966 | 11.6732 |\n",
      "val: {'recall': 0.992795, 'recall_grapheme': 0.989017, 'recall_vowel': 0.995607, 'recall_consonant': 0.997537, 'recall_word': 0.986842, 'acc_grapheme': 0.988536, 'acc_vowel': 0.996012, 'acc_consonant': 0.996411, 'acc_word': 0.986616, 'loss_grapheme': 0.075072, 'loss_vowel': 0.06406, 'loss_consonant': 0.048939, 'loss_word': 0.068606}\n",
      "   43 | 0.000025 | 160600/160716 | 17.5353 | 11.5847 |\n",
      "val: {'recall': 0.993747, 'recall_grapheme': 0.990533, 'recall_vowel': 0.99606, 'recall_consonant': 0.997862, 'recall_word': 0.988498, 'acc_grapheme': 0.989582, 'acc_vowel': 0.996561, 'acc_consonant': 0.99686, 'acc_word': 0.988261, 'loss_grapheme': 0.07299, 'loss_vowel': 0.067508, 'loss_consonant': 0.051682, 'loss_word': 0.066874}\n",
      "   44 | 0.000022 | 160600/160716 | 5.3467 | 11.9887 ||\n",
      "val: {'recall': 0.992248, 'recall_grapheme': 0.988028, 'recall_vowel': 0.995608, 'recall_consonant': 0.997329, 'recall_word': 0.985881, 'acc_grapheme': 0.987564, 'acc_vowel': 0.996087, 'acc_consonant': 0.996212, 'acc_word': 0.985744, 'loss_grapheme': 0.077822, 'loss_vowel': 0.058226, 'loss_consonant': 0.047295, 'loss_word': 0.071351}\n",
      "   45 | 0.000019 | 160600/160716 | 2.3381 | 11.8392 ||\n",
      "val: {'recall': 0.992748, 'recall_grapheme': 0.988967, 'recall_vowel': 0.995608, 'recall_consonant': 0.997451, 'recall_word': 0.986917, 'acc_grapheme': 0.988461, 'acc_vowel': 0.996262, 'acc_consonant': 0.996361, 'acc_word': 0.986766, 'loss_grapheme': 0.070164, 'loss_vowel': 0.055356, 'loss_consonant': 0.044495, 'loss_word': 0.064551}\n",
      "   46 | 0.000017 | 160600/160716 | 2.6611 | 11.8358 ||\n",
      "val: {'recall': 0.992936, 'recall_grapheme': 0.989229, 'recall_vowel': 0.995685, 'recall_consonant': 0.997603, 'recall_word': 0.986947, 'acc_grapheme': 0.988461, 'acc_vowel': 0.995987, 'acc_consonant': 0.996436, 'acc_word': 0.986891, 'loss_grapheme': 0.070484, 'loss_vowel': 0.055586, 'loss_consonant': 0.044091, 'loss_word': 0.065221}\n",
      "   47 | 0.000014 | 160600/160716 | 1.8078 | 11.5339 ||\n",
      "val: {'recall': 0.993619, 'recall_grapheme': 0.990226, 'recall_vowel': 0.996227, 'recall_consonant': 0.997796, 'recall_word': 0.98831, 'acc_grapheme': 0.989657, 'acc_vowel': 0.996411, 'acc_consonant': 0.99681, 'acc_word': 0.988236, 'loss_grapheme': 0.062961, 'loss_vowel': 0.051271, 'loss_consonant': 0.03945, 'loss_word': 0.060238}\n",
      "   48 | 0.000012 | 160600/160716 | 2.9675 | 11.8938 ||\n",
      "val: {'recall': 0.994276, 'recall_grapheme': 0.991425, 'recall_vowel': 0.996327, 'recall_consonant': 0.997927, 'recall_word': 0.989098, 'acc_grapheme': 0.990355, 'acc_vowel': 0.996586, 'acc_consonant': 0.997059, 'acc_word': 0.988959, 'loss_grapheme': 0.061696, 'loss_vowel': 0.055637, 'loss_consonant': 0.041405, 'loss_word': 0.058099}\n",
      "   49 | 0.000010 | 160600/160716 | 19.1487 | 11.9649 |\n",
      "val: {'recall': 0.993717, 'recall_grapheme': 0.990381, 'recall_vowel': 0.996335, 'recall_consonant': 0.997772, 'recall_word': 0.98811, 'acc_grapheme': 0.989532, 'acc_vowel': 0.996311, 'acc_consonant': 0.99666, 'acc_word': 0.988037, 'loss_grapheme': 0.071715, 'loss_vowel': 0.065527, 'loss_consonant': 0.050868, 'loss_word': 0.064341}\n",
      "   50 | 0.000008 | 160600/160716 | 17.1120 | 12.2664 |\n",
      "val: {'recall': 0.994094, 'recall_grapheme': 0.99107, 'recall_vowel': 0.996327, 'recall_consonant': 0.99791, 'recall_word': 0.989444, 'acc_grapheme': 0.990405, 'acc_vowel': 0.996586, 'acc_consonant': 0.997034, 'acc_word': 0.989333, 'loss_grapheme': 0.071173, 'loss_vowel': 0.06903, 'loss_consonant': 0.050873, 'loss_word': 0.064891}\n",
      "   51 | 0.000006 | 160600/160716 | 8.3956 | 11.2748 ||\n",
      "val: {'recall': 0.992432, 'recall_grapheme': 0.988547, 'recall_vowel': 0.995944, 'recall_consonant': 0.99669, 'recall_word': 0.986149, 'acc_grapheme': 0.987738, 'acc_vowel': 0.996037, 'acc_consonant': 0.995963, 'acc_word': 0.986093, 'loss_grapheme': 0.085711, 'loss_vowel': 0.068486, 'loss_consonant': 0.054903, 'loss_word': 0.0749}\n",
      "   52 | 0.000005 | 160600/160716 | 9.8796 | 11.0458 ||\n",
      "val: {'recall': 0.993731, 'recall_grapheme': 0.990309, 'recall_vowel': 0.996384, 'recall_consonant': 0.99792, 'recall_word': 0.988233, 'acc_grapheme': 0.989807, 'acc_vowel': 0.996311, 'acc_consonant': 0.99691, 'acc_word': 0.988162, 'loss_grapheme': 0.065796, 'loss_vowel': 0.055845, 'loss_consonant': 0.042759, 'loss_word': 0.062336}\n",
      "   53 | 0.000004 | 160600/160716 | 20.8151 | 11.6957 |\n",
      "val: {'recall': 0.993748, 'recall_grapheme': 0.990326, 'recall_vowel': 0.996392, 'recall_consonant': 0.997947, 'recall_word': 0.988196, 'acc_grapheme': 0.989807, 'acc_vowel': 0.996461, 'acc_consonant': 0.99691, 'acc_word': 0.988112, 'loss_grapheme': 0.0677, 'loss_vowel': 0.056483, 'loss_consonant': 0.043878, 'loss_word': 0.063959}\n",
      "   54 | 0.000003 | 160600/160716 | 19.5643 | 11.2399 |\n",
      "val: {'recall': 0.994089, 'recall_grapheme': 0.99109, 'recall_vowel': 0.996381, 'recall_consonant': 0.997795, 'recall_word': 0.98936, 'acc_grapheme': 0.989981, 'acc_vowel': 0.996511, 'acc_consonant': 0.996885, 'acc_word': 0.989208, 'loss_grapheme': 0.075085, 'loss_vowel': 0.073788, 'loss_consonant': 0.055646, 'loss_word': 0.066207}\n",
      "   55 | 0.000002 | 160600/160716 | 20.4341 | 11.3526 |\n",
      "val: {'recall': 0.993831, 'recall_grapheme': 0.990608, 'recall_vowel': 0.996322, 'recall_consonant': 0.997784, 'recall_word': 0.988673, 'acc_grapheme': 0.989732, 'acc_vowel': 0.996386, 'acc_consonant': 0.99671, 'acc_word': 0.98856, 'loss_grapheme': 0.075924, 'loss_vowel': 0.073879, 'loss_consonant': 0.055656, 'loss_word': 0.066235}\n",
      "   56 | 0.000001 | 160600/160716 | 16.5190 | 11.6747 |\n",
      "val: {'recall': 0.994189, 'recall_grapheme': 0.991243, 'recall_vowel': 0.99625, 'recall_consonant': 0.998021, 'recall_word': 0.989537, 'acc_grapheme': 0.990305, 'acc_vowel': 0.996536, 'acc_consonant': 0.997009, 'acc_word': 0.989383, 'loss_grapheme': 0.073363, 'loss_vowel': 0.074133, 'loss_consonant': 0.055022, 'loss_word': 0.062606}\n",
      "   57 | 0.000000 | 160600/160716 | 20.3821 | 11.7235 |\n",
      "val: {'recall': 0.993634, 'recall_grapheme': 0.990167, 'recall_vowel': 0.996366, 'recall_consonant': 0.997836, 'recall_word': 0.988332, 'acc_grapheme': 0.989508, 'acc_vowel': 0.996436, 'acc_consonant': 0.996685, 'acc_word': 0.988187, 'loss_grapheme': 0.067412, 'loss_vowel': 0.058849, 'loss_consonant': 0.045633, 'loss_word': 0.062094}\n",
      "   58 | 0.000000 | 160600/160716 | 1.8298 | 11.3104 ||\n",
      "val: {'recall': 0.994082, 'recall_grapheme': 0.990912, 'recall_vowel': 0.996463, 'recall_consonant': 0.99804, 'recall_word': 0.988688, 'acc_grapheme': 0.990106, 'acc_vowel': 0.996685, 'acc_consonant': 0.997084, 'acc_word': 0.98856, 'loss_grapheme': 0.059955, 'loss_vowel': 0.048395, 'loss_consonant': 0.037502, 'loss_word': 0.05828}\n",
      "   59 | 0.000000 | 160600/160716 | 12.2206 | 11.8528 |\n",
      "val: {'recall': 0.993947, 'recall_grapheme': 0.990814, 'recall_vowel': 0.996237, 'recall_consonant': 0.997925, 'recall_word': 0.988802, 'acc_grapheme': 0.990031, 'acc_vowel': 0.996436, 'acc_consonant': 0.99681, 'acc_word': 0.988735, 'loss_grapheme': 0.068298, 'loss_vowel': 0.06335, 'loss_consonant': 0.048498, 'loss_word': 0.061822}\n",
      "CYCLE: 2\n",
      "    0 | 0.000015 | 160600/160716 | 3.5354 | 11.9033 ||\n",
      "val: {'recall': 0.992039, 'recall_grapheme': 0.9882, 'recall_vowel': 0.99583, 'recall_consonant': 0.995924, 'recall_word': 0.985947, 'acc_grapheme': 0.987539, 'acc_vowel': 0.995938, 'acc_consonant': 0.995913, 'acc_word': 0.985844, 'loss_grapheme': 0.081415, 'loss_vowel': 0.062208, 'loss_consonant': 0.050522, 'loss_word': 0.072203}\n",
      "    1 | 0.000030 | 160600/160716 | 5.7630 | 11.6101 ||\n",
      "val: {'recall': 0.991894, 'recall_grapheme': 0.988306, 'recall_vowel': 0.995536, 'recall_consonant': 0.995429, 'recall_word': 0.985398, 'acc_grapheme': 0.987564, 'acc_vowel': 0.995863, 'acc_consonant': 0.995813, 'acc_word': 0.98537, 'loss_grapheme': 0.081032, 'loss_vowel': 0.058503, 'loss_consonant': 0.048225, 'loss_word': 0.073108}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2 | 0.000045 | 160600/160716 | 6.3126 | 11.7465 ||\n",
      "val: {'recall': 0.992897, 'recall_grapheme': 0.989372, 'recall_vowel': 0.995471, 'recall_consonant': 0.997373, 'recall_word': 0.987215, 'acc_grapheme': 0.988685, 'acc_vowel': 0.996162, 'acc_consonant': 0.996361, 'acc_word': 0.98709, 'loss_grapheme': 0.072414, 'loss_vowel': 0.062011, 'loss_consonant': 0.048047, 'loss_word': 0.06711}\n",
      "    3 | 0.000059 | 160600/160716 | 5.7804 | 11.6953 ||\n",
      "val: {'recall': 0.992867, 'recall_grapheme': 0.989194, 'recall_vowel': 0.9957, 'recall_consonant': 0.997381, 'recall_word': 0.987, 'acc_grapheme': 0.988486, 'acc_vowel': 0.995813, 'acc_consonant': 0.996212, 'acc_word': 0.986791, 'loss_grapheme': 0.070728, 'loss_vowel': 0.055763, 'loss_consonant': 0.044861, 'loss_word': 0.068359}\n",
      "    4 | 0.000074 | 160600/160716 | 20.0910 | 12.2234 |\n",
      "val: {'recall': 0.993096, 'recall_grapheme': 0.98954, 'recall_vowel': 0.995603, 'recall_consonant': 0.9977, 'recall_word': 0.986765, 'acc_grapheme': 0.988461, 'acc_vowel': 0.995838, 'acc_consonant': 0.996486, 'acc_word': 0.986542, 'loss_grapheme': 0.076906, 'loss_vowel': 0.069484, 'loss_consonant': 0.053104, 'loss_word': 0.070164}\n",
      "    5 | 0.000088 | 160600/160716 | 20.7939 | 11.9841 |\n",
      "val: {'recall': 0.992874, 'recall_grapheme': 0.989223, 'recall_vowel': 0.995865, 'recall_consonant': 0.997183, 'recall_word': 0.98736, 'acc_grapheme': 0.98881, 'acc_vowel': 0.995938, 'acc_consonant': 0.995987, 'acc_word': 0.987215, 'loss_grapheme': 0.07172, 'loss_vowel': 0.061408, 'loss_consonant': 0.046861, 'loss_word': 0.06604}\n",
      "    6 | 0.000101 | 160600/160716 | 15.4941 | 12.1950 |\n",
      "val: {'recall': 0.993481, 'recall_grapheme': 0.990295, 'recall_vowel': 0.995877, 'recall_consonant': 0.997458, 'recall_word': 0.987781, 'acc_grapheme': 0.988884, 'acc_vowel': 0.996311, 'acc_consonant': 0.996436, 'acc_word': 0.987564, 'loss_grapheme': 0.074941, 'loss_vowel': 0.07292, 'loss_consonant': 0.05314, 'loss_word': 0.06838}\n",
      "    7 | 0.000115 | 160600/160716 | 6.5406 | 11.5644 ||\n",
      "val: {'recall': 0.991698, 'recall_grapheme': 0.987495, 'recall_vowel': 0.994886, 'recall_consonant': 0.996915, 'recall_word': 0.985754, 'acc_grapheme': 0.98714, 'acc_vowel': 0.995763, 'acc_consonant': 0.995838, 'acc_word': 0.985645, 'loss_grapheme': 0.088921, 'loss_vowel': 0.07613, 'loss_consonant': 0.058449, 'loss_word': 0.076947}\n",
      "    8 | 0.000128 | 160600/160716 | 18.5975 | 12.1717 |\n",
      "val: {'recall': 0.993617, 'recall_grapheme': 0.990667, 'recall_vowel': 0.995921, 'recall_consonant': 0.997216, 'recall_word': 0.989228, 'acc_grapheme': 0.990081, 'acc_vowel': 0.996287, 'acc_consonant': 0.99666, 'acc_word': 0.989059, 'loss_grapheme': 0.068305, 'loss_vowel': 0.058409, 'loss_consonant': 0.044691, 'loss_word': 0.063633}\n",
      "    9 | 0.000140 | 160600/160716 | 1.0394 | 12.2490 ||\n",
      "val: {'recall': 0.993617, 'recall_grapheme': 0.990487, 'recall_vowel': 0.99597, 'recall_consonant': 0.997522, 'recall_word': 0.988195, 'acc_grapheme': 0.988959, 'acc_vowel': 0.996237, 'acc_consonant': 0.996361, 'acc_word': 0.987962, 'loss_grapheme': 0.065814, 'loss_vowel': 0.05333, 'loss_consonant': 0.040168, 'loss_word': 0.064123}\n",
      "   10 | 0.000139 | 090640/160716 | 0.8830 | 11.5729 ||"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-aad596905035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-b2c1bf117c1f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mvalidate_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-14841a105de8>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(args, model, train_loader, epoch, optimizer, lr_scheduler, grid)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m#loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/apex/amp/handle.py\u001b[0m in \u001b[0;36mscale_loss\u001b[0;34m(loss, optimizers, loss_id, model, delay_unscale, delay_overflow_check)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_amp_stash\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams_have_scaled_gradients\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_amp_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/apex/amp/_process_optimizer.py\u001b[0m in \u001b[0;36mprepare_backward_no_master_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mstash\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_fp32_grad_stash\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m# Set up to leverage grad copy elision:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_model(model, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
